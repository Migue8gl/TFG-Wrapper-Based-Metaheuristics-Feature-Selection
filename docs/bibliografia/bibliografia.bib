@misc{udacity2015curse,
  author       = {Udacity},
  title        = {Curse of Dimensionality - Georgia Tech - Machine Learning},
  year         = {2015},
  month        = feb,
  howpublished = {Retrieved 2022-06-29}
}


@article{miao_survey_2016,
  series   = {Promoting {Business} {Analytics} and {Quantitative} {Management} of {Technology}: 4th {International} {Conference} on {Information} {Technology} and {Quantitative} {Management} ({ITQM} 2016)},
  title    = {A {Survey} on {Feature} {Selection}},
  volume   = {91},
  issn     = {1877-0509},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050916313047},
  doi      = {10.1016/j.procs.2016.07.111},
  abstract = {Feature selection, as a dimensionality reduction technique, aims to choosing a small subset of the relevant features from the original features by removing irrelevant, redundant or noisy features. Feature selection usually can lead to better learning performance, i.e., higher learning accuracy, lower computational cost, and better model interpretability. Recently, researchers from computer vision, text mining and so on have proposed a variety of feature selection algorithms and in terms of theory and experiment, show the effectiveness of their works. This paper is aimed at reviewing the state of the art on these techniques. Furthermore, a thorough experiment is conducted to check if the use of feature selection can improve the performance of learning, considering some of the approaches mentioned in the literature. The experimental results show that unsupervised feature selection algorithms benefits machine learning tasks improving the performance of clustering.},
  urldate  = {2024-04-02},
  journal  = {Procedia Computer Science},
  author   = {Miao, Jianyu and Niu, Lingfeng},
  month    = jan,
  year     = {2016},
  keywords = {clustering, feature selection, machine learning, unsupervised},
  pages    = {919--926},
  file     = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/H3KRFFAZ/Miao and Niu - 2016 - A Survey on Feature Selection.pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/PVAA7HS6/S1877050916313047.html:text/html}
}

@book{Mostafa2012,
  title     = {Learning From Data},
  author    = {Abu-Mostafa, Yaser S. and Magdon-Ismail, Malik and Lin, Hsuan-Tien},
  keywords  = {general_machine_learning},
  publisher = {AMLBook},
  title     = {Learning From Data},
  year      = 2012
}

@inproceedings{kira_practical_1992,
  title    = {A {Practical} {Approach} to {Feature} {Selection}},
  isbn     = {978-1-55860-247-2},
  doi      = {10.1016/B978-1-55860-247-2.50037-1},
  abstract = {In real-world concept learning problems, the representation of data often uses many features, only a few of which may be related to the target concept. In this situation, feature selection is important both to speed up learning and to improve concept quality. A new feature selection algorithm Relief uses a statistical method and avoids heuristic search. Relief requires linear time in the number of given features and the number of training instances regardless of the target concept to be learned. Although the algorithm does not necessarily find the smallest subset of features, the size tends to be small because only statistically relevant features are selected. This paper focuses on empirical test results in two artificial domains; the LED Display domain and the Parity domain with and without noise. Comparison with other feature selection algorithms shows Reliefs advantages in terms of learning time and the accuracy of the learned concept, suggesting Reliefs practicality. © 1992 Proceedings of the 9th International Workshop on Machine Learning, ICML 1992. All rights reserved.},
  language = {English},
  author   = {Kira, K. and Rendell, L.A.},
  year     = {1992},
  pages    = {249--256},
  annote   = {Cited By :2345},
  file     = {Kira and Rendell - 1992 - A Practical Approach to Feature Selection.pdf:/home/migue8gl/Zotero/storage/4GZM4TLW/Kira and Rendell - 1992 - A Practical Approach to Feature Selection.pdf:application/pdf}
}


@article{ding_minimum_2005,
  title    = {Minimum redundancy feature selection from microarray gene expression data},
  volume   = {3},
  issn     = {0219-7200},
  doi      = {10.1142/S0219720005001004},
  abstract = {How to selecting a small subset out of the thousands of genes in microarray data is important for accurate classification of phenotypes. Widely used methods typically rank genes according to their differential expressions among phenotypes and pick the top-ranked genes. We observe that feature sets so obtained have certain redundancy and study methods to minimize it. We propose a minimum redundancy - maximum relevance (MRMR) feature selection framework. Genes selected via MRMR provide a more balanced coverage of the space and capture broader characteristics of phenotypes. They lead to significantly improved class predictions in extensive experiments on 6 gene expression data sets: NCI, Lymphoma, Lung, Child Leukemia, Leukemia, and Colon. Improvements are observed consistently among 4 classification methods: Naïve Bayes, Linear discriminant analysis, Logistic regression, and Support vector machines. © Imperial College Press.},
  language = {English},
  number   = {2},
  journal  = {Journal of Bioinformatics and Computational Biology},
  author   = {Ding, C. and Peng, H.},
  year     = {2005},
  keywords = {Cancer classification, Gene expression analysis, Gene selection, Naïve Bayes, SVM},
  pages    = {185--205},
  annote   = {Cited By :1780},
  file     = {Ding and Peng - 2005 - Minimum redundancy feature selection from microarray gene expression data.pdf:/home/migue8gl/Zotero/storage/BSLFQ4LD/Ding and Peng - 2005 - Minimum redundancy feature selection from microarray gene expression data.pdf:application/pdf}
}


@article{cortes_support-vector_1995,
  title    = {Support-vector networks},
  volume   = {20},
  url      = {http://link.springer.com/10.1007/BF00994018},
  doi      = {10.1007/BF00994018},
  abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very highdimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
  language = {en},
  number   = {3},
  urldate  = {2024-04-02},
  journal  = {Machine Learning},
  author   = {Cortes, Corinna and Vapnik, Vladimir},
  month    = sep,
  year     = {1995},
  pages    = {273--297},
  file     = {Cortes and Vapnik - 1995 - Support-vector networks.pdf:/home/migue8gl/Zotero/storage/S5FZIC4M/Cortes and Vapnik - 1995 - Support-vector networks.pdf:application/pdf}
}


@article{cover_nearest_1967,
  title    = {Nearest neighbor pattern classification},
  volume   = {13},
  url      = {http://ieeexplore.ieee.org/document/1053964/},
  doi      = {10.1109/TIT.1967.1053964},
  language = {en},
  number   = {1},
  urldate  = {2024-04-02},
  journal  = {IEEE Transactions on Information Theory},
  author   = {Cover, T. and Hart, P.},
  month    = jan,
  year     = {1967},
  pages    = {21--27},
  file     = {Cover and Hart - 1967 - Nearest neighbor pattern classification.pdf:/home/migue8gl/Zotero/storage/AM949IMF/Cover and Hart - 1967 - Nearest neighbor pattern classification.pdf:application/pdf}
}


@article{fix_discriminatory_1989,
  title      = {Discriminatory {Analysis}. {Nonparametric} {Discrimination}: {Consistency} {Properties}},
  volume     = {57},
  issn       = {0306-7734},
  shorttitle = {Discriminatory {Analysis}. {Nonparametric} {Discrimination}},
  url        = {https://www.jstor.org/stable/1403797},
  doi        = {10.2307/1403797},
  number     = {3},
  urldate    = {2024-04-02},
  journal    = {International Statistical Review / Revue Internationale de Statistique},
  author     = {Fix, Evelyn and Hodges, J. L.},
  year       = {1989},
  note       = {Publisher: [Wiley, International Statistical Institute (ISI)]},
  pages      = {238--247},
  file       = {JSTOR Full Text PDF:/home/migue8gl/Zotero/storage/AZUBG9M6/Fix and Hodges - 1989 - Discriminatory Analysis. Nonparametric Discrimination Consistency Properties.pdf:application/pdf}
}

@book{Clark1922,
  author    = {Clark, Wallace and Polakov, Walter Nicholas and Trabold, Frank W},
  title     = {The Gantt chart, a working tool of management},
  year      = {1922},
  publisher = {The Ronald press company},
  address   = {New York},
  keywords  = {Industrial efficiency, Production scheduling, Graphic methods, Gantt charts},
  language  = {English}
}

@article{woidasky_use_2021,
  title    = {Use pattern relevance for laptop repair and product lifetime},
  volume   = {288},
  issn     = {0959-6526},
  url      = {https://www.sciencedirect.com/science/article/pii/S0959652620354718},
  doi      = {10.1016/j.jclepro.2020.125425},
  abstract = {More than 166 million mobile computers worldwide are being sold annually. Their duration of use is not only influenced by the design and quality of components and production, but also by the user behaviour. In this descriptive study, the effects of user decisions on the duration of product life is discussed based on two surveys carried out in a university student environment in southwest Germany. Results show that use phase length expectation for the devices clearly exceed 5 years, but the actual use phase duration was found to be only about 80\% of this time span. Consequently, the use pattern for laptops was described covering the aspects of user knowledge, use and repair intensity, the motivation for and the mode of disposal. Results show that only a minor share of users reads the user manuals. Those who do so and adhere to their recommendations experience a clearly increased use phase length. During the entire product lifetime of mobile computers, about one third of all devices is being repaired. The most important repairs were battery replacements, power supply unit repairs, and housing repairs. After the end of their useful life, more than 60\% of the obsolete devices are still operational. Users pass on only 25\% of their obsolete devices for a second use phase, but almost 2/3 of all laptops are stored, and only 11\% are disposed of properly.},
  urldate  = {2024-04-16},
  journal  = {Journal of Cleaner Production},
  author   = {Woidasky, Jörg and Cetinkaya, Esra},
  month    = mar,
  year     = {2021},
  keywords = {Laptop, Obsolescence, Product lifetime, Repair, Students, Survey, Use pattern},
  pages    = {125425},
  file     = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/LPD5VSRU/Woidasky and Cetinkaya - 2021 - Use pattern relevance for laptop repair and product lifetime.pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/RRGSUU72/S0959652620354718.html:text/html}
}

@inbook{inbook,
  author = {Kulkarni, Anand and Krishnasamy, Ganesh and Abraham, Ajith},
  year   = {2017},
  month  = {09},
  pages  = {1-7},
  title  = {Introduction to Optimization},
  volume = {114},
  isbn   = {978-3-319-44253-2},
  doi    = {10.1007/978-3-319-44254-9_1}
}

@book{eiben2015,
  author    = {Eiben, A.E. and Smith, J.E.},
  title     = {Introduction to Evolutionary Computing},
  edition   = {2nd},
  series    = {Natural Computing Series},
  publisher = {Springer},
  year      = {2015},
  address   = {Berlin, Heidelberg},
  pages     = {30},
  doi       = {10.1007/978-3-662-44874-8},
  isbn      = {978-3-662-44873-1},
  note      = {S2CID 20912932}
}

@book{stone1972,
  author    = {Stone, James L.},
  title     = {Introduction to Computer Organization and Data Structures},
  year      = {1972},
  publisher = {McGraw-Hill},
  address   = {New York}
}

@book{johnjeffery_automata,
  title     = {Introduction to Automata Theory, Languages and Computation},
  author    = {John E. Hopcroft and Jeffrey D. Ullman},
  publisher = {Addison-Wesley Publishing Company},
  year      = {1979},
  series    = {Addison-Wesley Series in Computer Science},
  edition   = {1st}
}


@book{leeuwen_algorithms_1998,
  address   = {Amsterdam},
  edition   = {1. MIT Press paperback ed., 2. printing},
  series    = {Handbook of theoretical computer science / ed. by {Jan}. van {Leeuwen}},
  title     = {Algorithms and complexity},
  isbn      = {978-0-262-22038-5},
  url       = {http://www.gbv.de/dms/bowker/toc/9780444880710.pdf},
  abstract  = {This first part presents chapters on models of computation, complexity theory, data structures, and efficient computation in many recognized sub-disciplines of Theoretical Computer Science.},
  language  = {eng},
  urldate   = {2024-04-20},
  publisher = {Elsevier},
  author    = {Leeuwen, Jan van},
  year      = {1998},
  note      = {OCLC: 247934368}
}

@book{shalev2014understanding,
  title     = {Understanding Machine Learning: From Theory to Algorithms},
  author    = {Shalev-Shwartz, Shai and Ben-David, Shai},
  year      = {2014},
  publisher = {CUP},
  isbn      = {978-3-319-44253-2}
}

@misc{venkat2018curse,
  author = {Venkat, Naveen},
  year   = {2018},
  month  = {09},
  title  = {The Curse of Dimensionality: Inside Out},
  doi    = {10.13140/RG.2.2.29631.36006}
}

@book{bellman1957dynamic,
  title     = {Dynamic Programming},
  author    = {Bellman, Richard},
  publisher = {Princeton Univ Pr},
  year      = {1957},
  isbn      = {978-0691079516},
  url       = {http://libgen.li/file.php?md5=3a1794f608b48cbd4bce640735af75d2}
}

@misc{peng_interpreting_2024,
  title    = {Interpreting the {Curse} of {Dimensionality} from {Distance} {Concentration} and {Manifold} {Effect}},
  url      = {http://arxiv.org/abs/2401.00422},
  doi      = {10.48550/arXiv.2401.00422},
  abstract = {The characteristics of data like distribution and heterogeneity, become more complex and counterintuitive as the dimensionality increases. This phenomenon is known as curse of dimensionality, where common patterns and relationships (e.g., internal and boundary pattern) that hold in low-dimensional space may be invalid in higher-dimensional space. It leads to a decreasing performance for the regression, classification or clustering models or algorithms. Curse of dimensionality can be attributed to many causes. In this paper, we first summarize five challenges associated with manipulating high-dimensional data, and explains the potential causes for the failure of regression, classification or clustering tasks. Subsequently, we delve into two major causes of the curse of dimensionality, distance concentration and manifold effect, by performing theoretical and empirical analyses. The results demonstrate that nearest neighbor search (NNS) using three typical distance measurements, Minkowski distance, Chebyshev distance, and cosine distance, becomes meaningless as the dimensionality increases. Meanwhile, the data incorporates more redundant features, and the variance contribution of principal component analysis (PCA) is skewed towards a few dimensions. By interpreting the causes of the curse of dimensionality, we can better understand the limitations of current models and algorithms, and drive to improve the performance of data analysis and machine learning tasks in high-dimensional space.},
  language = {en-US},
  urldate  = {2024-04-20},
  author   = {Peng, Dehua and Gui, Zhipeng and Wu, Huayi},
  month    = jan,
  year     = {2024},
  note     = {arXiv:2401.00422 [cs]},
  keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning},
  annote   = {Comment: 17 pages, 11 figures},
  file     = {arXiv Fulltext PDF:/home/migue8gl/Zotero/storage/4WCAS47V/Peng et al. - 2024 - Interpreting the Curse of Dimensionality from Distance Concentration and Manifold Effect.pdf:application/pdf;arXiv.org Snapshot:/home/migue8gl/Zotero/storage/IP7LESZG/2401.html:text/html}
}

@book{bellman1961adaptive,
  title     = {Adaptive Control Processes},
  author    = {Bellman, Richard E.},
  year      = {1961},
  publisher = {Princeton University Press}
}

@inproceedings{beyer99nn,
  title     = {When Is ``Nearest Neighbor'' Meaningful?},
  author    = {Beyer, Kevin and Goldstein, Jonathan and Ramakrishnan, Raghu and Shaft, Uri},
  booktitle = {Proceedings of the 7th International Conference on Database Theory},
  year      = {1999},
  pages     = {217--235},
  publisher = {Springer},
  series    = {Lecture Notes in Computer Science},
  volume    = {1540},
  file      = {beyer99nn.pdf:papers\\beyer99nn.pdf:PDF},
  language  = {english},
  url       = {http://www.springerlink.com/link.asp?id=04p94cqnbge862kh}
}

@article{bianchi2009survey,
  title   = {A survey on metaheuristics for stochastic combinatorial optimization},
  author  = {Bianchi, Leonora and Dorigo, Marco and Gambardella, Luca Maria and Gutjahr, Walter J.},
  journal = {Natural Computing},
  volume  = {8},
  number  = {2},
  pages   = {239--287},
  year    = {2009},
  doi     = {10.1007/s11047-008-9098-4}
}

@inproceedings{xu2014exploration,
  title        = {Exploration-exploitation tradeoffs in metaheuristics: Survey and analysis},
  author       = {Xu, Jie and Zhang, Jun},
  booktitle    = {Proceedings of the 33rd Chinese control conference},
  year         = {2014},
  organization = {IEEE},
  pages        = {8633--8638}
}

@inproceedings{6896450,
  author    = {Xu, Junqin and Zhang, Jihui},
  booktitle = {Proceedings of the 33rd Chinese Control Conference},
  title     = {Exploration-exploitation tradeoffs in metaheuristics: Survey and analysis},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {8633-8638},
  keywords  = {Optimization;Sociology;Statistics;Heuristic algorithms;Algorithm design and analysis;Genetic algorithms;Evolutionary computation;Metaheuristics;Exploration;Exploitation;Human intelligence;Hard optimization problems;Complexity},
  doi       = {10.1109/ChiCC.2014.6896450}
}

@article{585893,
  author   = {Wolpert, D.H. and Macready, W.G.},
  journal  = {IEEE Transactions on Evolutionary Computation},
  title    = {No free lunch theorems for optimization},
  year     = {1997},
  volume   = {1},
  number   = {1},
  pages    = {67-82},
  keywords = {Iron;Evolutionary computation;Information theory;Minimax techniques;Simulated annealing;Algorithm design and analysis;Performance analysis;Probability distribution;Bayesian methods},
  doi      = {10.1109/4235.585893}
}

@misc{goldblum2023free,
  title         = {The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning},
  author        = {Micah Goldblum and Marc Finzi and Keefer Rowan and Andrew Gordon Wilson},
  year          = {2023},
  eprint        = {2304.05366},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@book{murphy2012machine,
  title     = {Machine Learning: A Probabilistic Perspective},
  author    = {Kevin P. Murphy},
  publisher = {The MIT Press},
  isbn      = {0262018020, 9780262018029},
  year      = {2012},
  series    = {Adaptive Computation and Machine Learning},
  edition   = {1},
  url       = {https://libgen.li/file.php?md5=8ecfeeb2e1f9a19c770fba1ff85fa566}
}

@misc{sah2020machine,
  author = {Sah, Shagan},
  year   = {2020},
  month  = {07},
  title  = {Machine Learning: A Review of Learning Types},
  doi    = {10.20944/preprints202007.0230.v1}
}

@misc{scikit-learn-svm,
  author       = {{scikit-learn Developers}},
  title        = {{Support Vector Machines — scikit-learn documentation}},
  howpublished = {\url{https://scikit-learn.org/stable/modules/svm.html}},
  year         = {2024},
  note         = {Archived from the original on 2017-11-08, Retrieved 2024-04-27}
}

@book{hastie2009elements,
  author    = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  title     = {The Elements of Statistical Learning : Data Mining, Inference, and Prediction},
  edition   = {Second},
  year      = {2008},
  publisher = {Springer},
  address   = {New York},
  pages     = {134}
}

@inproceedings{10.1007/978-3-540-39964-3_62,
  author    = {Guo, Gongde
               and Wang, Hui
               and Bell, David
               and Bi, Yaxin
               and Greer, Kieran},
  editor    = {Meersman, Robert
               and Tari, Zahir
               and Schmidt, Douglas C.},
  title     = {KNN Model-Based Approach in Classification},
  booktitle = {On The Move to Meaningful Internet Systems 2003: CoopIS, DOA, and ODBASE},
  year      = {2003},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {986--996},
  abstract  = {The k-Nearest-Neighbours (kNN) is a simple but effective method for classification. The major drawbacks with respect to kNN are (1) its low efficiency -- being a lazy learning method prohibits it in many applications such as dynamic web mining for a large repository, and (2) its dependency on the selection of a ``good value'' for k. In this paper, we propose a novel kNN type method for classification that is aimed at overcoming these shortcomings. Our method constructs a kNN model for the data, which replaces the data to serve as the basis of classification. The value of k is automatically determined, is varied for different data, and is optimal in terms of classification accuracy. The construction of the model reduces the dependency on k and makes classification faster. Experiments were carried out on some public datasets collected from the UCI machine learning repository in order to test our method. The experimental results show that the kNN based model compares well with C5.0 and kNN in terms of classification accuracy, but is more efficient than the standard kNN.},
  isbn      = {978-3-540-39964-3}
}

@book{10.5555/522098,
  author    = {Mitchell, Melanie},
  title     = {An  Introduction to Genetic Algorithms},
  year      = {1998},
  isbn      = {0262631857},
  publisher = {MIT Press},
  address   = {Cambridge, MA, USA},
  abstract  = {From the Publisher: "This is the best general book on Genetic Algorithms written to date. It covers background, history, and motivation; it selects important, informative examples of applications and discusses the use of Genetic Algorithms in scientific models; and it gives a good account of the status of the theory of Genetic Algorithms. Best of all the book presents its material in clear, straightforward, felicitous prose, accessible to anyone with a college-level scientific background. If you want a broad, solid understanding of Genetic Algorithms -- where they came from, what's being done with them, and where they are going -- this is the book. -- John H. Holland, Professor, Computer Science and Engineering, and Professor of Psychology, The University of Michigan; External Professor, the Santa Fe Institute. Genetic algorithms have been used in science and engineering as adaptive algorithms for solving practical problems and as computational models of natural evolutionary systems. This brief, accessible introduction describes some of the most interesting research in the field and also enables readers to implement and experiment with genetic algorithms on their own. It focuses in depth on a small set of important and interesting topics -- particularly in machine learning, scientific modeling, and artificial life -- and reviews a broad span of research, including the work of Mitchell and her colleagues. The descriptions of applications and modeling projects stretch beyond the strict boundaries of computer science to include dynamical systems theory, game theory, molecular biology, ecology, evolutionary biology, and population genetics, underscoring the exciting "general purpose" nature of genetic algorithms as search methods that can be employed across disciplines. An Introduction to Genetic Algorithms is accessible to students and researchers in any scientific discipline. It includes many thought and computer exercises that build on and reinforce the reader's understanding of the text. The first chapter introduces genetic algorithms and their terminology and describes two provocative applications in detail. The second and third chapters look at the use of genetic algorithms in machine learning (computer programs, data analysis and prediction, neural networks) and in scientific models (interactions among learning, evolution, and culture; sexual selection; ecosystems; evolutionary activity). Several approaches to the theory of genetic algorithms are discussed in depth in the fourth chapter. The fifth chapter takes up implementation, and the last chapter poses some currently unanswered questions and surveys prospects for the future of evolutionary computation.}
}

@article{mathew2012genetic,
  title   = {Genetic algorithm},
  author  = {Mathew, Tom V},
  journal = {Report submitted at IIT Bombay},
  volume  = {53},
  year    = {2012}
}

@article{mirjalili2019genetic,
  title     = {Genetic algorithm},
  author    = {Mirjalili, Seyedali and Mirjalili, Seyedali},
  journal   = {Evolutionary algorithms and neural networks: Theory and applications},
  pages     = {43--55},
  year      = {2019},
  publisher = {Springer}
}

@incollection{DAGDIA2020283,
  title     = {Chapter 15 - When Evolutionary Computing Meets Astro- and Geoinformatics},
  editor    = {Petr Škoda and Fathalrahman Adam},
  booktitle = {Knowledge Discovery in Big Data from Astronomy and Earth Observation},
  publisher = {Elsevier},
  pages     = {283-306},
  year      = {2020},
  isbn      = {978-0-12-819154-5},
  doi       = {https://doi.org/10.1016/B978-0-12-819154-5.00026-6},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780128191545000266},
  author    = {Zaineb Chelly Dagdia and Miroslav Mirchev},
  keywords  = {Evolutionary computation, Bio-inspired computing, Metaheuristics, Astroinformatics, Geoinformatics},
  abstract  = {Knowledge discovery from data typically includes solving some type of an optimization problem that can be efficiently addressed using algorithms belonging to the class of evolutionary and bio-inspired computation. In this chapter, we give an overview of the various kinds of evolutionary algorithms, such as genetic algorithms, evolutionary strategy, evolutionary and genetic programming, differential evolution, and coevolutionary algorithms, as well as several other bio-inspired approaches, like swarm intelligence and artificial immune systems. After elaborating on the methodology, we provide numerous examples of applications in astronomy and geoscience and show how these algorithms can be applied within a distributed environment, by making use of parallel computing, which is essential when dealing with Big Data.}
}

@misc{purduelecture,
  title        = {{Lecture 4: Real-Coded Genetic Algorithms}},
  howpublished = {Lecture notes},
  author       = {{Purdue University College of Engineering}},
  url          = {https://engineering.purdue.edu/~sudhoff/ee630/Lecture04.pdf},
  note         = {Accessed on April 27, 2024}
}

@article{miller_genetic_nodate,
  title    = {Genetic {Algorithms}, {Tournament} {Selection}, and the {Effects} of {Noise}},
  language = {en},
  author   = {Miller, Brad L},
  file     = {Miller - Genetic Algorithms, Tournament Selection, and the .pdf:/home/migue8gl/Zotero/storage/4VHER3RL/Miller - Genetic Algorithms, Tournament Selection, and the .pdf:application/pdf}
}

@article{kashef_advanced_2015,
  series   = {Advances in {Self}-{Organizing} {Maps} {Subtitle} of the special issue: {Selected} {Papers} from the {Workshop} on {Self}-{Organizing} {Maps} 2012 ({WSOM} 2012)},
  title    = {An advanced {ACO} algorithm for feature subset selection},
  volume   = {147},
  issn     = {0925-2312},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231214008601},
  doi      = {10.1016/j.neucom.2014.06.067},
  abstract = {Feature selection is an important task for data analysis and information retrieval processing, pattern classification systems, and data mining applications. It reduces the number of features by removing noisy, irrelevant and redundant data. In this paper, a novel feature selection algorithm based on Ant Colony Optimization (ACO), called Advanced Binary ACO (ABACO), is presented. Features are treated as graph nodes to construct a graph model and are fully connected to each other. In this graph, each node has two sub-nodes, one for selecting and the other for deselecting the feature. Ant colony algorithm is used to select nodes while ants should visit all features. The use of several statistical measures is examined as the heuristic function for visibility of the edges in the graph. At the end of a tour, each ant has a binary vector with the same length as the number of features, where 1 implies selecting and 0 implies deselecting the corresponding feature. The performance of proposed algorithm is compared to the performance of Binary Genetic Algorithm (BGA), Binary Particle Swarm Optimization (BPSO), CatfishBPSO, Improved Binary Gravitational Search Algorithm (IBGSA), and some prominent ACO-based algorithms on the task of feature selection on 12 well-known UCI datasets. Simulation results verify that the algorithm provides a suitable feature subset with good classification accuracy using a smaller feature set than competing feature selection methods.},
  language = {en-US},
  urldate  = {2024-02-07},
  journal  = {Neurocomputing},
  author   = {Kashef, Shima and Nezamabadi-pour, Hossein},
  month    = jan,
  year     = {2015},
  keywords = {Classification, Feature selection, Ant colony optimization (ACO), Binary ACO, Wrapper},
  pages    = {271--279},
  file     = {Kashef and Nezamabadi-pour - 2015 - An advanced ACO algorithm for feature subset selec.pdf:/home/migue8gl/Zotero/storage/ZZLQUR26/Kashef and Nezamabadi-pour - 2015 - An advanced ACO algorithm for feature subset selec.pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/NLD3NWL9/S0925231214008601.html:text/html}
}