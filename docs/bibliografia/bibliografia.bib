@misc{udacity2015curse,
  author = {Udacity},
  title = {Curse of Dimensionality - Georgia Tech - Machine Learning},
  year = {2015},
  month = feb,
  howpublished = {Retrieved 2022-06-29}
}


@article{miao_survey_2016,
	series = {Promoting {Business} {Analytics} and {Quantitative} {Management} of {Technology}: 4th {International} {Conference} on {Information} {Technology} and {Quantitative} {Management} ({ITQM} 2016)},
	title = {A {Survey} on {Feature} {Selection}},
	volume = {91},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050916313047},
	doi = {10.1016/j.procs.2016.07.111},
	abstract = {Feature selection, as a dimensionality reduction technique, aims to choosing a small subset of the relevant features from the original features by removing irrelevant, redundant or noisy features. Feature selection usually can lead to better learning performance, i.e., higher learning accuracy, lower computational cost, and better model interpretability. Recently, researchers from computer vision, text mining and so on have proposed a variety of feature selection algorithms and in terms of theory and experiment, show the effectiveness of their works. This paper is aimed at reviewing the state of the art on these techniques. Furthermore, a thorough experiment is conducted to check if the use of feature selection can improve the performance of learning, considering some of the approaches mentioned in the literature. The experimental results show that unsupervised feature selection algorithms benefits machine learning tasks improving the performance of clustering.},
	urldate = {2024-04-02},
	journal = {Procedia Computer Science},
	author = {Miao, Jianyu and Niu, Lingfeng},
	month = jan,
	year = {2016},
	keywords = {clustering, feature selection, machine learning, unsupervised},
	pages = {919--926},
	file = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/H3KRFFAZ/Miao and Niu - 2016 - A Survey on Feature Selection.pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/PVAA7HS6/S1877050916313047.html:text/html},
}

@book{Mostafa2012,
  added-at = {2019-10-11T10:10:38.000+0200},
  author = {Abu-Mostafa, Yaser S. and Magdon-Ismail, Malik and Lin, Hsuan-Tien},
  biburl = {https://www.bibsonomy.org/bibtex/2079c807902a5b01cf801a8c7cec519ed/lopusz_kdd},
  description = {Learning From Data: Yaser S. Abu-Mostafa, Malik Magdon-Ismail, Hsuan-Tien Lin: 9781600490064: Amazon.com: Books},
  interhash = {5665353d0134ff1dea4ae32e99335a76},
  intrahash = {079c807902a5b01cf801a8c7cec519ed},
  keywords = {general_machine_learning},
  publisher = {AMLBook},
  timestamp = {2019-10-12T23:47:07.000+0200},
  title = {Learning From Data},
  year = 2012
}

@inproceedings{kira_practical_1992,
	title = {A {Practical} {Approach} to {Feature} {Selection}},
	isbn = {978-1-55860-247-2},
	doi = {10.1016/B978-1-55860-247-2.50037-1},
	abstract = {In real-world concept learning problems, the representation of data often uses many features, only a few of which may be related to the target concept. In this situation, feature selection is important both to speed up learning and to improve concept quality. A new feature selection algorithm Relief uses a statistical method and avoids heuristic search. Relief requires linear time in the number of given features and the number of training instances regardless of the target concept to be learned. Although the algorithm does not necessarily find the smallest subset of features, the size tends to be small because only statistically relevant features are selected. This paper focuses on empirical test results in two artificial domains; the LED Display domain and the Parity domain with and without noise. Comparison with other feature selection algorithms shows Reliefs advantages in terms of learning time and the accuracy of the learned concept, suggesting Reliefs practicality. © 1992 Proceedings of the 9th International Workshop on Machine Learning, ICML 1992. All rights reserved.},
	language = {English},
	author = {Kira, K. and Rendell, L.A.},
	year = {1992},
	pages = {249--256},
	annote = {Cited By :2345},
	file = {Kira and Rendell - 1992 - A Practical Approach to Feature Selection.pdf:/home/migue8gl/Zotero/storage/4GZM4TLW/Kira and Rendell - 1992 - A Practical Approach to Feature Selection.pdf:application/pdf},
}


@article{ding_minimum_2005,
	title = {Minimum redundancy feature selection from microarray gene expression data},
	volume = {3},
	issn = {0219-7200},
	doi = {10.1142/S0219720005001004},
	abstract = {How to selecting a small subset out of the thousands of genes in microarray data is important for accurate classification of phenotypes. Widely used methods typically rank genes according to their differential expressions among phenotypes and pick the top-ranked genes. We observe that feature sets so obtained have certain redundancy and study methods to minimize it. We propose a minimum redundancy - maximum relevance (MRMR) feature selection framework. Genes selected via MRMR provide a more balanced coverage of the space and capture broader characteristics of phenotypes. They lead to significantly improved class predictions in extensive experiments on 6 gene expression data sets: NCI, Lymphoma, Lung, Child Leukemia, Leukemia, and Colon. Improvements are observed consistently among 4 classification methods: Naïve Bayes, Linear discriminant analysis, Logistic regression, and Support vector machines. © Imperial College Press.},
	language = {English},
	number = {2},
	journal = {Journal of Bioinformatics and Computational Biology},
	author = {Ding, C. and Peng, H.},
	year = {2005},
	keywords = {Cancer classification, Gene expression analysis, Gene selection, Naïve Bayes, SVM},
	pages = {185--205},
	annote = {Cited By :1780},
	file = {Ding and Peng - 2005 - Minimum redundancy feature selection from microarray gene expression data.pdf:/home/migue8gl/Zotero/storage/BSLFQ4LD/Ding and Peng - 2005 - Minimum redundancy feature selection from microarray gene expression data.pdf:application/pdf},
}


@article{cortes_support-vector_1995,
	title = {Support-vector networks},
	volume = {20},
	copyright = {http://www.springer.com/tdm},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00994018},
	doi = {10.1007/BF00994018},
	abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very highdimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
	language = {en},
	number = {3},
	urldate = {2024-04-02},
	journal = {Machine Learning},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	month = sep,
	year = {1995},
	pages = {273--297},
	file = {Cortes and Vapnik - 1995 - Support-vector networks.pdf:/home/migue8gl/Zotero/storage/S5FZIC4M/Cortes and Vapnik - 1995 - Support-vector networks.pdf:application/pdf},
}


@article{cover_nearest_1967,
	title = {Nearest neighbor pattern classification},
	volume = {13},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0018-9448, 1557-9654},
	url = {http://ieeexplore.ieee.org/document/1053964/},
	doi = {10.1109/TIT.1967.1053964},
	language = {en},
	number = {1},
	urldate = {2024-04-02},
	journal = {IEEE Transactions on Information Theory},
	author = {Cover, T. and Hart, P.},
	month = jan,
	year = {1967},
	pages = {21--27},
	file = {Cover and Hart - 1967 - Nearest neighbor pattern classification.pdf:/home/migue8gl/Zotero/storage/AM949IMF/Cover and Hart - 1967 - Nearest neighbor pattern classification.pdf:application/pdf},
}


@article{fix_discriminatory_1989,
	title = {Discriminatory {Analysis}. {Nonparametric} {Discrimination}: {Consistency} {Properties}},
	volume = {57},
	issn = {0306-7734},
	shorttitle = {Discriminatory {Analysis}. {Nonparametric} {Discrimination}},
	url = {https://www.jstor.org/stable/1403797},
	doi = {10.2307/1403797},
	number = {3},
	urldate = {2024-04-02},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	author = {Fix, Evelyn and Hodges, J. L.},
	year = {1989},
	note = {Publisher: [Wiley, International Statistical Institute (ISI)]},
	pages = {238--247},
	file = {JSTOR Full Text PDF:/home/migue8gl/Zotero/storage/AZUBG9M6/Fix and Hodges - 1989 - Discriminatory Analysis. Nonparametric Discrimination Consistency Properties.pdf:application/pdf},
}

@book{Clark1922,
  author = {Clark, Wallace and Polakov, Walter Nicholas and Trabold, Frank W},
  title = {The Gantt chart, a working tool of management},
  year = {1922},
  publisher = {The Ronald press company},
  address = {New York},
  keywords = {Industrial efficiency, Production scheduling, Graphic methods, Gantt charts},
  language = {English},
}

@online{payscale_barcelona,
  author = {PayScale},
  title = {Data Scientist Salary in Barcelona},
  year = {Year},
  url = {https://www.payscale.com/research/ES/Job=Data_Scientist/Salary/9b2d8f8e/Barcelona},
  urldate = {Date Accessed}
}
