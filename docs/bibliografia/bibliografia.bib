@misc{udacity2015curse,
  author       = {Udacity},
  title        = {Curse of Dimensionality - Georgia Tech - Machine Learning},
  year         = {2015},
  month        = feb,
  howpublished = {Retrieved 2022-06-29}
}


@article{miao_survey_2016,
  series   = {Promoting {Business} {Analytics} and {Quantitative} {Management} of {Technology}: 4th {International} {Conference} on {Information} {Technology} and {Quantitative} {Management} ({ITQM} 2016)},
  title    = {A {Survey} on {Feature} {Selection}},
  volume   = {91},
  issn     = {1877-0509},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050916313047},
  doi      = {10.1016/j.procs.2016.07.111},
  abstract = {Feature selection, as a dimensionality reduction technique, aims to choosing a small subset of the relevant features from the original features by removing irrelevant, redundant or noisy features. Feature selection usually can lead to better learning performance, i.e., higher learning accuracy, lower computational cost, and better model interpretability. Recently, researchers from computer vision, text mining and so on have proposed a variety of feature selection algorithms and in terms of theory and experiment, show the effectiveness of their works. This paper is aimed at reviewing the state of the art on these techniques. Furthermore, a thorough experiment is conducted to check if the use of feature selection can improve the performance of learning, considering some of the approaches mentioned in the literature. The experimental results show that unsupervised feature selection algorithms benefits machine learning tasks improving the performance of clustering.},
  urldate  = {2024-04-02},
  journal  = {Procedia Computer Science},
  author   = {Miao, Jianyu and Niu, Lingfeng},
  month    = jan,
  year     = {2016},
  keywords = {clustering, feature selection, machine learning, unsupervised},
  pages    = {919--926},
  file     = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/H3KRFFAZ/Miao and Niu - 2016 - A Survey on Feature Selection.pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/PVAA7HS6/S1877050916313047.html:text/html}
}

@book{Mostafa2012,
  title     = {Learning From Data},
  author    = {Abu-Mostafa, Yaser S. and Magdon-Ismail, Malik and Lin, Hsuan-Tien},
  keywords  = {general_machine_learning},
  publisher = {AMLBook},
  title     = {Learning From Data},
  year      = 2012
}

@inproceedings{kira_practical_1992,
  title    = {A {Practical} {Approach} to {Feature} {Selection}},
  isbn     = {978-1-55860-247-2},
  doi      = {10.1016/B978-1-55860-247-2.50037-1},
  abstract = {In real-world concept learning problems, the representation of data often uses many features, only a few of which may be related to the target concept. In this situation, feature selection is important both to speed up learning and to improve concept quality. A new feature selection algorithm Relief uses a statistical method and avoids heuristic search. Relief requires linear time in the number of given features and the number of training instances regardless of the target concept to be learned. Although the algorithm does not necessarily find the smallest subset of features, the size tends to be small because only statistically relevant features are selected. This paper focuses on empirical test results in two artificial domains; the LED Display domain and the Parity domain with and without noise. Comparison with other feature selection algorithms shows Reliefs advantages in terms of learning time and the accuracy of the learned concept, suggesting Reliefs practicality. © 1992 Proceedings of the 9th International Workshop on Machine Learning, ICML 1992. All rights reserved.},
  language = {English},
  author   = {Kira, K. and Rendell, L.A.},
  year     = {1992},
  pages    = {249--256},
  annote   = {Cited By :2345},
  file     = {Kira and Rendell - 1992 - A Practical Approach to Feature Selection.pdf:/home/migue8gl/Zotero/storage/4GZM4TLW/Kira and Rendell - 1992 - A Practical Approach to Feature Selection.pdf:application/pdf}
}


@article{ding_minimum_2005,
  title    = {Minimum redundancy feature selection from microarray gene expression data},
  volume   = {3},
  issn     = {0219-7200},
  doi      = {10.1142/S0219720005001004},
  abstract = {How to selecting a small subset out of the thousands of genes in microarray data is important for accurate classification of phenotypes. Widely used methods typically rank genes according to their differential expressions among phenotypes and pick the top-ranked genes. We observe that feature sets so obtained have certain redundancy and study methods to minimize it. We propose a minimum redundancy - maximum relevance (MRMR) feature selection framework. Genes selected via MRMR provide a more balanced coverage of the space and capture broader characteristics of phenotypes. They lead to significantly improved class predictions in extensive experiments on 6 gene expression data sets: NCI, Lymphoma, Lung, Child Leukemia, Leukemia, and Colon. Improvements are observed consistently among 4 classification methods: Naïve Bayes, Linear discriminant analysis, Logistic regression, and Support vector machines. © Imperial College Press.},
  language = {English},
  number   = {2},
  journal  = {Journal of Bioinformatics and Computational Biology},
  author   = {Ding, C. and Peng, H.},
  year     = {2005},
  keywords = {Cancer classification, Gene expression analysis, Gene selection, Naïve Bayes, SVM},
  pages    = {185--205},
  annote   = {Cited By :1780},
  file     = {Ding and Peng - 2005 - Minimum redundancy feature selection from microarray gene expression data.pdf:/home/migue8gl/Zotero/storage/BSLFQ4LD/Ding and Peng - 2005 - Minimum redundancy feature selection from microarray gene expression data.pdf:application/pdf}
}


@article{cortes_support-vector_1995,
  title    = {Support-vector networks},
  volume   = {20},
  url      = {http://link.springer.com/10.1007/BF00994018},
  doi      = {10.1007/BF00994018},
  abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very highdimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
  language = {en},
  number   = {3},
  urldate  = {2024-04-02},
  journal  = {Machine Learning},
  author   = {Cortes, Corinna and Vapnik, Vladimir},
  month    = sep,
  year     = {1995},
  pages    = {273--297},
  file     = {Cortes and Vapnik - 1995 - Support-vector networks.pdf:/home/migue8gl/Zotero/storage/S5FZIC4M/Cortes and Vapnik - 1995 - Support-vector networks.pdf:application/pdf}
}


@article{cover_nearest_1967,
  title    = {Nearest neighbor pattern classification},
  volume   = {13},
  url      = {http://ieeexplore.ieee.org/document/1053964/},
  doi      = {10.1109/TIT.1967.1053964},
  language = {en},
  number   = {1},
  urldate  = {2024-04-02},
  journal  = {IEEE Transactions on Information Theory},
  author   = {Cover, T. and Hart, P.},
  month    = jan,
  year     = {1967},
  pages    = {21--27},
  file     = {Cover and Hart - 1967 - Nearest neighbor pattern classification.pdf:/home/migue8gl/Zotero/storage/AM949IMF/Cover and Hart - 1967 - Nearest neighbor pattern classification.pdf:application/pdf}
}


@article{fix_discriminatory_1989,
  title      = {Discriminatory {Analysis}. {Nonparametric} {Discrimination}: {Consistency} {Properties}},
  volume     = {57},
  issn       = {0306-7734},
  shorttitle = {Discriminatory {Analysis}. {Nonparametric} {Discrimination}},
  url        = {https://www.jstor.org/stable/1403797},
  doi        = {10.2307/1403797},
  number     = {3},
  urldate    = {2024-04-02},
  journal    = {International Statistical Review / Revue Internationale de Statistique},
  author     = {Fix, Evelyn and Hodges, J. L.},
  year       = {1989},
  note       = {Publisher: [Wiley, International Statistical Institute (ISI)]},
  pages      = {238--247},
  file       = {JSTOR Full Text PDF:/home/migue8gl/Zotero/storage/AZUBG9M6/Fix and Hodges - 1989 - Discriminatory Analysis. Nonparametric Discrimination Consistency Properties.pdf:application/pdf}
}

@book{Clark1922,
  author    = {Clark, Wallace and Polakov, Walter Nicholas and Trabold, Frank W},
  title     = {The Gantt chart, a working tool of management},
  year      = {1922},
  publisher = {The Ronald press company},
  address   = {New York},
  keywords  = {Industrial efficiency, Production scheduling, Graphic methods, Gantt charts},
  language  = {English}
}

@article{woidasky_use_2021,
  title    = {Use pattern relevance for laptop repair and product lifetime},
  volume   = {288},
  issn     = {0959-6526},
  url      = {https://www.sciencedirect.com/science/article/pii/S0959652620354718},
  doi      = {10.1016/j.jclepro.2020.125425},
  abstract = {More than 166 million mobile computers worldwide are being sold annually. Their duration of use is not only influenced by the design and quality of components and production, but also by the user behaviour. In this descriptive study, the effects of user decisions on the duration of product life is discussed based on two surveys carried out in a university student environment in southwest Germany. Results show that use phase length expectation for the devices clearly exceed 5 years, but the actual use phase duration was found to be only about 80\% of this time span. Consequently, the use pattern for laptops was described covering the aspects of user knowledge, use and repair intensity, the motivation for and the mode of disposal. Results show that only a minor share of users reads the user manuals. Those who do so and adhere to their recommendations experience a clearly increased use phase length. During the entire product lifetime of mobile computers, about one third of all devices is being repaired. The most important repairs were battery replacements, power supply unit repairs, and housing repairs. After the end of their useful life, more than 60\% of the obsolete devices are still operational. Users pass on only 25\% of their obsolete devices for a second use phase, but almost 2/3 of all laptops are stored, and only 11\% are disposed of properly.},
  urldate  = {2024-04-16},
  journal  = {Journal of Cleaner Production},
  author   = {Woidasky, Jörg and Cetinkaya, Esra},
  month    = mar,
  year     = {2021},
  keywords = {Laptop, Obsolescence, Product lifetime, Repair, Students, Survey, Use pattern},
  pages    = {125425},
  file     = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/LPD5VSRU/Woidasky and Cetinkaya - 2021 - Use pattern relevance for laptop repair and product lifetime.pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/RRGSUU72/S0959652620354718.html:text/html}
}

@inbook{inbook,
  author = {Kulkarni, Anand and Krishnasamy, Ganesh and Abraham, Ajith},
  year   = {2017},
  month  = {09},
  pages  = {1-7},
  title  = {Introduction to Optimization},
  volume = {114},
  isbn   = {978-3-319-44253-2},
  doi    = {10.1007/978-3-319-44254-9_1}
}

@book{eiben2015,
  author    = {Eiben, A.E. and Smith, J.E.},
  title     = {Introduction to Evolutionary Computing},
  edition   = {2nd},
  series    = {Natural Computing Series},
  publisher = {Springer},
  year      = {2015},
  address   = {Berlin, Heidelberg},
  pages     = {30},
  doi       = {10.1007/978-3-662-44874-8},
  isbn      = {978-3-662-44873-1},
  note      = {S2CID 20912932}
}

@book{stone1972,
  author    = {Stone, James L.},
  title     = {Introduction to Computer Organization and Data Structures},
  year      = {1972},
  publisher = {McGraw-Hill},
  address   = {New York}
}

@book{johnjeffery_automata,
  title     = {Introduction to Automata Theory, Languages and Computation},
  author    = {John E. Hopcroft and Jeffrey D. Ullman},
  publisher = {Addison-Wesley Publishing Company},
  year      = {1979},
  series    = {Addison-Wesley Series in Computer Science},
  edition   = {1st}
}


@book{leeuwen_algorithms_1998,
  address   = {Amsterdam},
  edition   = {1. MIT Press paperback ed., 2. printing},
  series    = {Handbook of theoretical computer science / ed. by {Jan}. van {Leeuwen}},
  title     = {Algorithms and complexity},
  isbn      = {978-0-262-22038-5},
  url       = {http://www.gbv.de/dms/bowker/toc/9780444880710.pdf},
  abstract  = {This first part presents chapters on models of computation, complexity theory, data structures, and efficient computation in many recognized sub-disciplines of Theoretical Computer Science.},
  language  = {eng},
  urldate   = {2024-04-20},
  publisher = {Elsevier},
  author    = {Leeuwen, Jan van},
  year      = {1998},
  note      = {OCLC: 247934368}
}

@book{shalev2014understanding,
  title     = {Understanding Machine Learning: From Theory to Algorithms},
  author    = {Shalev-Shwartz, Shai and Ben-David, Shai},
  year      = {2014},
  publisher = {CUP},
  isbn      = {978-3-319-44253-2}
}

@misc{venkat2018curse,
  author = {Venkat, Naveen},
  year   = {2018},
  month  = {09},
  title  = {The Curse of Dimensionality: Inside Out},
  doi    = {10.13140/RG.2.2.29631.36006}
}

@book{bellman1957dynamic,
  title     = {Dynamic Programming},
  author    = {Bellman, Richard},
  publisher = {Princeton Univ Pr},
  year      = {1957},
  isbn      = {978-0691079516},
  url       = {http://libgen.li/file.php?md5=3a1794f608b48cbd4bce640735af75d2}
}

@misc{peng_interpreting_2024,
  title    = {Interpreting the {Curse} of {Dimensionality} from {Distance} {Concentration} and {Manifold} {Effect}},
  url      = {http://arxiv.org/abs/2401.00422},
  doi      = {10.48550/arXiv.2401.00422},
  abstract = {The characteristics of data like distribution and heterogeneity, become more complex and counterintuitive as the dimensionality increases. This phenomenon is known as curse of dimensionality, where common patterns and relationships (e.g., internal and boundary pattern) that hold in low-dimensional space may be invalid in higher-dimensional space. It leads to a decreasing performance for the regression, classification or clustering models or algorithms. Curse of dimensionality can be attributed to many causes. In this paper, we first summarize five challenges associated with manipulating high-dimensional data, and explains the potential causes for the failure of regression, classification or clustering tasks. Subsequently, we delve into two major causes of the curse of dimensionality, distance concentration and manifold effect, by performing theoretical and empirical analyses. The results demonstrate that nearest neighbor search (NNS) using three typical distance measurements, Minkowski distance, Chebyshev distance, and cosine distance, becomes meaningless as the dimensionality increases. Meanwhile, the data incorporates more redundant features, and the variance contribution of principal component analysis (PCA) is skewed towards a few dimensions. By interpreting the causes of the curse of dimensionality, we can better understand the limitations of current models and algorithms, and drive to improve the performance of data analysis and machine learning tasks in high-dimensional space.},
  language = {en-US},
  urldate  = {2024-04-20},
  author   = {Peng, Dehua and Gui, Zhipeng and Wu, Huayi},
  month    = jan,
  year     = {2024},
  note     = {arXiv:2401.00422 [cs]},
  keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning},
  annote   = {Comment: 17 pages, 11 figures},
  file     = {arXiv Fulltext PDF:/home/migue8gl/Zotero/storage/4WCAS47V/Peng et al. - 2024 - Interpreting the Curse of Dimensionality from Distance Concentration and Manifold Effect.pdf:application/pdf;arXiv.org Snapshot:/home/migue8gl/Zotero/storage/IP7LESZG/2401.html:text/html}
}

@book{bellman1961adaptive,
  title     = {Adaptive Control Processes},
  author    = {Bellman, Richard E.},
  year      = {1961},
  publisher = {Princeton University Press}
}

@inproceedings{beyer99nn,
  title     = {When Is ``Nearest Neighbor'' Meaningful?},
  author    = {Beyer, Kevin and Goldstein, Jonathan and Ramakrishnan, Raghu and Shaft, Uri},
  booktitle = {Proceedings of the 7th International Conference on Database Theory},
  year      = {1999},
  pages     = {217--235},
  publisher = {Springer},
  series    = {Lecture Notes in Computer Science},
  volume    = {1540},
  file      = {beyer99nn.pdf:papers\\beyer99nn.pdf:PDF},
  language  = {english},
  url       = {http://www.springerlink.com/link.asp?id=04p94cqnbge862kh}
}

@article{bianchi2009survey,
  title   = {A survey on metaheuristics for stochastic combinatorial optimization},
  author  = {Bianchi, Leonora and Dorigo, Marco and Gambardella, Luca Maria and Gutjahr, Walter J.},
  journal = {Natural Computing},
  volume  = {8},
  number  = {2},
  pages   = {239--287},
  year    = {2009},
  doi     = {10.1007/s11047-008-9098-4}
}

@inproceedings{xu2014exploration,
  title        = {Exploration-exploitation tradeoffs in metaheuristics: Survey and analysis},
  author       = {Xu, Jie and Zhang, Jun},
  booktitle    = {Proceedings of the 33rd Chinese control conference},
  year         = {2014},
  organization = {IEEE},
  pages        = {8633--8638}
}

@inproceedings{6896450,
  author    = {Xu, Junqin and Zhang, Jihui},
  booktitle = {Proceedings of the 33rd Chinese Control Conference},
  title     = {Exploration-exploitation tradeoffs in metaheuristics: Survey and analysis},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {8633-8638},
  keywords  = {Optimization;Sociology;Statistics;Heuristic algorithms;Algorithm design and analysis;Genetic algorithms;Evolutionary computation;Metaheuristics;Exploration;Exploitation;Human intelligence;Hard optimization problems;Complexity},
  doi       = {10.1109/ChiCC.2014.6896450}
}

@article{585893,
  author   = {Wolpert, D.H. and Macready, W.G.},
  journal  = {IEEE Transactions on Evolutionary Computation},
  title    = {No free lunch theorems for optimization},
  year     = {1997},
  volume   = {1},
  number   = {1},
  pages    = {67-82},
  keywords = {Iron;Evolutionary computation;Information theory;Minimax techniques;Simulated annealing;Algorithm design and analysis;Performance analysis;Probability distribution;Bayesian methods},
  doi      = {10.1109/4235.585893}
}

@misc{goldblum2023free,
  title         = {The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning},
  author        = {Micah Goldblum and Marc Finzi and Keefer Rowan and Andrew Gordon Wilson},
  year          = {2023},
  eprint        = {2304.05366},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@book{murphy2012machine,
  title     = {Machine Learning: A Probabilistic Perspective},
  author    = {Kevin P. Murphy},
  publisher = {The MIT Press},
  year      = {2012},
  series    = {Adaptive Computation and Machine Learning},
  edition   = {1},
  url       = {https://libgen.li/file.php?md5=8ecfeeb2e1f9a19c770fba1ff85fa566}
}

@misc{sah2020machine,
  author = {Sah, Shagan},
  year   = {2020},
  month  = {07},
  title  = {Machine Learning: A Review of Learning Types},
  doi    = {10.20944/preprints202007.0230.v1}
}

@misc{scikit-learn-svm,
  author       = {{scikit-learn Developers}},
  title        = {{Support Vector Machines — scikit-learn documentation}},
  howpublished = {\url{https://scikit-learn.org/stable/modules/svm.html}},
  year         = {2024},
  note         = {Archived from the original on 2017-11-08, Retrieved 2024-04-27}
}

@book{hastie2009elements,
  author    = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  title     = {The Elements of Statistical Learning : Data Mining, Inference, and Prediction},
  edition   = {Second},
  year      = {2008},
  publisher = {Springer},
  address   = {New York},
  pages     = {134}
}

@inproceedings{10.1007/978-3-540-39964-3_62,
  author    = {Guo, Gongde
               and Wang, Hui
               and Bell, David
               and Bi, Yaxin
               and Greer, Kieran},
  editor    = {Meersman, Robert
               and Tari, Zahir
               and Schmidt, Douglas C.},
  title     = {KNN Model-Based Approach in Classification},
  booktitle = {On The Move to Meaningful Internet Systems 2003: CoopIS, DOA, and ODBASE},
  year      = {2003},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {986--996},
  abstract  = {The k-Nearest-Neighbours (kNN) is a simple but effective method for classification. The major drawbacks with respect to kNN are (1) its low efficiency -- being a lazy learning method prohibits it in many applications such as dynamic web mining for a large repository, and (2) its dependency on the selection of a ``good value'' for k. In this paper, we propose a novel kNN type method for classification that is aimed at overcoming these shortcomings. Our method constructs a kNN model for the data, which replaces the data to serve as the basis of classification. The value of k is automatically determined, is varied for different data, and is optimal in terms of classification accuracy. The construction of the model reduces the dependency on k and makes classification faster. Experiments were carried out on some public datasets collected from the UCI machine learning repository in order to test our method. The experimental results show that the kNN based model compares well with C5.0 and kNN in terms of classification accuracy, but is more efficient than the standard kNN.},
  isbn      = {978-3-540-39964-3}
}

@book{10.5555/522098,
  author    = {Mitchell, Melanie},
  title     = {An  Introduction to Genetic Algorithms},
  year      = {1998},
  isbn      = {0262631857},
  publisher = {MIT Press},
  address   = {Cambridge, MA, USA},
  abstract  = {From the Publisher: "This is the best general book on Genetic Algorithms written to date. It covers background, history, and motivation; it selects important, informative examples of applications and discusses the use of Genetic Algorithms in scientific models; and it gives a good account of the status of the theory of Genetic Algorithms. Best of all the book presents its material in clear, straightforward, felicitous prose, accessible to anyone with a college-level scientific background. If you want a broad, solid understanding of Genetic Algorithms -- where they came from, what's being done with them, and where they are going -- this is the book. -- John H. Holland, Professor, Computer Science and Engineering, and Professor of Psychology, The University of Michigan; External Professor, the Santa Fe Institute. Genetic algorithms have been used in science and engineering as adaptive algorithms for solving practical problems and as computational models of natural evolutionary systems. This brief, accessible introduction describes some of the most interesting research in the field and also enables readers to implement and experiment with genetic algorithms on their own. It focuses in depth on a small set of important and interesting topics -- particularly in machine learning, scientific modeling, and artificial life -- and reviews a broad span of research, including the work of Mitchell and her colleagues. The descriptions of applications and modeling projects stretch beyond the strict boundaries of computer science to include dynamical systems theory, game theory, molecular biology, ecology, evolutionary biology, and population genetics, underscoring the exciting "general purpose" nature of genetic algorithms as search methods that can be employed across disciplines. An Introduction to Genetic Algorithms is accessible to students and researchers in any scientific discipline. It includes many thought and computer exercises that build on and reinforce the reader's understanding of the text. The first chapter introduces genetic algorithms and their terminology and describes two provocative applications in detail. The second and third chapters look at the use of genetic algorithms in machine learning (computer programs, data analysis and prediction, neural networks) and in scientific models (interactions among learning, evolution, and culture; sexual selection; ecosystems; evolutionary activity). Several approaches to the theory of genetic algorithms are discussed in depth in the fourth chapter. The fifth chapter takes up implementation, and the last chapter poses some currently unanswered questions and surveys prospects for the future of evolutionary computation.}
}

@article{mathew2012genetic,
  title   = {Genetic algorithm},
  author  = {Mathew, Tom V},
  journal = {Report submitted at IIT Bombay},
  volume  = {53},
  year    = {2012}
}

@article{mirjalili2019genetic,
  title     = {Genetic algorithm},
  author    = {Mirjalili, Seyedali and Mirjalili, Seyedali},
  journal   = {Evolutionary algorithms and neural networks: Theory and applications},
  pages     = {43--55},
  year      = {2019},
  publisher = {Springer}
}

@incollection{DAGDIA2020283,
  title     = {Chapter 15 - When Evolutionary Computing Meets Astro- and Geoinformatics},
  editor    = {Petr Škoda and Fathalrahman Adam},
  booktitle = {Knowledge Discovery in Big Data from Astronomy and Earth Observation},
  publisher = {Elsevier},
  pages     = {283-306},
  year      = {2020},
  isbn      = {978-0-12-819154-5},
  doi       = {https://doi.org/10.1016/B978-0-12-819154-5.00026-6},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780128191545000266},
  author    = {Zaineb Chelly Dagdia and Miroslav Mirchev},
  keywords  = {Evolutionary computation, Bio-inspired computing, Metaheuristics, Astroinformatics, Geoinformatics},
  abstract  = {Knowledge discovery from data typically includes solving some type of an optimization problem that can be efficiently addressed using algorithms belonging to the class of evolutionary and bio-inspired computation. In this chapter, we give an overview of the various kinds of evolutionary algorithms, such as genetic algorithms, evolutionary strategy, evolutionary and genetic programming, differential evolution, and coevolutionary algorithms, as well as several other bio-inspired approaches, like swarm intelligence and artificial immune systems. After elaborating on the methodology, we provide numerous examples of applications in astronomy and geoscience and show how these algorithms can be applied within a distributed environment, by making use of parallel computing, which is essential when dealing with Big Data.}
}

@misc{purduelecture,
  title        = {{Lecture 4: Real-Coded Genetic Algorithms}},
  howpublished = {Lecture notes},
  author       = {{Purdue University College of Engineering}},
  url          = {https://engineering.purdue.edu/~sudhoff/ee630/Lecture04.pdf},
  note         = {Accessed on April 27, 2024}
}

@article{miller_genetic_nodate,
  title    = {Genetic {Algorithms}, {Tournament} {Selection}, and the {Effects} of {Noise}},
  language = {en},
  author   = {Miller, Brad L},
  file     = {Miller - Genetic Algorithms, Tournament Selection, and the .pdf:/home/migue8gl/Zotero/storage/4VHER3RL/Miller - Genetic Algorithms, Tournament Selection, and the .pdf:application/pdf}
}

@article{kashef_advanced_2015,
  series   = {Advances in {Self}-{Organizing} {Maps} {Subtitle} of the special issue: {Selected} {Papers} from the {Workshop} on {Self}-{Organizing} {Maps} 2012 ({WSOM} 2012)},
  title    = {An advanced {ACO} algorithm for feature subset selection},
  volume   = {147},
  issn     = {0925-2312},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231214008601},
  doi      = {10.1016/j.neucom.2014.06.067},
  abstract = {Feature selection is an important task for data analysis and information retrieval processing, pattern classification systems, and data mining applications. It reduces the number of features by removing noisy, irrelevant and redundant data. In this paper, a novel feature selection algorithm based on Ant Colony Optimization (ACO), called Advanced Binary ACO (ABACO), is presented. Features are treated as graph nodes to construct a graph model and are fully connected to each other. In this graph, each node has two sub-nodes, one for selecting and the other for deselecting the feature. Ant colony algorithm is used to select nodes while ants should visit all features. The use of several statistical measures is examined as the heuristic function for visibility of the edges in the graph. At the end of a tour, each ant has a binary vector with the same length as the number of features, where 1 implies selecting and 0 implies deselecting the corresponding feature. The performance of proposed algorithm is compared to the performance of Binary Genetic Algorithm (BGA), Binary Particle Swarm Optimization (BPSO), CatfishBPSO, Improved Binary Gravitational Search Algorithm (IBGSA), and some prominent ACO-based algorithms on the task of feature selection on 12 well-known UCI datasets. Simulation results verify that the algorithm provides a suitable feature subset with good classification accuracy using a smaller feature set than competing feature selection methods.},
  language = {en-US},
  urldate  = {2024-02-07},
  journal  = {Neurocomputing},
  author   = {Kashef, Shima and Nezamabadi-pour, Hossein},
  month    = jan,
  year     = {2015},
  keywords = {Classification, Feature selection, Ant colony optimization (ACO), Binary ACO, Wrapper},
  pages    = {271--279},
  file     = {Kashef and Nezamabadi-pour - 2015 - An advanced ACO algorithm for feature subset selec.pdf:/home/migue8gl/Zotero/storage/ZZLQUR26/Kashef and Nezamabadi-pour - 2015 - An advanced ACO algorithm for feature subset selec.pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/NLD3NWL9/S0925231214008601.html:text/html}
}


@inproceedings{dorigo_ant_1999,
  title      = {Ant colony optimization: a new meta-heuristic},
  volume     = {2},
  shorttitle = {Ant colony optimization},
  url        = {https://ieeexplore.ieee.org/document/782657},
  doi        = {10.1109/CEC.1999.782657},
  abstract   = {Recently, a number of algorithms inspired by the foraging behavior of ant colonies have been applied to the solution of difficult discrete optimization problems. We put these algorithms in a common framework by defining the Ant Colony Optimization (ACO) meta-heuristic. A couple of paradigmatic examples of applications of these novel meta-heuristic are given, as well as a brief overview of existing applications.},
  language   = {en-US},
  urldate    = {2024-01-30},
  booktitle  = {Proceedings of the 1999 {Congress} on {Evolutionary} {Computation}-{CEC99} ({Cat}. {No}. {99TH8406})},
  author     = {Dorigo, M. and Di Caro, G.},
  month      = jul,
  year       = {1999},
  keywords   = {Ant colony optimization, Circuits, Cities and towns, Cost function, Routing, Time measurement, Traveling salesman problems, Vehicles},
  pages      = {1470--1477 Vol. 2},
  file       = {IEEE Xplore Abstract Record:/home/migue8gl/Zotero/storage/M4P2QDQR/782657.html:text/html;IEEE Xplore Full Text PDF:/home/migue8gl/Zotero/storage/FHR2H3UG/Dorigo and Di Caro - 1999 - Ant colony optimization a new meta-heuristic.pdf:application/pdf}
}


@inproceedings{kennedy_particle_1995,
  title     = {Particle swarm optimization},
  volume    = {4},
  url       = {https://ieeexplore.ieee.org/document/488968},
  doi       = {10.1109/ICNN.1995.488968},
  abstract  = {A concept for the optimization of nonlinear functions using particle swarm methodology is introduced. The evolution of several paradigms is outlined, and an implementation of one of the paradigms is discussed. Benchmark testing of the paradigm is described, and applications, including nonlinear function optimization and neural network training, are proposed. The relationships between particle swarm optimization and both artificial life and genetic algorithms are described.},
  language  = {en-US},
  urldate   = {2023-10-14},
  booktitle = {Proceedings of {ICNN}'95 - {International} {Conference} on {Neural} {Networks}},
  author    = {Kennedy, J. and Eberhart, R.},
  month     = nov,
  year      = {1995},
  pages     = {1942--1948 vol.4},
  file      = {IEEE Xplore Abstract Record:/home/migue8gl/Zotero/storage/MXU25Q68/488968.html:text/html;IEEE Xplore Full Text PDF:/home/migue8gl/Zotero/storage/72554G6C/Kennedy and Eberhart - 1995 - Particle swarm optimization.pdf:application/pdf}
}


@article{storn_differential_1997,
  title    = {Differential {Evolution} – {A} {Simple} and {Efficient} {Heuristic} for global {Optimization} over {Continuous} {Spaces}},
  volume   = {11},
  issn     = {1573-2916},
  url      = {https://doi.org/10.1023/A:1008202821328},
  doi      = {10.1023/A:1008202821328},
  abstract = {A new heuristic approach for minimizing possiblynonlinear and non-differentiable continuous spacefunctions is presented. By means of an extensivetestbed it is demonstrated that the new methodconverges faster and with more certainty than manyother acclaimed global optimization methods. The newmethod requires few control variables, is robust, easyto use, and lends itself very well to parallelcomputation.},
  language = {en},
  number   = {4},
  urldate  = {2024-03-25},
  journal  = {Journal of Global Optimization},
  author   = {Storn, Rainer and Price, Kenneth},
  month    = dec,
  year     = {1997},
  keywords = {evolution strategy, genetic algorithm, global optimization, nonlinear optimization, Stochastic optimization},
  pages    = {341--359},
  file     = {Full Text PDF:/home/migue8gl/Zotero/storage/WP8D2ZSD/Storn and Price - 1997 - Differential Evolution – A Simple and Efficient Heuristic for global Optimization over Continuous Sp.pdf:application/pdf}
}

@book{10.5555/1557464,
  author    = {Engelbrecht, Andries P.},
  title     = {Computational Intelligence: An Introduction},
  year      = {2007},
  isbn      = {0470035617},
  publisher = {Wiley Publishing},
  edition   = {2nd},
  abstract  = {Computational Intelligence: An Introduction, Second Edition offers an in-depth exploration into the adaptive mechanisms that enable intelligent behaviour in complex and changing environments. The main focus of this text is centred on the computational modelling of biological and natural intelligent systems, encompassing swarm intelligence, fuzzy systems, artificial neutral networks, artificial immune systems and evolutionary computation. Engelbrecht provides readers with a wide knowledge of Computational Intelligence (CI) paradigms and algorithms; inviting readers to implement and problem solve real-world, complex problems within the CI development framework. This implementation framework will enable readers to tackle new problems without any difficulty through a single Java class as part of the CI library. Key features of this second edition include: A tutorial, hands-on based presentation of the material. State-of-the-art coverage of the most recent developments in computational intelligence with more elaborate discussions on intelligence and artificial intelligence (AI). New discussion of Darwinian evolution versus Lamarckian evolution, also including swarm robotics, hybrid systems and artificial immune systems. A section on how to perform empirical studies; topics including statistical analysis of stochastic algorithms, and an open source library of CI algorithms. Tables, illustrations, graphs, examples, assignments, Java code implementing the algorithms, and a complete CI implementation and experimental framework. Computational Intelligence: An Introduction, Second Edition is essential reading for third and fourth year undergraduate and postgraduate students studying CI. The first edition has been prescribed by a number of overseas universities and is thus a valuable teaching tool. In addition, it will also be a useful resource for researchers in Computational Intelligence and Artificial Intelligence, as well as engineers, statisticians, operational researchers, and bioinformaticians with an interest in applying AI or CI to solve problems in their domains. Check out http://www.ci.cs.up.ac.za for examples, assignments and Java code implementing the algorithms.}
}

@article{genetic-drift,
  author = {Price, Kenneth},
  year   = {2008},
  month  = {01},
  pages  = {},
  title  = {Eliminating Drift Bias from the Differential Evolution Algorithm},
  volume = {143},
  isbn   = {978-3-540-68827-3},
  doi    = {10.1007/978-3-540-68830-3_2}
}

@article{karaboga_idea_nodate,
  title    = {{AN} {IDEA} {BASED} {ON} {HONEY} {BEE} {SWARM} {FOR} {NUMERICAL} {OPTIMIZATION}},
  language = {en},
  author   = {Karaboga, Dervis},
  file     = {Karaboga - AN IDEA BASED ON HONEY BEE SWARM FOR NUMERICAL OPT.pdf:/home/migue8gl/Zotero/storage/8WJFAE6Z/Karaboga - AN IDEA BASED ON HONEY BEE SWARM FOR NUMERICAL OPT.pdf:application/pdf}
}

@article{saremi_grasshopper_2017,
  title      = {Grasshopper {Optimisation} {Algorithm}: {Theory} and application},
  volume     = {105},
  issn       = {0965-9978},
  shorttitle = {Grasshopper {Optimisation} {Algorithm}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0965997816305646},
  doi        = {10.1016/j.advengsoft.2017.01.004},
  abstract   = {This paper proposes an optimisation algorithm called Grasshopper Optimisation Algorithm (GOA) and applies it to challenging problems in structural optimisation. The proposed algorithm mathematically models and mimics the behaviour of grasshopper swarms in nature for solving optimisation problems. The GOA algorithm is first benchmarked on a set of test problems including CEC2005 to test and verify its performance qualitatively and quantitatively. It is then employed to find the optimal shape for a 52-bar truss, 3-bar truss, and cantilever beam to demonstrate its applicability. The results show that the proposed algorithm is able to provide superior results compared to well-known and recent algorithms in the literature. The results of the real applications also prove the merits of GOA in solving real problems with unknown search spaces.},
  language   = {en-US},
  urldate    = {2023-11-04},
  journal    = {Advances in Engineering Software},
  author     = {Saremi, Shahrzad and Mirjalili, Seyedali and Lewis, Andrew},
  month      = mar,
  year       = {2017},
  keywords   = {Algorithm, Benchmark, Constrained optimization, Heuristic algorithm, Optimization, Metaheuristics, Optimization techniques},
  pages      = {30--47},
  file       = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/KTWESFVS/Saremi et al. - 2017 - Grasshopper Optimisation Algorithm Theory and app.pdf:application/pdf}
}

@article{mirjalili_dragonfly_2016,
  title      = {Dragonfly algorithm: a new meta-heuristic optimization technique for solving single-objective, discrete, and multi-objective problems},
  volume     = {27},
  issn       = {1433-3058},
  shorttitle = {Dragonfly algorithm},
  url        = {https://doi.org/10.1007/s00521-015-1920-1},
  doi        = {10.1007/s00521-015-1920-1},
  abstract   = {A novel swarm intelligence optimization technique is proposed called dragonfly algorithm (DA). The main inspiration of the DA algorithm originates from the static and dynamic swarming behaviours of dragonflies in nature. Two essential phases of optimization, exploration and exploitation, are designed by modelling the social interaction of dragonflies in navigating, searching for foods, and avoiding enemies when swarming dynamically or statistically. The paper also considers the proposal of binary and multi-objective versions of DA called binary DA (BDA) and multi-objective DA (MODA), respectively. The proposed algorithms are benchmarked by several mathematical test functions and one real case study qualitatively and quantitatively. The results of DA and BDA prove that the proposed algorithms are able to improve the initial random population for a given problem, converge towards the global optimum, and provide very competitive results compared to other well-known algorithms in the literature. The results of MODA also show that this algorithm tends to find very accurate approximations of Pareto optimal solutions with high uniform distribution for multi-objective problems. The set of designs obtained for the submarine propeller design problem demonstrate the merits of MODA in solving challenging real problems with unknown true Pareto optimal front as well. Note that the source codes of the DA, BDA, and MODA algorithms are publicly available at http://www.alimirjalili.com/DA.html.},
  language   = {en},
  number     = {4},
  urldate    = {2023-11-06},
  journal    = {Neural Computing and Applications},
  author     = {Mirjalili, Seyedali},
  month      = may,
  year       = {2016},
  note       = {Number: 4},
  keywords   = {Benchmark, Constrained optimization, Genetic algorithm, Optimization, Particle swarm optimization, Multi-objective optimization, Binary optimization, Evolutionary algorithms, Swarm intelligence},
  pages      = {1053--1073},
  file       = {Full Text PDF:/home/migue8gl/Zotero/storage/VTNIPPGY/Mirjalili - 2016 - Dragonfly algorithm a new meta-heuristic optimiza.pdf:application/pdf}
}

@article{mirjalili_whale_2016,
  title    = {The {Whale} {Optimization} {Algorithm}},
  volume   = {95},
  issn     = {0965-9978},
  url      = {https://www.sciencedirect.com/science/article/pii/S0965997816300163},
  doi      = {10.1016/j.advengsoft.2016.01.008},
  abstract = {This paper proposes a novel nature-inspired meta-heuristic optimization algorithm, called Whale Optimization Algorithm (WOA), which mimics the social behavior of humpback whales. The algorithm is inspired by the bubble-net hunting strategy. WOA is tested with 29 mathematical optimization problems and 6 structural design problems. Optimization results prove that the WOA algorithm is very competitive compared to the state-of-art meta-heuristic algorithms as well as conventional methods. The source codes of the WOA algorithm are publicly available at http://www.alimirjalili.com/WOA.html},
  language = {en-US},
  urldate  = {2023-10-14},
  journal  = {Advances in Engineering Software},
  author   = {Mirjalili, Seyedali and Lewis, Andrew},
  month    = may,
  year     = {2016},
  keywords = {Algorithm, Benchmark, Constrained optimization, Genetic algorithm, Heuristic algorithm, Optimization, Particle swarm optimization, Structural optimization},
  pages    = {51--67},
  file     = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/ZTC5T2CZ/Mirjalili and Lewis - 2016 - The Whale Optimization Algorithm.pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/55KPQJ66/S0965997816300163.html:text/html}
}

@misc{yang_new_2010,
  title     = {A {New} {Metaheuristic} {Bat}-{Inspired} {Algorithm}},
  url       = {http://arxiv.org/abs/1004.4170},
  doi       = {10.48550/arXiv.1004.4170},
  abstract  = {Metaheuristic algorithms such as particle swarm optimization, firefly algorithm and harmony search are now becoming powerful methods for solving many tough optimization problems. In this paper, we propose a new metaheuristic method, the Bat Algorithm, based on the echolocation behaviour of bats. We also intend to combine the advantages of existing algorithms into the new bat algorithm. After a detailed formulation and explanation of its implementation, we will then compare the proposed algorithm with other existing algorithms, including genetic algorithms and particle swarm optimization. Simulations show that the proposed algorithm seems much superior to other algorithms, and further studies are also discussed.},
  language  = {en-US},
  urldate   = {2024-01-22},
  publisher = {arXiv},
  author    = {Yang, Xin-She},
  month     = apr,
  year      = {2010},
  note      = {Issue: arXiv:1004.4170
               arXiv:1004.4170 [physics]},
  keywords  = {Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control, Physics - Biological Physics, Physics - Computational Physics},
  annote    = {Comment: 10 pages, 2 figures},
  file      = {arXiv Fulltext PDF:/home/migue8gl/Zotero/storage/ZY9PHJR8/Yang - 2010 - A New Metaheuristic Bat-Inspired Algorithm.pdf:application/pdf;arXiv.org Snapshot:/home/migue8gl/Zotero/storage/U6UBCKSN/1004.html:text/html}
}

@article{mirjalili_grey_2014,
  title    = {Grey {Wolf} {Optimizer}},
  volume   = {69},
  issn     = {0965-9978},
  url      = {https://www.sciencedirect.com/science/article/pii/S0965997813001853},
  doi      = {10.1016/j.advengsoft.2013.12.007},
  abstract = {This work proposes a new meta-heuristic called Grey Wolf Optimizer (GWO) inspired by grey wolves (Canis lupus). The GWO algorithm mimics the leadership hierarchy and hunting mechanism of grey wolves in nature. Four types of grey wolves such as alpha, beta, delta, and omega are employed for simulating the leadership hierarchy. In addition, the three main steps of hunting, searching for prey, encircling prey, and attacking prey, are implemented. The algorithm is then benchmarked on 29 well-known test functions, and the results are verified by a comparative study with Particle Swarm Optimization (PSO), Gravitational Search Algorithm (GSA), Differential Evolution (DE), Evolutionary Programming (EP), and Evolution Strategy (ES). The results show that the GWO algorithm is able to provide very competitive results compared to these well-known meta-heuristics. The paper also considers solving three classical engineering design problems (tension/compression spring, welded beam, and pressure vessel designs) and presents a real application of the proposed method in the field of optical engineering. The results of the classical engineering design problems and real application prove that the proposed algorithm is applicable to challenging problems with unknown search spaces.},
  language = {en-US},
  urldate  = {2023-11-18},
  journal  = {Advances in Engineering Software},
  author   = {Mirjalili, Seyedali and Mirjalili, Seyed Mohammad and Lewis, Andrew},
  month    = mar,
  year     = {2014},
  keywords = {Constrained optimization, Heuristic algorithm, Optimization, Metaheuristics, Optimization techniques, GWO},
  pages    = {46--61},
  file     = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/N5Z899BT/Mirjalili et al. - 2014 - Grey Wolf Optimizer.pdf:application/pdf}
}

@incollection{yang_chapter_2014,
  address   = {Oxford},
  title     = {Chapter 8 - {Firefly} {Algorithms}},
  isbn      = {978-0-12-416743-8},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780124167438000087},
  abstract  = {Firefly algorithms (FA) appeared about five years ago, in 2008, and the literature has expanded dramatically with diverse applications. In this chapter, we introduce the standard firefly algorithm, then briefly review the variants, together with a selection of recent publications. We also analyze the characteristics of FA and try to answer the question of why FA is so efficient.},
  language  = {en-US},
  urldate   = {2024-01-23},
  booktitle = {Nature-{Inspired} {Optimization} {Algorithms}},
  publisher = {Elsevier},
  author    = {Yang, Xin-She},
  editor    = {Yang, Xin-She},
  month     = jan,
  year      = {2014},
  doi       = {10.1016/B978-0-12-416743-8.00008-7},
  keywords  = {Optimization, Attraction, Firefly algorithm, Metaheuristic, Multimodality, Nature-inspired},
  pages     = {111--127},
  file      = {ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/WII5DSR8/B9780124167438000087.html:text/html}
}

@misc{noauthor_levy_nodate,
  title   = {Lévy {Flight} - an overview {\textbar} {ScienceDirect} {Topics}},
  url     = {https://www.sciencedirect.com/topics/physics-and-astronomy/levy-flight},
  urldate = {2024-05-06},
  file    = {Lévy Flight - an overview | ScienceDirect Topics:/home/migue8gl/Zotero/storage/VTPWKHYC/levy-flight.html:text/html}
}

@misc{yang_cuckoo_2010,
  title     = {Cuckoo {Search} via {Levy} {Flights}},
  url       = {http://arxiv.org/abs/1003.1594},
  doi       = {10.48550/arXiv.1003.1594},
  abstract  = {In this paper, we intend to formulate a new metaheuristic algorithm, called Cuckoo Search (CS), for solving optimization problems. This algorithm is based on the obligate brood parasitic behaviour of some cuckoo species in combination with the Levy flight behaviour of some birds and fruit flies. We validate the proposed algorithm against test functions and then compare its performance with those of genetic algorithms and particle swarm optimization. Finally, we discuss the implication of the results and suggestion for further research.},
  language  = {en-US},
  urldate   = {2024-03-13},
  publisher = {arXiv},
  author    = {Yang, Xin-She and Deb, Suash},
  month     = mar,
  year      = {2010},
  note      = {arXiv:1003.1594 [math]
               version: 1},
  keywords  = {Mathematics - Optimization and Control, optimization and control},
  file      = {arXiv Fulltext PDF:/home/migue8gl/Zotero/storage/9RR8Y8M9/Yang and Deb - 2010 - Cuckoo Search via Levy Flights.pdf:application/pdf;arXiv.org Snapshot:/home/migue8gl/Zotero/storage/VF5UBB2Q/1003.html:text/html}
}

@inproceedings{as,
  author  = {Colorni, Alberto and Dorigo, Marco and Maniezzo, Vittorio},
  year    = {1991},
  month   = {01},
  pages   = {},
  title   = {Distributed Optimization by Ant Colonies},
  journal = {Proceedings of the First European Conference on Artificial Life}
}

@book{simon2013evolutionary,
  title     = {Evolutionary Optimization Algorithms},
  author    = {Simon, D.},
  isbn      = {9781118659502},
  url       = {https://books.google.es/books?id=gwUwIEPqk30C},
  year      = {2013},
  publisher = {Wiley}
}

@inproceedings{dorigo_ant_1999,
  title      = {Ant colony optimization: a new meta-heuristic},
  volume     = {2},
  shorttitle = {Ant colony optimization},
  url        = {https://ieeexplore.ieee.org/document/782657},
  doi        = {10.1109/CEC.1999.782657},
  abstract   = {Recently, a number of algorithms inspired by the foraging behavior of ant colonies have been applied to the solution of difficult discrete optimization problems. We put these algorithms in a common framework by defining the Ant Colony Optimization (ACO) meta-heuristic. A couple of paradigmatic examples of applications of these novel meta-heuristic are given, as well as a brief overview of existing applications.},
  language   = {en-US},
  urldate    = {2024-01-30},
  booktitle  = {Proceedings of the 1999 {Congress} on {Evolutionary} {Computation}-{CEC99} ({Cat}. {No}. {99TH8406})},
  author     = {Dorigo, M. and Di Caro, G.},
  month      = jul,
  year       = {1999},
  keywords   = {Ant colony optimization, Circuits, Cities and towns, Cost function, Routing, Time measurement, Traveling salesman problems, Vehicles},
  pages      = {1470--1477 Vol. 2},
  file       = {IEEE Xplore Abstract Record:/home/migue8gl/Zotero/storage/M4P2QDQR/782657.html:text/html;IEEE Xplore Full Text PDF:/home/migue8gl/Zotero/storage/FHR2H3UG/Dorigo and Di Caro - 1999 - Ant colony optimization a new meta-heuristic.pdf:application/pdf}
}

@book{Holland:1975,
  added-at = {2009-06-26T15:25:19.000+0200},
  address = {Ann Arbor, MI},
  author = {Holland, John H.},
  biburl = {https://www.bibsonomy.org/bibtex/2c29f3ed4d5bbe62a26e61a0e786e176a/butz},
  description = {diverse cognitive systems bib},
  interhash = {5556ff81387180d09ef7583ae68dd32f},
  intrahash = {c29f3ed4d5bbe62a26e61a0e786e176a},
  keywords = {imported},
  note = {second edition, 1992},
  publisher = {University of Michigan Press},
  timestamp = {2009-06-26T15:25:35.000+0200},
  title = {Adaptation in Natural and Artificial Systems},
  year = 1975
}

