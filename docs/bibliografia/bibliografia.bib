@misc{udacity2015curse,
  author       = {Udacity},
  title        = {Curse of Dimensionality - Georgia Tech - Machine Learning},
  year         = {2015},
  month        = feb,
  howpublished = {Retrieved 2022-06-29}
}


@article{miao_survey_2016,
  series   = {Promoting {Business} {Analytics} and {Quantitative} {Management} of {Technology}: 4th {International} {Conference} on {Information} {Technology} and {Quantitative} {Management} ({ITQM} 2016)},
  title    = {A {Survey} on {Feature} {Selection}},
  volume   = {91},
  issn     = {1877-0509},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050916313047},
  doi      = {10.1016/j.procs.2016.07.111},
  abstract = {Feature selection, as a dimensionality reduction technique, aims to choosing a small subset of the relevant features from the original features by removing irrelevant, redundant or noisy features. Feature selection usually can lead to better learning performance, i.e., higher learning accuracy, lower computational cost, and better model interpretability. Recently, researchers from computer vision, text mining and so on have proposed a variety of feature selection algorithms and in terms of theory and experiment, show the effectiveness of their works. This paper is aimed at reviewing the state of the art on these techniques. Furthermore, a thorough experiment is conducted to check if the use of feature selection can improve the performance of learning, considering some of the approaches mentioned in the literature. The experimental results show that unsupervised feature selection algorithms benefits machine learning tasks improving the performance of clustering.},
  urldate  = {2024-04-02},
  journal  = {Procedia Computer Science},
  author   = {Miao, Jianyu and Niu, Lingfeng},
  month    = jan,
  year     = {2016},
  keywords = {clustering, feature selection, machine learning, unsupervised},
  pages    = {919--926},
  file     = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/H3KRFFAZ/Miao and Niu - 2016 - A Survey on Feature Selection.pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/PVAA7HS6/S1877050916313047.html:text/html}
}

@book{Mostafa2012,
  title     = {Learning From Data},
  author    = {Abu-Mostafa, Yaser S. and Magdon-Ismail, Malik and Lin, Hsuan-Tien},
  keywords  = {general_machine_learning},
  publisher = {AMLBook},
  title     = {Learning From Data},
  year      = 2012
}

@inproceedings{kira_practical_1992,
  title    = {A {Practical} {Approach} to {Feature} {Selection}},
  isbn     = {978-1-55860-247-2},
  doi      = {10.1016/B978-1-55860-247-2.50037-1},
  abstract = {In real-world concept learning problems, the representation of data often uses many features, only a few of which may be related to the target concept. In this situation, feature selection is important both to speed up learning and to improve concept quality. A new feature selection algorithm Relief uses a statistical method and avoids heuristic search. Relief requires linear time in the number of given features and the number of training instances regardless of the target concept to be learned. Although the algorithm does not necessarily find the smallest subset of features, the size tends to be small because only statistically relevant features are selected. This paper focuses on empirical test results in two artificial domains; the LED Display domain and the Parity domain with and without noise. Comparison with other feature selection algorithms shows Reliefs advantages in terms of learning time and the accuracy of the learned concept, suggesting Reliefs practicality. © 1992 Proceedings of the 9th International Workshop on Machine Learning, ICML 1992. All rights reserved.},
  language = {English},
  author   = {Kira, K. and Rendell, L.A.},
  year     = {1992},
  pages    = {249--256},
  annote   = {Cited By :2345},
  file     = {Kira and Rendell - 1992 - A Practical Approach to Feature Selection.pdf:/home/migue8gl/Zotero/storage/4GZM4TLW/Kira and Rendell - 1992 - A Practical Approach to Feature Selection.pdf:application/pdf}
}


@article{ding_minimum_2005,
  title    = {Minimum redundancy feature selection from microarray gene expression data},
  volume   = {3},
  issn     = {0219-7200},
  doi      = {10.1142/S0219720005001004},
  abstract = {How to selecting a small subset out of the thousands of genes in microarray data is important for accurate classification of phenotypes. Widely used methods typically rank genes according to their differential expressions among phenotypes and pick the top-ranked genes. We observe that feature sets so obtained have certain redundancy and study methods to minimize it. We propose a minimum redundancy - maximum relevance (MRMR) feature selection framework. Genes selected via MRMR provide a more balanced coverage of the space and capture broader characteristics of phenotypes. They lead to significantly improved class predictions in extensive experiments on 6 gene expression data sets: NCI, Lymphoma, Lung, Child Leukemia, Leukemia, and Colon. Improvements are observed consistently among 4 classification methods: Naïve Bayes, Linear discriminant analysis, Logistic regression, and Support vector machines. © Imperial College Press.},
  language = {English},
  number   = {2},
  journal  = {Journal of Bioinformatics and Computational Biology},
  author   = {Ding, C. and Peng, H.},
  year     = {2005},
  keywords = {Cancer classification, Gene expression analysis, Gene selection, Naïve Bayes, SVM},
  pages    = {185--205},
  annote   = {Cited By :1780},
  file     = {Ding and Peng - 2005 - Minimum redundancy feature selection from microarray gene expression data.pdf:/home/migue8gl/Zotero/storage/BSLFQ4LD/Ding and Peng - 2005 - Minimum redundancy feature selection from microarray gene expression data.pdf:application/pdf}
}


@article{cortes_support-vector_1995,
  title    = {Support-vector networks},
  volume   = {20},
  url      = {http://link.springer.com/10.1007/BF00994018},
  doi      = {10.1007/BF00994018},
  abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very highdimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
  language = {en},
  number   = {3},
  urldate  = {2024-04-02},
  journal  = {Machine Learning},
  author   = {Cortes, Corinna and Vapnik, Vladimir},
  month    = sep,
  year     = {1995},
  pages    = {273--297},
  file     = {Cortes and Vapnik - 1995 - Support-vector networks.pdf:/home/migue8gl/Zotero/storage/S5FZIC4M/Cortes and Vapnik - 1995 - Support-vector networks.pdf:application/pdf}
}


@article{cover_nearest_1967,
  title    = {Nearest neighbor pattern classification},
  volume   = {13},
  url      = {http://ieeexplore.ieee.org/document/1053964/},
  doi      = {10.1109/TIT.1967.1053964},
  language = {en},
  number   = {1},
  urldate  = {2024-04-02},
  journal  = {IEEE Transactions on Information Theory},
  author   = {Cover, T. and Hart, P.},
  month    = jan,
  year     = {1967},
  pages    = {21--27},
  file     = {Cover and Hart - 1967 - Nearest neighbor pattern classification.pdf:/home/migue8gl/Zotero/storage/AM949IMF/Cover and Hart - 1967 - Nearest neighbor pattern classification.pdf:application/pdf}
}


@article{fix_discriminatory_1989,
  title      = {Discriminatory {Analysis}. {Nonparametric} {Discrimination}: {Consistency} {Properties}},
  volume     = {57},
  issn       = {0306-7734},
  shorttitle = {Discriminatory {Analysis}. {Nonparametric} {Discrimination}},
  url        = {https://www.jstor.org/stable/1403797},
  doi        = {10.2307/1403797},
  number     = {3},
  urldate    = {2024-04-02},
  journal    = {International Statistical Review / Revue Internationale de Statistique},
  author     = {Fix, Evelyn and Hodges, J. L.},
  year       = {1989},
  note       = {Publisher: [Wiley, International Statistical Institute (ISI)]},
  pages      = {238--247},
  file       = {JSTOR Full Text PDF:/home/migue8gl/Zotero/storage/AZUBG9M6/Fix and Hodges - 1989 - Discriminatory Analysis. Nonparametric Discrimination Consistency Properties.pdf:application/pdf}
}

@book{Clark1922,
  author    = {Clark, Wallace and Polakov, Walter Nicholas and Trabold, Frank W},
  title     = {The Gantt chart, a working tool of management},
  year      = {1922},
  publisher = {The Ronald press company},
  address   = {New York},
  keywords  = {Industrial efficiency, Production scheduling, Graphic methods, Gantt charts},
  language  = {English}
}

@article{woidasky_use_2021,
  title    = {Use pattern relevance for laptop repair and product lifetime},
  volume   = {288},
  issn     = {0959-6526},
  url      = {https://www.sciencedirect.com/science/article/pii/S0959652620354718},
  doi      = {10.1016/j.jclepro.2020.125425},
  abstract = {More than 166 million mobile computers worldwide are being sold annually. Their duration of use is not only influenced by the design and quality of components and production, but also by the user behaviour. In this descriptive study, the effects of user decisions on the duration of product life is discussed based on two surveys carried out in a university student environment in southwest Germany. Results show that use phase length expectation for the devices clearly exceed 5 years, but the actual use phase duration was found to be only about 80\% of this time span. Consequently, the use pattern for laptops was described covering the aspects of user knowledge, use and repair intensity, the motivation for and the mode of disposal. Results show that only a minor share of users reads the user manuals. Those who do so and adhere to their recommendations experience a clearly increased use phase length. During the entire product lifetime of mobile computers, about one third of all devices is being repaired. The most important repairs were battery replacements, power supply unit repairs, and housing repairs. After the end of their useful life, more than 60\% of the obsolete devices are still operational. Users pass on only 25\% of their obsolete devices for a second use phase, but almost 2/3 of all laptops are stored, and only 11\% are disposed of properly.},
  urldate  = {2024-04-16},
  journal  = {Journal of Cleaner Production},
  author   = {Woidasky, Jörg and Cetinkaya, Esra},
  month    = mar,
  year     = {2021},
  keywords = {Laptop, Obsolescence, Product lifetime, Repair, Students, Survey, Use pattern},
  pages    = {125425},
  file     = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/LPD5VSRU/Woidasky and Cetinkaya - 2021 - Use pattern relevance for laptop repair and product lifetime.pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/RRGSUU72/S0959652620354718.html:text/html}
}

@inbook{inbook,
  author = {Kulkarni, Anand and Krishnasamy, Ganesh and Abraham, Ajith},
  year   = {2017},
  month  = {09},
  pages  = {1-7},
  title  = {Introduction to Optimization},
  volume = {114},
  isbn   = {978-3-319-44253-2},
  doi    = {10.1007/978-3-319-44254-9_1}
}

@book{eiben2015,
  author    = {Eiben, A.E. and Smith, J.E.},
  title     = {Introduction to Evolutionary Computing},
  edition   = {2nd},
  series    = {Natural Computing Series},
  publisher = {Springer},
  year      = {2015},
  address   = {Berlin, Heidelberg},
  pages     = {30},
  doi       = {10.1007/978-3-662-44874-8},
  isbn      = {978-3-662-44873-1},
  note      = {S2CID 20912932}
}

@book{stone1972,
  author    = {Stone, James L.},
  title     = {Introduction to Computer Organization and Data Structures},
  year      = {1972},
  publisher = {McGraw-Hill},
  address   = {New York}
}

@book{johnjeffery_automata,
  title     = {Introduction to Automata Theory, Languages and Computation},
  author    = {John E. Hopcroft and Jeffrey D. Ullman},
  publisher = {Addison-Wesley Publishing Company},
  year      = {1979},
  series    = {Addison-Wesley Series in Computer Science},
  edition   = {1st}
}


@book{leeuwen_algorithms_1998,
  address   = {Amsterdam},
  edition   = {1. MIT Press paperback ed., 2. printing},
  series    = {Handbook of theoretical computer science / ed. by {Jan}. van {Leeuwen}},
  title     = {Algorithms and complexity},
  isbn      = {978-0-262-22038-5},
  url       = {http://www.gbv.de/dms/bowker/toc/9780444880710.pdf},
  abstract  = {This first part presents chapters on models of computation, complexity theory, data structures, and efficient computation in many recognized sub-disciplines of Theoretical Computer Science.},
  language  = {eng},
  urldate   = {2024-04-20},
  publisher = {Elsevier},
  author    = {Leeuwen, Jan van},
  year      = {1998},
  note      = {OCLC: 247934368}
}

@book{shalev2014understanding,
  title     = {Understanding Machine Learning: From Theory to Algorithms},
  author    = {Shalev-Shwartz, Shai and Ben-David, Shai},
  year      = {2014},
  publisher = {CUP},
  isbn      = {978-3-319-44253-2}
}

@misc{venkat2018curse,
  author = {Venkat, Naveen},
  year   = {2018},
  month  = {09},
  title  = {The Curse of Dimensionality: Inside Out},
  doi    = {10.13140/RG.2.2.29631.36006}
}

@book{bellman1957dynamic,
  title     = {Dynamic Programming},
  author    = {Bellman, Richard},
  publisher = {Princeton Univ Pr},
  year      = {1957},
  isbn      = {978-0691079516},
  url       = {http://libgen.li/file.php?md5=3a1794f608b48cbd4bce640735af75d2}
}

@misc{peng_interpreting_2024,
  title    = {Interpreting the {Curse} of {Dimensionality} from {Distance} {Concentration} and {Manifold} {Effect}},
  url      = {http://arxiv.org/abs/2401.00422},
  doi      = {10.48550/arXiv.2401.00422},
  abstract = {The characteristics of data like distribution and heterogeneity, become more complex and counterintuitive as the dimensionality increases. This phenomenon is known as curse of dimensionality, where common patterns and relationships (e.g., internal and boundary pattern) that hold in low-dimensional space may be invalid in higher-dimensional space. It leads to a decreasing performance for the regression, classification or clustering models or algorithms. Curse of dimensionality can be attributed to many causes. In this paper, we first summarize five challenges associated with manipulating high-dimensional data, and explains the potential causes for the failure of regression, classification or clustering tasks. Subsequently, we delve into two major causes of the curse of dimensionality, distance concentration and manifold effect, by performing theoretical and empirical analyses. The results demonstrate that nearest neighbor search (NNS) using three typical distance measurements, Minkowski distance, Chebyshev distance, and cosine distance, becomes meaningless as the dimensionality increases. Meanwhile, the data incorporates more redundant features, and the variance contribution of principal component analysis (PCA) is skewed towards a few dimensions. By interpreting the causes of the curse of dimensionality, we can better understand the limitations of current models and algorithms, and drive to improve the performance of data analysis and machine learning tasks in high-dimensional space.},
  language = {en-US},
  urldate  = {2024-04-20},
  author   = {Peng, Dehua and Gui, Zhipeng and Wu, Huayi},
  month    = jan,
  year     = {2024},
  note     = {arXiv:2401.00422 [cs]},
  keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning},
  annote   = {Comment: 17 pages, 11 figures},
  file     = {arXiv Fulltext PDF:/home/migue8gl/Zotero/storage/4WCAS47V/Peng et al. - 2024 - Interpreting the Curse of Dimensionality from Distance Concentration and Manifold Effect.pdf:application/pdf;arXiv.org Snapshot:/home/migue8gl/Zotero/storage/IP7LESZG/2401.html:text/html}
}

@book{bellman1961adaptive,
  title     = {Adaptive Control Processes},
  author    = {Bellman, Richard E.},
  year      = {1961},
  publisher = {Princeton University Press}
}

@inproceedings{beyer99nn,
  title     = {When Is ``Nearest Neighbor'' Meaningful?},
  author    = {Beyer, Kevin and Goldstein, Jonathan and Ramakrishnan, Raghu and Shaft, Uri},
  booktitle = {Proceedings of the 7th International Conference on Database Theory},
  year      = {1999},
  pages     = {217--235},
  publisher = {Springer},
  series    = {Lecture Notes in Computer Science},
  volume    = {1540},
  file      = {beyer99nn.pdf:papers\\beyer99nn.pdf:PDF},
  language  = {english},
  url       = {http://www.springerlink.com/link.asp?id=04p94cqnbge862kh}
}

@article{bianchi2009survey,
  title   = {A survey on metaheuristics for stochastic combinatorial optimization},
  author  = {Bianchi, Leonora and Dorigo, Marco and Gambardella, Luca Maria and Gutjahr, Walter J.},
  journal = {Natural Computing},
  volume  = {8},
  number  = {2},
  pages   = {239--287},
  year    = {2009},
  doi     = {10.1007/s11047-008-9098-4}
}

@inproceedings{xu2014exploration,
  title        = {Exploration-exploitation tradeoffs in metaheuristics: Survey and analysis},
  author       = {Xu, Jie and Zhang, Jun},
  booktitle    = {Proceedings of the 33rd Chinese control conference},
  year         = {2014},
  organization = {IEEE},
  pages        = {8633--8638}
}

@inproceedings{6896450,
  author    = {Xu, Junqin and Zhang, Jihui},
  booktitle = {Proceedings of the 33rd Chinese Control Conference},
  title     = {Exploration-exploitation tradeoffs in metaheuristics: Survey and analysis},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {8633-8638},
  keywords  = {Optimization;Sociology;Statistics;Heuristic algorithms;Algorithm design and analysis;Genetic algorithms;Evolutionary computation;Metaheuristics;Exploration;Exploitation;Human intelligence;Hard optimization problems;Complexity},
  doi       = {10.1109/ChiCC.2014.6896450}
}

@article{585893,
  author   = {Wolpert, D.H. and Macready, W.G.},
  journal  = {IEEE Transactions on Evolutionary Computation},
  title    = {No free lunch theorems for optimization},
  year     = {1997},
  volume   = {1},
  number   = {1},
  pages    = {67-82},
  keywords = {Iron;Evolutionary computation;Information theory;Minimax techniques;Simulated annealing;Algorithm design and analysis;Performance analysis;Probability distribution;Bayesian methods},
  doi      = {10.1109/4235.585893}
}

@misc{goldblum2023free,
  title         = {The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning},
  author        = {Micah Goldblum and Marc Finzi and Keefer Rowan and Andrew Gordon Wilson},
  year          = {2023},
  eprint        = {2304.05366},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@book{murphy2012machine,
  title     = {Machine Learning: A Probabilistic Perspective},
  author    = {Kevin P. Murphy},
  publisher = {The MIT Press},
  year      = {2012},
  series    = {Adaptive Computation and Machine Learning},
  edition   = {1},
  url       = {https://libgen.li/file.php?md5=8ecfeeb2e1f9a19c770fba1ff85fa566}
}

@misc{sah2020machine,
  author = {Sah, Shagan},
  year   = {2020},
  month  = {07},
  title  = {Machine Learning: A Review of Learning Types},
  doi    = {10.20944/preprints202007.0230.v1}
}

@misc{scikit-learn-svm,
  author       = {{scikit-learn Developers}},
  title        = {{Support Vector Machines — scikit-learn documentation}},
  howpublished = {\url{https://scikit-learn.org/stable/modules/svm.html}},
  year         = {2024},
  note         = {Archived from the original on 2017-11-08, Retrieved 2024-04-27}
}

@book{hastie2009elements,
  author    = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  title     = {The Elements of Statistical Learning : Data Mining, Inference, and Prediction},
  edition   = {Second},
  year      = {2008},
  publisher = {Springer},
  address   = {New York},
  pages     = {134}
}

@inproceedings{10.1007/978-3-540-39964-3_62,
  author    = {Guo, Gongde
               and Wang, Hui
               and Bell, David
               and Bi, Yaxin
               and Greer, Kieran},
  editor    = {Meersman, Robert
               and Tari, Zahir
               and Schmidt, Douglas C.},
  title     = {KNN Model-Based Approach in Classification},
  booktitle = {On The Move to Meaningful Internet Systems 2003: CoopIS, DOA, and ODBASE},
  year      = {2003},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {986--996},
  abstract  = {The k-Nearest-Neighbours (kNN) is a simple but effective method for classification. The major drawbacks with respect to kNN are (1) its low efficiency -- being a lazy learning method prohibits it in many applications such as dynamic web mining for a large repository, and (2) its dependency on the selection of a ``good value'' for k. In this paper, we propose a novel kNN type method for classification that is aimed at overcoming these shortcomings. Our method constructs a kNN model for the data, which replaces the data to serve as the basis of classification. The value of k is automatically determined, is varied for different data, and is optimal in terms of classification accuracy. The construction of the model reduces the dependency on k and makes classification faster. Experiments were carried out on some public datasets collected from the UCI machine learning repository in order to test our method. The experimental results show that the kNN based model compares well with C5.0 and kNN in terms of classification accuracy, but is more efficient than the standard kNN.},
  isbn      = {978-3-540-39964-3}
}

@book{10.5555/522098,
  author    = {Mitchell, Melanie},
  title     = {An  Introduction to Genetic Algorithms},
  year      = {1998},
  isbn      = {0262631857},
  publisher = {MIT Press},
  address   = {Cambridge, MA, USA},
  abstract  = {From the Publisher: "This is the best general book on Genetic Algorithms written to date. It covers background, history, and motivation; it selects important, informative examples of applications and discusses the use of Genetic Algorithms in scientific models; and it gives a good account of the status of the theory of Genetic Algorithms. Best of all the book presents its material in clear, straightforward, felicitous prose, accessible to anyone with a college-level scientific background. If you want a broad, solid understanding of Genetic Algorithms -- where they came from, what's being done with them, and where they are going -- this is the book. -- John H. Holland, Professor, Computer Science and Engineering, and Professor of Psychology, The University of Michigan; External Professor, the Santa Fe Institute. Genetic algorithms have been used in science and engineering as adaptive algorithms for solving practical problems and as computational models of natural evolutionary systems. This brief, accessible introduction describes some of the most interesting research in the field and also enables readers to implement and experiment with genetic algorithms on their own. It focuses in depth on a small set of important and interesting topics -- particularly in machine learning, scientific modeling, and artificial life -- and reviews a broad span of research, including the work of Mitchell and her colleagues. The descriptions of applications and modeling projects stretch beyond the strict boundaries of computer science to include dynamical systems theory, game theory, molecular biology, ecology, evolutionary biology, and population genetics, underscoring the exciting "general purpose" nature of genetic algorithms as search methods that can be employed across disciplines. An Introduction to Genetic Algorithms is accessible to students and researchers in any scientific discipline. It includes many thought and computer exercises that build on and reinforce the reader's understanding of the text. The first chapter introduces genetic algorithms and their terminology and describes two provocative applications in detail. The second and third chapters look at the use of genetic algorithms in machine learning (computer programs, data analysis and prediction, neural networks) and in scientific models (interactions among learning, evolution, and culture; sexual selection; ecosystems; evolutionary activity). Several approaches to the theory of genetic algorithms are discussed in depth in the fourth chapter. The fifth chapter takes up implementation, and the last chapter poses some currently unanswered questions and surveys prospects for the future of evolutionary computation.}
}

@article{mathew2012genetic,
  title   = {Genetic algorithm},
  author  = {Mathew, Tom V},
  journal = {Report submitted at IIT Bombay},
  volume  = {53},
  year    = {2012}
}

@article{mirjalili2019genetic,
  title     = {Genetic algorithm},
  author    = {Mirjalili, Seyedali and Mirjalili, Seyedali},
  journal   = {Evolutionary algorithms and neural networks: Theory and applications},
  pages     = {43--55},
  year      = {2019},
  publisher = {Springer}
}

@incollection{DAGDIA2020283,
  title     = {Chapter 15 - When Evolutionary Computing Meets Astro- and Geoinformatics},
  editor    = {Petr Škoda and Fathalrahman Adam},
  booktitle = {Knowledge Discovery in Big Data from Astronomy and Earth Observation},
  publisher = {Elsevier},
  pages     = {283-306},
  year      = {2020},
  isbn      = {978-0-12-819154-5},
  doi       = {https://doi.org/10.1016/B978-0-12-819154-5.00026-6},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780128191545000266},
  author    = {Zaineb Chelly Dagdia and Miroslav Mirchev},
  keywords  = {Evolutionary computation, Bio-inspired computing, Metaheuristics, Astroinformatics, Geoinformatics},
  abstract  = {Knowledge discovery from data typically includes solving some type of an optimization problem that can be efficiently addressed using algorithms belonging to the class of evolutionary and bio-inspired computation. In this chapter, we give an overview of the various kinds of evolutionary algorithms, such as genetic algorithms, evolutionary strategy, evolutionary and genetic programming, differential evolution, and coevolutionary algorithms, as well as several other bio-inspired approaches, like swarm intelligence and artificial immune systems. After elaborating on the methodology, we provide numerous examples of applications in astronomy and geoscience and show how these algorithms can be applied within a distributed environment, by making use of parallel computing, which is essential when dealing with Big Data.}
}

@misc{purduelecture,
  title        = {{Lecture 4: Real-Coded Genetic Algorithms}},
  howpublished = {Lecture notes},
  author       = {{Purdue University College of Engineering}},
  url          = {https://engineering.purdue.edu/~sudhoff/ee630/Lecture04.pdf},
  note         = {Accessed on April 27, 2024}
}

@article{miller_genetic_nodate,
  title    = {Genetic {Algorithms}, {Tournament} {Selection}, and the {Effects} of {Noise}},
  language = {en},
  author   = {Miller, Brad L},
  file     = {Miller - Genetic Algorithms, Tournament Selection, and the .pdf:/home/migue8gl/Zotero/storage/4VHER3RL/Miller - Genetic Algorithms, Tournament Selection, and the .pdf:application/pdf}
}

@article{kashef_advanced_2015,
  series   = {Advances in {Self}-{Organizing} {Maps} {Subtitle} of the special issue: {Selected} {Papers} from the {Workshop} on {Self}-{Organizing} {Maps} 2012 ({WSOM} 2012)},
  title    = {An advanced {ACO} algorithm for feature subset selection},
  volume   = {147},
  issn     = {0925-2312},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231214008601},
  doi      = {10.1016/j.neucom.2014.06.067},
  abstract = {Feature selection is an important task for data analysis and information retrieval processing, pattern classification systems, and data mining applications. It reduces the number of features by removing noisy, irrelevant and redundant data. In this paper, a novel feature selection algorithm based on Ant Colony Optimization (ACO), called Advanced Binary ACO (ABACO), is presented. Features are treated as graph nodes to construct a graph model and are fully connected to each other. In this graph, each node has two sub-nodes, one for selecting and the other for deselecting the feature. Ant colony algorithm is used to select nodes while ants should visit all features. The use of several statistical measures is examined as the heuristic function for visibility of the edges in the graph. At the end of a tour, each ant has a binary vector with the same length as the number of features, where 1 implies selecting and 0 implies deselecting the corresponding feature. The performance of proposed algorithm is compared to the performance of Binary Genetic Algorithm (BGA), Binary Particle Swarm Optimization (BPSO), CatfishBPSO, Improved Binary Gravitational Search Algorithm (IBGSA), and some prominent ACO-based algorithms on the task of feature selection on 12 well-known UCI datasets. Simulation results verify that the algorithm provides a suitable feature subset with good classification accuracy using a smaller feature set than competing feature selection methods.},
  language = {en-US},
  urldate  = {2024-02-07},
  journal  = {Neurocomputing},
  author   = {Kashef, Shima and Nezamabadi-pour, Hossein},
  month    = jan,
  year     = {2015},
  keywords = {Classification, Feature selection, Ant colony optimization (ACO), Binary ACO, Wrapper},
  pages    = {271--279},
  file     = {Kashef and Nezamabadi-pour - 2015 - An advanced ACO algorithm for feature subset selec.pdf:/home/migue8gl/Zotero/storage/ZZLQUR26/Kashef and Nezamabadi-pour - 2015 - An advanced ACO algorithm for feature subset selec.pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/NLD3NWL9/S0925231214008601.html:text/html}
}


@inproceedings{dorigo_ant_1999,
  title      = {Ant colony optimization: a new meta-heuristic},
  volume     = {2},
  shorttitle = {Ant colony optimization},
  url        = {https://ieeexplore.ieee.org/document/782657},
  doi        = {10.1109/CEC.1999.782657},
  abstract   = {Recently, a number of algorithms inspired by the foraging behavior of ant colonies have been applied to the solution of difficult discrete optimization problems. We put these algorithms in a common framework by defining the Ant Colony Optimization (ACO) meta-heuristic. A couple of paradigmatic examples of applications of these novel meta-heuristic are given, as well as a brief overview of existing applications.},
  language   = {en-US},
  urldate    = {2024-01-30},
  booktitle  = {Proceedings of the 1999 {Congress} on {Evolutionary} {Computation}-{CEC99} ({Cat}. {No}. {99TH8406})},
  author     = {Dorigo, M. and Di Caro, G.},
  month      = jul,
  year       = {1999},
  keywords   = {Ant colony optimization, Circuits, Cities and towns, Cost function, Routing, Time measurement, Traveling salesman problems, Vehicles},
  pages      = {1470--1477 Vol. 2},
  file       = {IEEE Xplore Abstract Record:/home/migue8gl/Zotero/storage/M4P2QDQR/782657.html:text/html;IEEE Xplore Full Text PDF:/home/migue8gl/Zotero/storage/FHR2H3UG/Dorigo and Di Caro - 1999 - Ant colony optimization a new meta-heuristic.pdf:application/pdf}
}


@inproceedings{kennedy_particle_1995,
  title     = {Particle swarm optimization},
  volume    = {4},
  url       = {https://ieeexplore.ieee.org/document/488968},
  doi       = {10.1109/ICNN.1995.488968},
  abstract  = {A concept for the optimization of nonlinear functions using particle swarm methodology is introduced. The evolution of several paradigms is outlined, and an implementation of one of the paradigms is discussed. Benchmark testing of the paradigm is described, and applications, including nonlinear function optimization and neural network training, are proposed. The relationships between particle swarm optimization and both artificial life and genetic algorithms are described.},
  language  = {en-US},
  urldate   = {2023-10-14},
  booktitle = {Proceedings of {ICNN}'95 - {International} {Conference} on {Neural} {Networks}},
  author    = {Kennedy, J. and Eberhart, R.},
  month     = nov,
  year      = {1995},
  pages     = {1942--1948 vol.4},
  file      = {IEEE Xplore Abstract Record:/home/migue8gl/Zotero/storage/MXU25Q68/488968.html:text/html;IEEE Xplore Full Text PDF:/home/migue8gl/Zotero/storage/72554G6C/Kennedy and Eberhart - 1995 - Particle swarm optimization.pdf:application/pdf}
}


@article{storn_differential_1997,
  title    = {Differential {Evolution} – {A} {Simple} and {Efficient} {Heuristic} for global {Optimization} over {Continuous} {Spaces}},
  volume   = {11},
  issn     = {1573-2916},
  url      = {https://doi.org/10.1023/A:1008202821328},
  doi      = {10.1023/A:1008202821328},
  abstract = {A new heuristic approach for minimizing possiblynonlinear and non-differentiable continuous spacefunctions is presented. By means of an extensivetestbed it is demonstrated that the new methodconverges faster and with more certainty than manyother acclaimed global optimization methods. The newmethod requires few control variables, is robust, easyto use, and lends itself very well to parallelcomputation.},
  language = {en},
  number   = {4},
  urldate  = {2024-03-25},
  journal  = {Journal of Global Optimization},
  author   = {Storn, Rainer and Price, Kenneth},
  month    = dec,
  year     = {1997},
  keywords = {evolution strategy, genetic algorithm, global optimization, nonlinear optimization, Stochastic optimization},
  pages    = {341--359},
  file     = {Full Text PDF:/home/migue8gl/Zotero/storage/WP8D2ZSD/Storn and Price - 1997 - Differential Evolution – A Simple and Efficient Heuristic for global Optimization over Continuous Sp.pdf:application/pdf}
}

@book{10.5555/1557464,
  author    = {Engelbrecht, Andries P.},
  title     = {Computational Intelligence: An Introduction},
  year      = {2007},
  isbn      = {0470035617},
  publisher = {Wiley Publishing},
  edition   = {2nd},
  abstract  = {Computational Intelligence: An Introduction, Second Edition offers an in-depth exploration into the adaptive mechanisms that enable intelligent behaviour in complex and changing environments. The main focus of this text is centred on the computational modelling of biological and natural intelligent systems, encompassing swarm intelligence, fuzzy systems, artificial neutral networks, artificial immune systems and evolutionary computation. Engelbrecht provides readers with a wide knowledge of Computational Intelligence (CI) paradigms and algorithms; inviting readers to implement and problem solve real-world, complex problems within the CI development framework. This implementation framework will enable readers to tackle new problems without any difficulty through a single Java class as part of the CI library. Key features of this second edition include: A tutorial, hands-on based presentation of the material. State-of-the-art coverage of the most recent developments in computational intelligence with more elaborate discussions on intelligence and artificial intelligence (AI). New discussion of Darwinian evolution versus Lamarckian evolution, also including swarm robotics, hybrid systems and artificial immune systems. A section on how to perform empirical studies; topics including statistical analysis of stochastic algorithms, and an open source library of CI algorithms. Tables, illustrations, graphs, examples, assignments, Java code implementing the algorithms, and a complete CI implementation and experimental framework. Computational Intelligence: An Introduction, Second Edition is essential reading for third and fourth year undergraduate and postgraduate students studying CI. The first edition has been prescribed by a number of overseas universities and is thus a valuable teaching tool. In addition, it will also be a useful resource for researchers in Computational Intelligence and Artificial Intelligence, as well as engineers, statisticians, operational researchers, and bioinformaticians with an interest in applying AI or CI to solve problems in their domains. Check out http://www.ci.cs.up.ac.za for examples, assignments and Java code implementing the algorithms.}
}

@article{genetic-drift,
  author = {Price, Kenneth},
  year   = {2008},
  month  = {01},
  pages  = {},
  title  = {Eliminating Drift Bias from the Differential Evolution Algorithm},
  volume = {143},
  isbn   = {978-3-540-68827-3},
  doi    = {10.1007/978-3-540-68830-3_2}
}

@article{karaboga_idea_nodate,
  title    = {{AN} {IDEA} {BASED} {ON} {HONEY} {BEE} {SWARM} {FOR} {NUMERICAL} {OPTIMIZATION}},
  language = {en},
  author   = {Karaboga, Dervis},
  file     = {Karaboga - AN IDEA BASED ON HONEY BEE SWARM FOR NUMERICAL OPT.pdf:/home/migue8gl/Zotero/storage/8WJFAE6Z/Karaboga - AN IDEA BASED ON HONEY BEE SWARM FOR NUMERICAL OPT.pdf:application/pdf}
}

@article{saremi_grasshopper_2017,
  title      = {Grasshopper {Optimisation} {Algorithm}: {Theory} and application},
  volume     = {105},
  issn       = {0965-9978},
  shorttitle = {Grasshopper {Optimisation} {Algorithm}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0965997816305646},
  doi        = {10.1016/j.advengsoft.2017.01.004},
  abstract   = {This paper proposes an optimisation algorithm called Grasshopper Optimisation Algorithm (GOA) and applies it to challenging problems in structural optimisation. The proposed algorithm mathematically models and mimics the behaviour of grasshopper swarms in nature for solving optimisation problems. The GOA algorithm is first benchmarked on a set of test problems including CEC2005 to test and verify its performance qualitatively and quantitatively. It is then employed to find the optimal shape for a 52-bar truss, 3-bar truss, and cantilever beam to demonstrate its applicability. The results show that the proposed algorithm is able to provide superior results compared to well-known and recent algorithms in the literature. The results of the real applications also prove the merits of GOA in solving real problems with unknown search spaces.},
  language   = {en-US},
  urldate    = {2023-11-04},
  journal    = {Advances in Engineering Software},
  author     = {Saremi, Shahrzad and Mirjalili, Seyedali and Lewis, Andrew},
  month      = mar,
  year       = {2017},
  keywords   = {Algorithm, Benchmark, Constrained optimization, Heuristic algorithm, Optimization, Metaheuristics, Optimization techniques},
  pages      = {30--47},
  file       = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/KTWESFVS/Saremi et al. - 2017 - Grasshopper Optimisation Algorithm Theory and app.pdf:application/pdf}
}

@article{mirjalili_dragonfly_2016,
  title      = {Dragonfly algorithm: a new meta-heuristic optimization technique for solving single-objective, discrete, and multi-objective problems},
  volume     = {27},
  issn       = {1433-3058},
  shorttitle = {Dragonfly algorithm},
  url        = {https://doi.org/10.1007/s00521-015-1920-1},
  doi        = {10.1007/s00521-015-1920-1},
  abstract   = {A novel swarm intelligence optimization technique is proposed called dragonfly algorithm (DA). The main inspiration of the DA algorithm originates from the static and dynamic swarming behaviours of dragonflies in nature. Two essential phases of optimization, exploration and exploitation, are designed by modelling the social interaction of dragonflies in navigating, searching for foods, and avoiding enemies when swarming dynamically or statistically. The paper also considers the proposal of binary and multi-objective versions of DA called binary DA (BDA) and multi-objective DA (MODA), respectively. The proposed algorithms are benchmarked by several mathematical test functions and one real case study qualitatively and quantitatively. The results of DA and BDA prove that the proposed algorithms are able to improve the initial random population for a given problem, converge towards the global optimum, and provide very competitive results compared to other well-known algorithms in the literature. The results of MODA also show that this algorithm tends to find very accurate approximations of Pareto optimal solutions with high uniform distribution for multi-objective problems. The set of designs obtained for the submarine propeller design problem demonstrate the merits of MODA in solving challenging real problems with unknown true Pareto optimal front as well. Note that the source codes of the DA, BDA, and MODA algorithms are publicly available at http://www.alimirjalili.com/DA.html.},
  language   = {en},
  number     = {4},
  urldate    = {2023-11-06},
  journal    = {Neural Computing and Applications},
  author     = {Mirjalili, Seyedali},
  month      = may,
  year       = {2016},
  note       = {Number: 4},
  keywords   = {Benchmark, Constrained optimization, Genetic algorithm, Optimization, Particle swarm optimization, Multi-objective optimization, Binary optimization, Evolutionary algorithms, Swarm intelligence},
  pages      = {1053--1073},
  file       = {Full Text PDF:/home/migue8gl/Zotero/storage/VTNIPPGY/Mirjalili - 2016 - Dragonfly algorithm a new meta-heuristic optimiza.pdf:application/pdf}
}

@article{mirjalili_whale_2016,
  title    = {The {Whale} {Optimization} {Algorithm}},
  volume   = {95},
  issn     = {0965-9978},
  url      = {https://www.sciencedirect.com/science/article/pii/S0965997816300163},
  doi      = {10.1016/j.advengsoft.2016.01.008},
  abstract = {This paper proposes a novel nature-inspired meta-heuristic optimization algorithm, called Whale Optimization Algorithm (WOA), which mimics the social behavior of humpback whales. The algorithm is inspired by the bubble-net hunting strategy. WOA is tested with 29 mathematical optimization problems and 6 structural design problems. Optimization results prove that the WOA algorithm is very competitive compared to the state-of-art meta-heuristic algorithms as well as conventional methods. The source codes of the WOA algorithm are publicly available at http://www.alimirjalili.com/WOA.html},
  language = {en-US},
  urldate  = {2023-10-14},
  journal  = {Advances in Engineering Software},
  author   = {Mirjalili, Seyedali and Lewis, Andrew},
  month    = may,
  year     = {2016},
  keywords = {Algorithm, Benchmark, Constrained optimization, Genetic algorithm, Heuristic algorithm, Optimization, Particle swarm optimization, Structural optimization},
  pages    = {51--67},
  file     = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/ZTC5T2CZ/Mirjalili and Lewis - 2016 - The Whale Optimization Algorithm.pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/55KPQJ66/S0965997816300163.html:text/html}
}

@misc{yang_new_2010,
  title     = {A {New} {Metaheuristic} {Bat}-{Inspired} {Algorithm}},
  url       = {http://arxiv.org/abs/1004.4170},
  doi       = {10.48550/arXiv.1004.4170},
  abstract  = {Metaheuristic algorithms such as particle swarm optimization, firefly algorithm and harmony search are now becoming powerful methods for solving many tough optimization problems. In this paper, we propose a new metaheuristic method, the Bat Algorithm, based on the echolocation behaviour of bats. We also intend to combine the advantages of existing algorithms into the new bat algorithm. After a detailed formulation and explanation of its implementation, we will then compare the proposed algorithm with other existing algorithms, including genetic algorithms and particle swarm optimization. Simulations show that the proposed algorithm seems much superior to other algorithms, and further studies are also discussed.},
  language  = {en-US},
  urldate   = {2024-01-22},
  publisher = {arXiv},
  author    = {Yang, Xin-She},
  month     = apr,
  year      = {2010},
  note      = {Issue: arXiv:1004.4170
               arXiv:1004.4170 [physics]},
  keywords  = {Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control, Physics - Biological Physics, Physics - Computational Physics},
  annote    = {Comment: 10 pages, 2 figures},
  file      = {arXiv Fulltext PDF:/home/migue8gl/Zotero/storage/ZY9PHJR8/Yang - 2010 - A New Metaheuristic Bat-Inspired Algorithm.pdf:application/pdf;arXiv.org Snapshot:/home/migue8gl/Zotero/storage/U6UBCKSN/1004.html:text/html}
}

@article{mirjalili_grey_2014,
  title    = {Grey {Wolf} {Optimizer}},
  volume   = {69},
  issn     = {0965-9978},
  url      = {https://www.sciencedirect.com/science/article/pii/S0965997813001853},
  doi      = {10.1016/j.advengsoft.2013.12.007},
  abstract = {This work proposes a new meta-heuristic called Grey Wolf Optimizer (GWO) inspired by grey wolves (Canis lupus). The GWO algorithm mimics the leadership hierarchy and hunting mechanism of grey wolves in nature. Four types of grey wolves such as alpha, beta, delta, and omega are employed for simulating the leadership hierarchy. In addition, the three main steps of hunting, searching for prey, encircling prey, and attacking prey, are implemented. The algorithm is then benchmarked on 29 well-known test functions, and the results are verified by a comparative study with Particle Swarm Optimization (PSO), Gravitational Search Algorithm (GSA), Differential Evolution (DE), Evolutionary Programming (EP), and Evolution Strategy (ES). The results show that the GWO algorithm is able to provide very competitive results compared to these well-known meta-heuristics. The paper also considers solving three classical engineering design problems (tension/compression spring, welded beam, and pressure vessel designs) and presents a real application of the proposed method in the field of optical engineering. The results of the classical engineering design problems and real application prove that the proposed algorithm is applicable to challenging problems with unknown search spaces.},
  language = {en-US},
  urldate  = {2023-11-18},
  journal  = {Advances in Engineering Software},
  author   = {Mirjalili, Seyedali and Mirjalili, Seyed Mohammad and Lewis, Andrew},
  month    = mar,
  year     = {2014},
  keywords = {Constrained optimization, Heuristic algorithm, Optimization, Metaheuristics, Optimization techniques, GWO},
  pages    = {46--61},
  file     = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/N5Z899BT/Mirjalili et al. - 2014 - Grey Wolf Optimizer.pdf:application/pdf}
}

@incollection{yang_chapter_2014,
  address   = {Oxford},
  title     = {Chapter 8 - {Firefly} {Algorithms}},
  isbn      = {978-0-12-416743-8},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780124167438000087},
  abstract  = {Firefly algorithms (FA) appeared about five years ago, in 2008, and the literature has expanded dramatically with diverse applications. In this chapter, we introduce the standard firefly algorithm, then briefly review the variants, together with a selection of recent publications. We also analyze the characteristics of FA and try to answer the question of why FA is so efficient.},
  language  = {en-US},
  urldate   = {2024-01-23},
  booktitle = {Nature-{Inspired} {Optimization} {Algorithms}},
  publisher = {Elsevier},
  author    = {Yang, Xin-She},
  editor    = {Yang, Xin-She},
  month     = jan,
  year      = {2014},
  doi       = {10.1016/B978-0-12-416743-8.00008-7},
  keywords  = {Optimization, Attraction, Firefly algorithm, Metaheuristic, Multimodality, Nature-inspired},
  pages     = {111--127},
  file      = {ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/WII5DSR8/B9780124167438000087.html:text/html}
}

@misc{noauthor_levy_nodate,
  title   = {Lévy {Flight} - an overview {\textbar} {ScienceDirect} {Topics}},
  url     = {https://www.sciencedirect.com/topics/physics-and-astronomy/levy-flight},
  urldate = {2024-05-06},
  file    = {Lévy Flight - an overview | ScienceDirect Topics:/home/migue8gl/Zotero/storage/VTPWKHYC/levy-flight.html:text/html}
}

@misc{yang_cuckoo_2010,
  title     = {Cuckoo {Search} via {Levy} {Flights}},
  url       = {http://arxiv.org/abs/1003.1594},
  doi       = {10.48550/arXiv.1003.1594},
  abstract  = {In this paper, we intend to formulate a new metaheuristic algorithm, called Cuckoo Search (CS), for solving optimization problems. This algorithm is based on the obligate brood parasitic behaviour of some cuckoo species in combination with the Levy flight behaviour of some birds and fruit flies. We validate the proposed algorithm against test functions and then compare its performance with those of genetic algorithms and particle swarm optimization. Finally, we discuss the implication of the results and suggestion for further research.},
  language  = {en-US},
  urldate   = {2024-03-13},
  publisher = {arXiv},
  author    = {Yang, Xin-She and Deb, Suash},
  month     = mar,
  year      = {2010},
  note      = {arXiv:1003.1594 [math]
               version: 1},
  keywords  = {Mathematics - Optimization and Control, optimization and control},
  file      = {arXiv Fulltext PDF:/home/migue8gl/Zotero/storage/9RR8Y8M9/Yang and Deb - 2010 - Cuckoo Search via Levy Flights.pdf:application/pdf;arXiv.org Snapshot:/home/migue8gl/Zotero/storage/VF5UBB2Q/1003.html:text/html}
}

@inproceedings{as,
  author  = {Colorni, Alberto and Dorigo, Marco and Maniezzo, Vittorio},
  year    = {1991},
  month   = {01},
  pages   = {},
  title   = {Distributed Optimization by Ant Colonies},
  journal = {Proceedings of the First European Conference on Artificial Life}
}

@book{simon2013evolutionary,
  title     = {Evolutionary Optimization Algorithms},
  author    = {Simon, D.},
  isbn      = {9781118659502},
  url       = {https://books.google.es/books?id=gwUwIEPqk30C},
  year      = {2013},
  publisher = {Wiley}
}

@inproceedings{dorigo_ant_1999,
  title      = {Ant colony optimization: a new meta-heuristic},
  volume     = {2},
  shorttitle = {Ant colony optimization},
  url        = {https://ieeexplore.ieee.org/document/782657},
  doi        = {10.1109/CEC.1999.782657},
  abstract   = {Recently, a number of algorithms inspired by the foraging behavior of ant colonies have been applied to the solution of difficult discrete optimization problems. We put these algorithms in a common framework by defining the Ant Colony Optimization (ACO) meta-heuristic. A couple of paradigmatic examples of applications of these novel meta-heuristic are given, as well as a brief overview of existing applications.},
  language   = {en-US},
  urldate    = {2024-01-30},
  booktitle  = {Proceedings of the 1999 {Congress} on {Evolutionary} {Computation}-{CEC99} ({Cat}. {No}. {99TH8406})},
  author     = {Dorigo, M. and Di Caro, G.},
  month      = jul,
  year       = {1999},
  keywords   = {Ant colony optimization, Circuits, Cities and towns, Cost function, Routing, Time measurement, Traveling salesman problems, Vehicles},
  pages      = {1470--1477 Vol. 2},
  file       = {IEEE Xplore Abstract Record:/home/migue8gl/Zotero/storage/M4P2QDQR/782657.html:text/html;IEEE Xplore Full Text PDF:/home/migue8gl/Zotero/storage/FHR2H3UG/Dorigo and Di Caro - 1999 - Ant colony optimization a new meta-heuristic.pdf:application/pdf}
}

@book{Holland:1975,
  added-at    = {2009-06-26T15:25:19.000+0200},
  address     = {Ann Arbor, MI},
  author      = {Holland, John H.},
  biburl      = {https://www.bibsonomy.org/bibtex/2c29f3ed4d5bbe62a26e61a0e786e176a/butz},
  description = {diverse cognitive systems bib},
  interhash   = {5556ff81387180d09ef7583ae68dd32f},
  intrahash   = {c29f3ed4d5bbe62a26e61a0e786e176a},
  keywords    = {imported},
  note        = {second edition, 1992},
  publisher   = {University of Michigan Press},
  timestamp   = {2009-06-26T15:25:35.000+0200},
  title       = {Adaptation in Natural and Artificial Systems},
  year        = 1975
}

@misc{jaderberg2017population,
  title         = {Population Based Training of Neural Networks},
  author        = {Max Jaderberg and Valentin Dalibard and Simon Osindero and Wojciech M. Czarnecki and Jeff Donahue and Ali Razavi and Oriol Vinyals and Tim Green and Iain Dunning and Karen Simonyan and Chrisantha Fernando and Koray Kavukcuoglu},
  year          = {2017},
  eprint        = {1711.09846},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@article{dokeroglu_comprehensive_2022,
  title    = {A comprehensive survey on recent metaheuristics for feature selection},
  volume   = {494},
  issn     = {0925-2312},
  url      = {https://www.sciencedirect.com/science/article/pii/S092523122200474X},
  doi      = {10.1016/j.neucom.2022.04.083},
  abstract = {Feature selection has become an indispensable machine learning process for data preprocessing due to the ever-increasing sizes in actual data. There have been many solution methods proposed for feature selection since the 1970s. For the last two decades, we have witnessed the superiority of metaheuristic feature selection algorithms, and tens of new ones are being proposed every year. This survey focuses on the most outstanding recent metaheuristic feature selection algorithms of the last two decades in terms of their performance in exploration/exploitation operators, selection methods, transfer functions, fitness value evaluations, and parameter setting techniques. Current challenges of the metaheuristic feature selection algorithms and possible future research topics are examined and brought to the attention of the researchers as well.},
  language = {en-US},
  urldate  = {2023-09-22},
  journal  = {Neurocomputing},
  author   = {Dokeroglu, Tansel and Deniz, Ayça and Kiziloz, Hakan Ezgi},
  month    = jul,
  year     = {2022},
  keywords = {Classification, Feature selection, Machine learning, Metaheuristic algorithms, Survey},
  pages    = {269--296},
  file     = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/UWHN69RV/Dokeroglu et al. - 2022 - A comprehensive survey on recent metaheuristics fo.pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/UWLDEKLD/S092523122200474X.html:text/html}
}

@article{boussaid_survey_2013,
  series   = {Prediction, {Control} and {Diagnosis} using {Advanced} {Neural} {Computations}},
  title    = {A survey on optimization metaheuristics},
  volume   = {237},
  issn     = {0020-0255},
  url      = {https://www.sciencedirect.com/science/article/pii/S0020025513001588},
  doi      = {10.1016/j.ins.2013.02.041},
  abstract = {Metaheuristics are widely recognized as efficient approaches for many hard optimization problems. This paper provides a survey of some of the main metaheuristics. It outlines the components and concepts that are used in various metaheuristics in order to analyze their similarities and differences. The classification adopted in this paper differentiates between single solution based metaheuristics and population based metaheuristics. The literature survey is accompanied by the presentation of references for further details, including applications. Recent trends are also briefly discussed.},
  urldate  = {2024-04-20},
  journal  = {Information Sciences},
  author   = {Boussaïd, Ilhem and Lepagnot, Julien and Siarry, Patrick},
  month    = jul,
  year     = {2013},
  keywords = {Diversification, Intensification, Population based metaheuristic, Single solution based metaheuristic},
  pages    = {82--117},
  file     = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/5ECZKIIT/Boussaïd et al. - 2013 - A survey on optimization metaheuristics.pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/23PYTLBJ/S0020025513001588.html:text/html}
}

@article{agrawal_metaheuristic_2021,
  title      = {Metaheuristic algorithms on feature selection: {A} survey of one decade of research (2009-2019)},
  volume     = {9},
  issn       = {2169-3536},
  shorttitle = {Metaheuristic algorithms on feature selection},
  doi        = {10.1109/ACCESS.2021.3056407},
  abstract   = {Feature selection is a critical and prominent task in machine learning. To reduce the dimension of the feature set while maintaining the accuracy of the performance is the main aim of the feature selection problem. Various methods have been developed to classify the datasets. However, metaheuristic algorithms have achieved great attention in solving numerous optimization problem. Therefore, this paper presents an extensive literature review on solving feature selection problem using metaheuristic algorithms which are developed in the ten years (2009-2019). Further, metaheuristic algorithms have been classified into four categories based on their behaviour. Moreover, a categorical list of more than a hundred metaheuristic algorithms is presented. To solve the feature selection problem, only binary variants of metaheuristic algorithms have been reviewed and corresponding to their categories, a detailed description of them explained. The metaheuristic algorithms in solving feature selection problem are given with their binary classification, name of the classifier used, datasets and the evaluation metrics. After reviewing the papers, challenges and issues are also identified in obtaining the best feature subset using different metaheuristic algorithms. Finally, some research gaps are also highlighted for the researchers who want to pursue their research in developing or modifying metaheuristic algorithms for classification. For an application, a case study is presented in which datasets are adopted from the UCI repository and numerous metaheuristic algorithms are employed to obtain the optimal feature subset. © 2013 IEEE.},
  language   = {en-US},
  journal    = {IEEE Access},
  author     = {Agrawal, P. and Abutarboush, H.F. and Ganesh, T. and Mohamed, A.W.},
  year       = {2021},
  keywords   = {Classification, Feature selection, Metaheuristic algorithms, Binary variants, Literature review},
  pages      = {26766--26791},
  annote     = {Cited By :174},
  file       = {Full Text:/home/migue8gl/Zotero/storage/WLMA626P/Agrawal et al. - 2021 - Metaheuristic algorithms on feature selection A s.pdf:application/pdf;Snapshot:/home/migue8gl/Zotero/storage/5982WRQH/display.html:text/html}
}

@article{GUY2008585,
  title    = {Avoidance of conspecific odour by carabid beetles: a mechanism for the emergence of scale-free searching patterns},
  journal  = {Animal Behaviour},
  volume   = {76},
  number   = {3},
  pages    = {585-591},
  year     = {2008},
  issn     = {0003-3472},
  doi      = {https://doi.org/10.1016/j.anbehav.2008.04.004},
  url      = {https://www.sciencedirect.com/science/article/pii/S0003347208001668},
  author   = {Adam G. Guy and David A. Bohan and Stephen J. Powers and Andrew M. Reynolds},
  keywords = {carabid, conspecific avoidance, Lévy flight, optimal searching strategy, },
  abstract = {We monitored the movements of a starved model predator, the carabid beetle Pterostichus melanarius, in arenas containing test papers upon which beetles had previously walked and unexposed control papers. Significantly, beetles accumulated on the unexposed controls, indicating conspecific avoidance (i.e. behaviour designed to avoid locations previously traversed by individuals of the same species). This finding is novel and important because optimal Lévy-flight (scale-free) search patterns for the location of sparsely and randomly distributed prey resources can emerge from conspecific avoidance. This finding may account for the shortcomings of random-walk (scale-finite) models when used to predict large-scale movement and search patterns by extrapolating from observations made at small scales.}
}

@article{emary_binary_2016,
  title    = {Binary grey wolf optimization approaches for feature selection},
  volume   = {172},
  issn     = {0925-2312},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231215010504},
  doi      = {10.1016/j.neucom.2015.06.083},
  abstract = {In this work, a novel binary version of the grey wolf optimization (GWO) is proposed and used to select optimal feature subset for classification purposes. Grey wolf optimizer (GWO) is one of the latest bio-inspired optimization techniques, which simulate the hunting process of grey wolves in nature. The binary version introduced here is performed using two different approaches. In the first approach, individual steps toward the first three best solutions are binarized and then stochastic crossover is performed among the three basic moves to find the updated binary grey wolf position. In the second approach, sigmoidal function is used to squash the continuous updated position, then stochastically threshold these values to find the updated binary grey wolf position. The two approach for binary grey wolf optimization (bGWO) are hired in the feature selection domain for finding feature subset maximizing the classification accuracy while minimizing the number of selected features. The proposed binary versions were compared to two of the common optimizers used in this domain namely particle swarm optimizer and genetic algorithms. A set of assessment indicators are used to evaluate and compared the different methods over 18 different datasets from the UCI repository. Results prove the capability of the proposed binary version of grey wolf optimization (bGWO) to search the feature space for optimal feature combinations regardless of the initialization and the used stochastic operators.},
  language = {en-US},
  urldate  = {2023-11-18},
  journal  = {Neurocomputing},
  author   = {Emary, E. and Zawbaa, Hossam M. and Hassanien, Aboul Ella},
  month    = jan,
  year     = {2016},
  keywords = {Feature selection, Binary grey wolf optimization, Bio-inspired optimization, Evolutionary computation, Grey wolf optimization},
  pages    = {371--381},
  file     = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/6KQVQUMH/Emary et al. - 2016 - Binary grey wolf optimization approaches for featu.pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/KDV2ATYS/S0925231215010504.html:text/html}
}

@inproceedings{hussien_s-shaped_2019,
  address   = {Singapore},
  series    = {Advances in {Intelligent} {Systems} and {Computing}},
  title     = {S-shaped {Binary} {Whale} {Optimization} {Algorithm} for {Feature} {Selection}},
  isbn      = {978-981-10-8863-6},
  doi       = {10.1007/978-981-10-8863-6_9},
  abstract  = {Whale optimization algorithm is one of the recent nature-inspired optimization technique based on the behavior of bubble-net hunting strategy. In this paper, a novel binary version of whale optimization algorithm (bWOA) is proposed to select the optimal feature subset for dimensionality reduction and classifications problem. The new approach is based on a sigmoid transfer function (S-shape). By dealing with the feature selection problem, a free position of the whale must be transformed to their corresponding binary solutions. This transformation is performed by applying an S-shaped transfer function in every dimension that defines the probability of transforming the position vectors’ elements from 0 to 1 and vice versa and hence force the search agents to move in a binary space. K-NN classifier is applied to ensure that the selected features are the relevant ones. A set of criteria are used to evaluate and compare the proposed bWOA-S with the native one over eleven different datasets. The results proved that the new algorithm has a significant performance in finding the optimal feature.},
  language  = {en},
  booktitle = {Recent {Trends} in {Signal} and {Image} {Processing}},
  publisher = {Springer},
  author    = {Hussien, Abdelazim G. and Hassanien, Aboul Ella and Houssein, Essam H. and Bhattacharyya, Siddhartha and Amin, Mohamed},
  editor    = {Bhattacharyya, Siddhartha and Mukherjee, Anirban and Bhaumik, Hrishikesh and Das, Swagatam and Yoshida, Kaori},
  year      = {2019},
  keywords  = {Classification, Feature selection, Whale optimization algorithm, Binary whale optimization algorithm, Dimensionality reduction, S-shaped},
  pages     = {79--87},
  file      = {Full Text PDF:/home/migue8gl/Zotero/storage/ATDN6867/Hussien et al. - 2019 - S-shaped Binary Whale Optimization Algorithm for F.pdf:application/pdf}
}

@article{mafarja_whale_2018,
  title    = {Whale optimization approaches for wrapper feature selection},
  volume   = {62},
  issn     = {1568-4946},
  url      = {https://www.sciencedirect.com/science/article/pii/S1568494617306695},
  doi      = {10.1016/j.asoc.2017.11.006},
  abstract = {Classification accuracy highly dependents on the nature of the features in a dataset which may contain irrelevant or redundant data. The main aim of feature selection is to eliminate these types of features to enhance the classification accuracy. The wrapper feature selection model works on the feature set to reduce the number of features and improve the classification accuracy simultaneously. In this work, a new wrapper feature selection approach is proposed based on Whale Optimization Algorithm (WOA). WOA is a newly proposed algorithm that has not been systematically applied to feature selection problems yet. Two binary variants of the WOA algorithm are proposed to search the optimal feature subsets for classification purposes. In the first one, we aim to study the influence of using the Tournament and Roulette Wheel selection mechanisms instead of using a random operator in the searching process. In the second approach, crossover and mutation operators are used to enhance the exploitation of the WOA algorithm. The proposed methods are tested on standard benchmark datasets and then compared to three algorithms such as Particle Swarm Optimization (PSO), Genetic Algorithm (GA), the Ant Lion Optimizer (ALO), and five standard filter feature selection methods. The paper also considers an extensive study of the parameter setting for the proposed technique. The results show the efficiency of the proposed approaches in searching for the optimal feature subsets.},
  urldate  = {2023-11-20},
  journal  = {Applied Soft Computing},
  author   = {Mafarja, Majdi and Mirjalili, Seyedali},
  month    = jan,
  year     = {2018},
  keywords = {Classification, Feature selection, Optimization, Crossover, Evolutionary operators, Mutation, Selection, Whale optimization algorithm, WOA},
  pages    = {441--453},
  file     = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/CTUVPYU5/Mafarja and Mirjalili - 2018 - Whale optimization approaches for wrapper feature .pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/PRMTLC84/S1568494617306695.html:text/html}
}

@article{mafarja_binary_2018,
  title    = {Binary dragonfly optimization for feature selection using time-varying transfer functions},
  volume   = {161},
  issn     = {0950-7051},
  doi      = {10.1016/j.knosys.2018.08.003},
  abstract = {The Dragonfly Algorithm (DA) is a recently proposed heuristic search algorithm that was shown to have excellent performance for numerous optimization problems. In this paper, a wrapper-feature selection algorithm is proposed based on the Binary Dragonfly Algorithm (BDA). The key component of the BDA is the transfer function that maps a continuous search space to a discrete search space. In this study, eight transfer functions, categorized into two families (S-shaped and V-shaped functions) are integrated into the BDA and evaluated using eighteen benchmark datasets obtained from the UCI data repository. The main contribution of this paper is the proposal of time-varying S-shaped and V-shaped transfer functions to leverage the impact of the step vector on balancing exploration and exploitation. During the early stages of the optimization process, the probability of changing the position of an element is high, which facilitates the exploration of new solutions starting from the initial population. On the other hand, the probability of changing the position of an element becomes lower towards the end of the optimization process. This behavior is obtained by considering the current iteration number as a parameter of transfer functions. The performance of the proposed approaches is compared with that of other state-of-art approaches including the DA, binary grey wolf optimizer (bGWO), binary gravitational search algorithm (BGSA), binary bat algorithm (BBA), particle swarm optimization (PSO), and genetic algorithm in terms of classification accuracy, sensitivity, specificity, area under the curve, and number of selected attributes. Results show that the time-varying S-shaped BDA approach outperforms compared approaches. © 2018},
  language = {English},
  journal  = {Knowledge-Based Systems},
  author   = {Mafarja, M. and Aljarah, I. and Heidari, A.A. and Faris, H. and Fournier-Viger, P. and Li, X. and Mirjalili, S.},
  year     = {2018},
  keywords = {Classification, Feature selection, Optimization, Binary dragonfly algorithm, Transfer functions},
  pages    = {185--204},
  annote   = {Cited By :314},
  file     = {Accepted Version:/home/migue8gl/Zotero/storage/H7UKQ553/Mafarja et al. - 2018 - Binary dragonfly optimization for feature selectio.pdf:application/pdf;Snapshot:/home/migue8gl/Zotero/storage/G9CP53ND/display.html:text/html}
}

@article{mafarja_binary_2019,
  title    = {Binary grasshopper optimisation algorithm approaches for feature selection problems},
  volume   = {117},
  issn     = {0957-4174},
  url      = {https://www.sciencedirect.com/science/article/pii/S0957417418305864},
  doi      = {10.1016/j.eswa.2018.09.015},
  abstract = {Feature Selection (FS) is a challenging machine learning-related task that aims at reducing the number of features by removing irrelevant, redundant and noisy data while maintaining an acceptable level of classification accuracy. FS can be considered as an optimisation problem. Due to the difficulty of this problem and having a large number of local solutions, stochastic optimisation algorithms are promising techniques to solve this problem. As a seminal attempt, binary variants of the recent Grasshopper Optimisation Algorithm (GOA) are proposed in this work and employed to select the optimal feature subset for classification purposes within a wrapper-based framework. Two mechanisms are employed to design a binary GOA, the first one is based on Sigmoid and V-shaped transfer functions, and will be indicated by BGOA-S and BGOA-V, respectively. While the second mechanism uses a novel technique that combines the best solution obtained so far. In addition, a mutation operator is employed to enhance the exploration phase in BGOA algorithm (BGOA-M). The proposed methods are evaluated using 25 standard UCI datasets and compared with 8 well-regarded metaheuristic wrapper-based approaches, and six well known filter-based (e.g., correlation FS) approaches. The comparative results show the superior performance of the BGOA and BGOA-M methods compared to other similar techniques in the literature.},
  urldate  = {2023-10-23},
  journal  = {Expert Systems with Applications},
  author   = {Mafarja, Majdi and Aljarah, Ibrahim and Faris, Hossam and Hammouri, Abdelaziz I. and Al-Zoubi, Ala’ M. and Mirjalili, Seyedali},
  month    = mar,
  year     = {2019},
  keywords = {Classification, Feature selection, Binary grasshopper optimisation algorithm, GOA, Optimisation},
  pages    = {267--286},
  file     = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/XFB4W5YF/Mafarja et al. - 2019 - Binary grasshopper optimisation algorithm approach.pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/LW2R4T9P/S0957417418305864.html:text/html}
}

@article{mirjalili_binary_2014,
  title    = {Binary bat algorithm},
  volume   = {25},
  issn     = {0941-0643},
  doi      = {10.1007/s00521-013-1525-5},
  abstract = {Bat algorithm (BA) is one of the recently proposed heuristic algorithms imitating the echolocation behavior of bats to perform global optimization. The superior performance of this algorithm has been proven among the other most well-known algorithms such as genetic algorithm (GA) and particle swarm optimization (PSO). However, the original version of this algorithm is suitable for continuous problems, so it cannot be applied to binary problems directly. In this paper, a binary version of this algorithm is proposed. A comparative study with binary PSO and GA over twenty-two benchmark functions is conducted to draw a conclusion. Furthermore, Wilcoxon's rank-sum nonparametric statistical test was carried out at 5 \% significance level to judge whether the results of the proposed algorithm differ from those of the other algorithms in a statistically significant way. The results prove that the proposed binary bat algorithm (BBA) is able to significantly outperform others on majority of the benchmark functions. In addition, there is a real application of the proposed method in optical engineering called optical buffer design at the end of the paper. The results of the real application also evidence the superior performance of BBA in practice. © 2013 Springer-Verlag London.},
  language = {en-US},
  number   = {3-4},
  journal  = {Neural Computing and Applications},
  author   = {Mirjalili, S. and Mirjalili, S.M. and Yang, X.-S.},
  year     = {2014},
  note     = {Number: 3-4},
  keywords = {Optimization, Binary optimization, Bat algorithm, Bio-inspired algorithm, Discrete evolutionary algorithms, Discrete optimization, Optical buffer design},
  pages    = {663--681},
  annote   = {Cited By :533},
  file     = {Full Text:/home/migue8gl/Zotero/storage/34IXLTIH/Mirjalili et al. - 2014 - Binary bat algorithm.pdf:application/pdf;Snapshot:/home/migue8gl/Zotero/storage/UY7D95V8/display.html:text/html}
}

@article{zhang_return-cost-based_2017,
  title    = {A return-cost-based binary firefly algorithm for feature selection},
  volume   = {418-419},
  issn     = {00200255},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0020025516314098},
  doi      = {10.1016/j.ins.2017.08.047},
  abstract = {Various real-world applications can be formulated as feature selection problems, which have been known to be NP-hard. In this paper, we propose an effective feature selection method based on ﬁreﬂy algorithm (FFA), called return-cost-based binary FFA (RcBBFA). The proposed method has the capability of preventing premature convergence and is particularly eﬃcient attributed to the following three aspects. An indicator based on the return-cost is ﬁrst deﬁned to measure a ﬁreﬂy’s attractiveness from other ﬁreﬂies. Then, a Pareto dominance-based strategy is presented to seek the attractive one for each ﬁreﬂy. Finally, a binary movement operator based on the return-cost attractiveness and the adaptive jump is developed to update the position of a ﬁreﬂy. The experimental results on a series of public datasets show that the proposed method is competitive in comparison with other feature selection algorithms, including the traditional algorithms, the GA-based algorithm, the PSO-based algorithm, and the FFA-based algorithms.},
  language = {en},
  urldate  = {2024-03-21},
  journal  = {Information Sciences},
  author   = {Zhang, Yong and Song, Xian-fang and Gong, Dun-wei},
  month    = dec,
  year     = {2017},
  pages    = {561--574},
  file     = {Zhang et al. - 2017 - A return-cost-based binary firefly algorithm for feature selection.pdf:/home/migue8gl/Zotero/storage/G3EZ98LN/Zhang et al. - 2017 - A return-cost-based binary firefly algorithm for feature selection.pdf:application/pdf}
}

@inproceedings{rodrigues_bcs_2013,
  title      = {{BCS}: {A} {Binary} {Cuckoo} {Search} algorithm for feature selection},
  shorttitle = {{BCS}},
  url        = {https://ieeexplore.ieee.org/document/6571881},
  doi        = {10.1109/ISCAS.2013.6571881},
  abstract   = {Feature selection has been actively pursued in the last years, since to find the most discriminative set of features can enhance the recognition rates and also to make feature extraction faster. In this paper, the propose a new feature selection called Binary Cuckoo Search, which is based on the behavior of cuckoo birds. The experiments were carried out in the context of theft detection in power distribution systems in two datasets obtained from a Brazilian electrical power company, and have demonstrated the robustness of the proposed technique against with several others nature-inspired optimization techniques.},
  urldate    = {2024-03-13},
  booktitle  = {2013 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})},
  author     = {Rodrigues, D. and Pereira, L. A. M. and Almeida, T. N. S. and Papa, J. P. and Souza, A. N. and Ramos, C. C. O. and Yang, Xin-She},
  month      = may,
  year       = {2013},
  note       = {ISSN: 2158-1525},
  keywords   = {Optimization, Accuracy, Birds, Context, Search problems, Training, Vectors},
  pages      = {465--468},
  file       = {IEEE Xplore Full Text PDF:/home/migue8gl/Zotero/storage/Y3P36CDY/Rodrigues et al. - 2013 - BCS A Binary Cuckoo Search algorithm for feature selection.pdf:application/pdf}
}

@article{mirjalili_s-shaped_2013,
  title    = {S-shaped versus {V}-shaped transfer functions for binary {Particle} {Swarm} {Optimization}},
  volume   = {9},
  issn     = {2210-6502},
  url      = {https://www.sciencedirect.com/science/article/pii/S2210650212000648},
  doi      = {10.1016/j.swevo.2012.09.002},
  abstract = {Particle Swarm Optimization (PSO) is one of the most widely used heuristic algorithms. The simplicity and inexpensive computational cost makes this algorithm very popular and powerful in solving a wide range of problems. The binary version of this algorithm has been introduced for solving binary problems. The main part of the binary version is a transfer function which is responsible to map a continuous search space to a discrete search space. Currently there appears to be insufficient focus on the transfer function in the literature despite its apparent importance. In this study six new transfer functions divided into two families, s-shaped and v-shaped, are introduced and evaluated. Twenty-five benchmark optimization functions provided by CEC 2005 special session are employed to evaluate these transfer functions and select the best one in terms of avoiding local minima and convergence speed. In order to validate the performance of the best transfer function, a comparative study with six recent modifications of BPSO is provided as well. The results prove that the new introduced v-shaped family of transfer functions significantly improves the performance of the original binary PSO.},
  urldate  = {2023-10-29},
  journal  = {Swarm and Evolutionary Computation},
  author   = {Mirjalili, Seyedali and Lewis, Andrew},
  month    = apr,
  year     = {2013},
  keywords = {Heuristic algorithm, BPSO, Evolutionary algorithm, Particle swarm, PSO, Transfer function},
  pages    = {1--14},
  file     = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/UFF5LLAM/Mirjalili and Lewis - 2013 - S-shaped versus V-shaped transfer functions for bi.pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/A2A7AGLA/S2210650212000648.html:text/html}
}

@article{kiran_binary_2021,
  title    = {A binary artificial bee colony algorithm and its performance assessment},
  volume   = {175},
  issn     = {09574174},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S095741742100258X},
  doi      = {10.1016/j.eswa.2021.114817},
  abstract = {Artificial bee colony algorithm, ABC for short, is a swarm-based optimization algorithm proposed for solving continuous optimization problems. Due to its simple but effective structure, some binary versions of the algo­ rithm have been developed. In this study, we focus on modification of its xor-based binary version, called as binABC. The solution update rule of basic ABC is replaced with a xor logic gate in binABC algorithm, and binABC works on discretely-structured solution space. The rest of components in binABC are the same as with the basic ABC algorithm. In order to improve local search capability and convergence characteristics of binABC, a stig­ mergic behavior-based update rule for onlooker bees of binABC and extended version of xor-based update rule are proposed in the present study. The developed version of binABC is applied to solve a modern benchmark problem set (CEC2015). To validate the performance of proposed algorithm, a series of comparisons are con­ ducted on this problem set. The proposed algorithm is first compared with the basic ABC and binABC on CEC2015 set. After its performance validation, six binary versions of ABC algorithm are considered for com­ parison of the algorithms, and a comprehensive comparison among the state-of-art variants of swarm intelligence or evolutionary computation algorithms is conducted on this set of functions. Finally, an uncapacitated facility location problem set, a pure binary optimization problem, is considered for the comparison of the proposed algorithm and binary variants of ABC algorithm. The experimental results and comparisons show that the pro­ posed algorithm is successful and effective in solving binary optimization problems as its basic version in solving continuous optimization problems.},
  language = {en},
  urldate  = {2023-11-28},
  journal  = {Expert Systems with Applications},
  author   = {Kiran, Mustafa Servet},
  month    = aug,
  year     = {2021},
  pages    = {114817},
  file     = {Kiran - 2021 - A binary artificial bee colony algorithm and its p.pdf:/home/migue8gl/Zotero/storage/GJETXHIK/Kiran - 2021 - A binary artificial bee colony algorithm and its p.pdf:application/pdf}
}

@inproceedings{pampara_binary_2006,
  title     = {Binary {Differential} {Evolution}},
  url       = {https://ieeexplore.ieee.org/document/1688535},
  doi       = {10.1109/CEC.2006.1688535},
  abstract  = {The ability of differential evolution (DE) to perform well in continuous-valued search spaces is well documented. The arithmetic reproduction operator used by differential evolution is simple, however, the manner in which the operator is defined, makes it practically impossible to effectively apply the standard DE to other problem spaces. An interesting and unique mapping method is examined which will enable the DE algorithm to operate within binary space. Using angle modulation, a bit string can be generated using a trigonometric generating function. The DE is used to evolve the coefficients to the trigonometric function, thereby allowing a mapping from continuous-space to binary-space. Instead of evolving the higher-dimensional binary solution directly, angle modulation is used together with DE to reduce the complexity of the problem into a 4-dimensional continuous-valued problem. Experimental results indicate the effectiveness of the technique and the viability for the DE to operate in binary space.},
  urldate   = {2024-03-25},
  booktitle = {2006 {IEEE} {International} {Conference} on {Evolutionary} {Computation}},
  author    = {Pampara, G. and Engelbrecht, A.P. and Franken, N.},
  month     = jul,
  year      = {2006},
  note      = {ISSN: 1941-0026},
  keywords  = {Particle swarm optimization, Evolutionary computation, Africa, Arithmetic, Computer science, Genetic algorithms, Optimization methods, Probability density function, Stochastic processes, Testing},
  pages     = {1873--1879},
  file      = {IEEE Xplore Abstract Record:/home/migue8gl/Zotero/storage/VM24977G/1688535.html:text/html;IEEE Xplore Full Text PDF:/home/migue8gl/Zotero/storage/3QAB84LU/Pampara et al. - 2006 - Binary Differential Evolution.pdf:application/pdf}
}

@inproceedings{socha_aco_2004,
  address   = {Berlin, Heidelberg},
  series    = {Lecture {Notes} in {Computer} {Science}},
  title     = {{ACO} for {Continuous} and {Mixed}-{Variable} {Optimization}},
  isbn      = {978-3-540-28646-2},
  doi       = {10.1007/978-3-540-28646-2_3},
  abstract  = {This paper presents how the Ant Colony Optimization (ACO) metaheuristic can be extended to continuous search domains and applied to both continuous and mixed discrete-continuous optimization problems. The paper describes the general underlying idea, enumerates some possible design choices, presents a first implementation, and provides some preliminary results obtained on well-known benchmark problems. The proposed method is compared to other ant, as well as non-ant methods for continuous optimization.},
  language  = {en},
  booktitle = {Ant {Colony} {Optimization} and {Swarm} {Intelligence}},
  publisher = {Springer},
  author    = {Socha, Krzysztof},
  editor    = {Dorigo, Marco and Birattari, Mauro and Blum, Christian and Gambardella, Luca Maria and Mondada, Francesco and Stützle, Thomas},
  year      = {2004},
  keywords  = {Continuous Domain, Continuous Optimization, Future Generation Computer System, Normal Probability Density Function, Probability Density Function},
  pages     = {25--36},
  file      = {Full Text PDF:/home/migue8gl/Zotero/storage/QEBT8DZE/Socha - 2004 - ACO for Continuous and Mixed-Variable Optimization.pdf:application/pdf}
}


@article{dokeroglu_comprehensive_2022,
  title    = {A comprehensive survey on recent metaheuristics for feature selection},
  volume   = {494},
  issn     = {0925-2312},
  url      = {https://www.sciencedirect.com/science/article/pii/S092523122200474X},
  doi      = {10.1016/j.neucom.2022.04.083},
  abstract = {Feature selection has become an indispensable machine learning process for data preprocessing due to the ever-increasing sizes in actual data. There have been many solution methods proposed for feature selection since the 1970s. For the last two decades, we have witnessed the superiority of metaheuristic feature selection algorithms, and tens of new ones are being proposed every year. This survey focuses on the most outstanding recent metaheuristic feature selection algorithms of the last two decades in terms of their performance in exploration/exploitation operators, selection methods, transfer functions, fitness value evaluations, and parameter setting techniques. Current challenges of the metaheuristic feature selection algorithms and possible future research topics are examined and brought to the attention of the researchers as well.},
  language = {en-US},
  urldate  = {2023-09-22},
  journal  = {Neurocomputing},
  author   = {Dokeroglu, Tansel and Deniz, Ayça and Kiziloz, Hakan Ezgi},
  month    = jul,
  year     = {2022},
  keywords = {Classification, Feature selection, Machine learning, Metaheuristic algorithms, Survey},
  pages    = {269--296},
  file     = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/UWHN69RV/Dokeroglu et al. - 2022 - A comprehensive survey on recent metaheuristics fo.pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/UWLDEKLD/S092523122200474X.html:text/html}
}

@article{mirjalili_s-shaped_2013,
  title    = {S-shaped versus {V}-shaped transfer functions for binary {Particle} {Swarm} {Optimization}},
  volume   = {9},
  issn     = {2210-6502},
  url      = {https://www.sciencedirect.com/science/article/pii/S2210650212000648},
  doi      = {10.1016/j.swevo.2012.09.002},
  abstract = {Particle Swarm Optimization (PSO) is one of the most widely used heuristic algorithms. The simplicity and inexpensive computational cost makes this algorithm very popular and powerful in solving a wide range of problems. The binary version of this algorithm has been introduced for solving binary problems. The main part of the binary version is a transfer function which is responsible to map a continuous search space to a discrete search space. Currently there appears to be insufficient focus on the transfer function in the literature despite its apparent importance. In this study six new transfer functions divided into two families, s-shaped and v-shaped, are introduced and evaluated. Twenty-five benchmark optimization functions provided by CEC 2005 special session are employed to evaluate these transfer functions and select the best one in terms of avoiding local minima and convergence speed. In order to validate the performance of the best transfer function, a comparative study with six recent modifications of BPSO is provided as well. The results prove that the new introduced v-shaped family of transfer functions significantly improves the performance of the original binary PSO.},
  urldate  = {2023-10-29},
  journal  = {Swarm and Evolutionary Computation},
  author   = {Mirjalili, Seyedali and Lewis, Andrew},
  month    = apr,
  year     = {2013},
  keywords = {Heuristic algorithm, BPSO, Evolutionary algorithm, Particle swarm, PSO, Transfer function},
  pages    = {1--14},
  file     = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/UFF5LLAM/Mirjalili and Lewis - 2013 - S-shaped versus V-shaped transfer functions for bi.pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/A2A7AGLA/S2210650212000648.html:text/html}
}

@article{he_novel_2022,
  title    = {Novel binary differential evolution algorithm based on {Taper}-shaped transfer functions for binary optimization problems},
  volume   = {69},
  issn     = {2210-6502},
  url      = {https://www.sciencedirect.com/science/article/pii/S221065022100184X},
  doi      = {10.1016/j.swevo.2021.101022},
  abstract = {In order to efficiently solve the binary optimization problems by using differential evolution (DE), a class of new transfer functions, Taper-shaped transfer function, is firstly proposed by using power functions. Then, the novel binary differential evolution algorithm based on Taper-shaped transfer functions (T-NBDE) is proposed. T-NBDE transforms a real vector representing the individual encoding into a binary vector by using the Taper-shaped transfer function, which is suitable for solving binary optimization problems. For verifying the practicability of Taper-shaped transfer functions and the excellent performance of T-NBDE, T-NBDE is firstly compared with binary DE based on S-shaped, U-shaped and V-shaped transfer functions, respectively. Subsequently, it is compared with the state-of-the-art algorithms for solving the knapsack problem with a single continuous variable (KPC) and the uncapacitated facility location problem (UFLP). The comparison results show that Taper-shaped transfer functions are competitive than existing transfer functions, and T-NBDE is more effective than existing algorithms for solving KPC problem and UFLP problem.},
  urldate  = {2024-03-27},
  journal  = {Swarm and Evolutionary Computation},
  author   = {He, Yichao and Zhang, Fazhan and Mirjalili, Seyedali and Zhang, Tong},
  month    = mar,
  year     = {2022},
  keywords = {Evolutionary algorithm, Transfer function, Differential evolution, Knapsack problem with a single continuous variable, Uncapacitated facility location problem},
  pages    = {101022},
  file     = {ScienceDirect Full Text PDF:/home/migue8gl/Zotero/storage/G5R4QE9C/He et al. - 2022 - Novel binary differential evolution algorithm based on Taper-shaped transfer functions for binary op.pdf:application/pdf;ScienceDirect Snapshot:/home/migue8gl/Zotero/storage/Q3WZ2QHX/S221065022100184X.html:text/html}
}

@misc{citicugr,
  title        = {{CITIC-UGR Sala de Servidores y Cl\'usteres de Computadores}},
  howpublished = {\url{http://citic.ugr.es/pages/informacion_general/equipamiento/cluster}},
  note         = {[Online; accessed 12-May-2024]}
}

@misc{scikit-learn,
  title        = {{MinMaxScaler}},
  howpublished = {Scikit-learn Documentation},
  month        = {},
  year         = {},
  note         = {URL: \url{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html}}
}

@article{molina_comprehensive_2020,
  title      = {Comprehensive {Taxonomies} of {Nature}- and {Bio}-inspired {Optimization}: {Inspiration} {Versus} {Algorithmic} {Behavior}, {Critical} {Analysis} {Recommendations}},
  volume     = {12},
  issn       = {1866-9956},
  shorttitle = {Comprehensive {Taxonomies} of {Nature}- and {Bio}-inspired {Optimization}},
  doi        = {10.1007/s12559-020-09730-8},
  abstract   = {In recent algorithmic family simulates different biological processes observed in Nature in order to efficiently address complex optimization problems. In the last years the number of bio-inspired optimization approaches in literature has grown considerably, reaching unprecedented levels that dark the future prospects of this field of research. This paper addresses this problem by proposing two comprehensive, principle-based taxonomies that allow researchers to organize existing and future algorithmic developments into well-defined categories, considering two different criteria: the source of inspiration and the behavior of each algorithm. Using these taxonomies we review more than three hundred publications dealing with nature-inspired and bio-inspired algorithms, and proposals falling within each of these categories are examined, leading to a critical summary of design trends and similarities between them, and the identification of the most similar classical algorithm for each reviewed paper. From our analysis we conclude that a poor relationship is often found between the natural inspiration of an algorithm and its behavior. Furthermore, similarities in terms of behavior between different algorithms are greater than what is claimed in their public disclosure: specifically, we show that more than one-third of the reviewed bio-inspired solvers are versions of classical algorithms. Grounded on the conclusions of our critical analysis, we give several recommendations and points of improvement for better methodological practices in this active and growing research field. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.},
  language   = {English},
  number     = {5},
  journal    = {Cognitive Computation},
  author     = {Molina, D. and Poyatos, J. and Ser, J.D. and García, S. and Hussain, A. and Herrera, F.},
  year       = {2020},
  keywords   = {Bio-inspired optimization, Classification, Nature-inspired algorithms, Taxonomy},
  pages      = {897--939},
  annote     = {Cited By :114},
  file       = {Snapshot:/home/migue8gl/Zotero/storage/MJ69XSAV/display.html:text/html;Submitted Version:/home/migue8gl/Zotero/storage/KL5GKIEZ/Molina et al. - 2020 - Comprehensive Taxonomies of Nature- and Bio-inspired Optimization Inspiration Versus Algorithmic Be.pdf:application/pdf}
}