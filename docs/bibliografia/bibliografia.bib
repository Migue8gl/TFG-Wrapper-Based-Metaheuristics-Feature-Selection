@misc{udacity2015curse,
  author = {Udacity},
  title  = {Curse of Dimensionality - Georgia Tech - Machine Learning},
  year   = {2015},
  month  = {Feb},
  note   = {[Online]. Available: Retrieved 2022-06-29}
}


@article{miao_survey_2016,
  author   = {J. Miao and L. Niu},
  title    = {A Survey on Feature Selection},
  journal  = {Procedia Computer Science},
  series   = {Promoting Business Analytics and Quantitative Management of Technology: 4th International Conference on Information Technology and Quantitative Management (ITQM 2016)},
  volume   = {91},
  pages    = {919--926},
  year     = {2016},
  month    = {Jan.},
  doi      = {10.1016/j.procs.2016.07.111},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050916313047},
  urldate  = {2024-04-02},
  abstract = {Feature selection, as a dimensionality reduction technique, aims to choosing a small subset of the relevant features from the original features by removing irrelevant, redundant or noisy features. Feature selection usually can lead to better learning performance, i.e., higher learning accuracy, lower computational cost, and better model interpretability. Recently, researchers from computer vision, text mining and so on have proposed a variety of feature selection algorithms and in terms of theory and experiment, show the effectiveness of their works. This paper is aimed at reviewing the state of the art on these techniques. Furthermore, a thorough experiment is conducted to check if the use of feature selection can improve the performance of learning, considering some of the approaches mentioned in the literature. The experimental results show that unsupervised feature selection algorithms benefits machine learning tasks improving the performance of clustering.},
  issn     = {1877-0509},
  keywords = {clustering, feature selection, machine learning, unsupervised}
}

@book{Mostafa2012,
  author    = {Y.S. Abu-Mostafa and M. Magdon-Ismail and H.T. Lin},
  title     = {Learning From Data},
  publisher = {AMLBook},
  year      = {2012},
  keywords  = {general_machine_learning}
}

@inproceedings{kira_practical_1992,
  author    = {K. Kira and L. A. Rendell},
  title     = {A Practical Approach to Feature Selection},
  booktitle = {Proc. 9th International Workshop on Machine Learning (ICML 1992)},
  pages     = {249--256},
  year      = {1992},
  isbn      = {978-1-55860-247-2},
  doi       = {10.1016/B978-1-55860-247-2.50037-1},
  abstract  = {In real-world concept learning problems, the representation of data often uses many features, only a few of which may be related to the target concept. In this situation, feature selection is important both to speed up learning and to improve concept quality. A new feature selection algorithm Relief uses a statistical method and avoids heuristic search. Relief requires linear time in the number of given features and the number of training instances regardless of the target concept to be learned. Although the algorithm does not necessarily find the smallest subset of features, the size tends to be small because only statistically relevant features are selected. This paper focuses on empirical test results in two artificial domains; the LED Display domain and the Parity domain with and without noise. Comparison with other feature selection algorithms shows Relief's advantages in terms of learning time and the accuracy of the learned concept, suggesting Relief's practicality. © 1992 Proceedings of the 9th International Workshop on Machine Learning, ICML 1992. All rights reserved.},
  language  = {English},
  annote    = {Cited By :2345}
}



@article{ding_minimum_2005,
  author   = {C. Ding and H. Peng},
  title    = {Minimum Redundancy Feature Selection from Microarray Gene Expression Data},
  journal  = {Journal of Bioinformatics and Computational Biology},
  volume   = {3},
  number   = {2},
  pages    = {185--205},
  year     = {2005},
  doi      = {10.1142/S0219720005001004},
  issn     = {0219-7200},
  abstract = {How to selecting a small subset out of the thousands of genes in microarray data is important for accurate classification of phenotypes. Widely used methods typically rank genes according to their differential expressions among phenotypes and pick the top-ranked genes. We observe that feature sets so obtained have certain redundancy and study methods to minimize it. We propose a minimum redundancy - maximum relevance (MRMR) feature selection framework. Genes selected via MRMR provide a more balanced coverage of the space and capture broader characteristics of phenotypes. They lead to significantly improved class predictions in extensive experiments on 6 gene expression data sets: NCI, Lymphoma, Lung, Child Leukemia, Leukemia, and Colon. Improvements are observed consistently among 4 classification methods: Naïve Bayes, Linear discriminant analysis, Logistic regression, and Support vector machines. © Imperial College Press.},
  language = {English},
  keywords = {Cancer classification, Gene expression analysis, Gene selection, Naïve Bayes, SVM},
  annote   = {Cited By :1780}
}


@article{cortes_support-vector_1995,
  author   = {C. Cortes and V. Vapnik},
  title    = {Support-Vector Networks},
  journal  = {Machine Learning},
  volume   = {20},
  number   = {3},
  pages    = {273--297},
  year     = {1995},
  month    = sep,
  doi      = {10.1007/BF00994018},
  url      = {http://link.springer.com/10.1007/BF00994018},
  abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimensional feature space. In this feature space, a linear decision surface is constructed. Special properties of the decision surface ensure high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
  language = {en},
  urldate  = {2024-04-02}
}



@article{cover_nearest_1967,
  author   = {T. Cover and P. Hart},
  title    = {Nearest Neighbor Pattern Classification},
  journal  = {IEEE Transactions on Information Theory},
  volume   = {13},
  number   = {1},
  pages    = {21--27},
  year     = {1967},
  month    = jan,
  doi      = {10.1109/TIT.1967.1053964},
  url      = {http://ieeexplore.ieee.org/document/1053964/},
  language = {en},
  urldate  = {2024-04-02}
}


@article{fix_discriminatory_1989,
  author     = {E. Fix and J. L. Hodges},
  title      = {Discriminatory Analysis. Nonparametric Discrimination: Consistency Properties},
  journal    = {International Statistical Review / Revue Internationale de Statistique},
  volume     = {57},
  number     = {3},
  pages      = {238--247},
  year       = {1989},
  issn       = {0306-7734},
  doi        = {10.2307/1403797},
  url        = {https://www.jstor.org/stable/1403797},
  urldate    = {2024-04-02},
  note       = {Publisher: [Wiley, International Statistical Institute (ISI)]},
  shorttitle = {Discriminatory Analysis. Nonparametric Discrimination}
}


@book{Clark1922,
  author    = {W. Clark and W.N. Polakov and F.W. Trabold},
  title     = {The Gantt Chart, a Working Tool of Management},
  year      = {1922},
  publisher = {The Ronald Press Company},
  address   = {New York},
  keywords  = {Industrial Efficiency, Production Scheduling, Graphic Methods, Gantt Charts},
  language  = {English}
}


@article{woidasky_use_2021,
  author   = {J. Woidasky and E. Cetinkaya},
  title    = {Use Pattern Relevance for Laptop Repair and Product Lifetime},
  journal  = {Journal of Cleaner Production},
  volume   = {288},
  pages    = {125425},
  year     = {2021},
  month    = mar,
  issn     = {0959-6526},
  doi      = {10.1016/j.jclepro.2020.125425},
  url      = {https://www.sciencedirect.com/science/article/pii/S0959652620354718},
  urldate  = {2024-04-16},
  abstract = {More than 166 million mobile computers worldwide are being sold annually. Their duration of use is not only influenced by the design and quality of components and production, but also by the user behaviour. In this descriptive study, the effects of user decisions on the duration of product life is discussed based on two surveys carried out in a university student environment in southwest Germany. Results show that use phase length expectation for the devices clearly exceed 5 years, but the actual use phase duration was found to be only about 80\% of this time span. Consequently, the use pattern for laptops was described covering the aspects of user knowledge, use and repair intensity, the motivation for and the mode of disposal. Results show that only a minor share of users reads the user manuals. Those who do so and adhere to their recommendations experience a clearly increased use phase length. During the entire product lifetime of mobile computers, about one third of all devices is being repaired. The most important repairs were battery replacements, power supply unit repairs, and housing repairs. After the end of their useful life, more than 60\% of the obsolete devices are still operational. Users pass on only 25\% of their obsolete devices for a second use phase, but almost 2/3 of all laptops are stored, and only 11\% are disposed of properly.},
  keywords = {Laptop, Obsolescence, Product lifetime, Repair, Students, Survey, Use pattern}
}


@inbook{inbook,
  author    = {Kulkarni, A. Anand and Krishnasamy, G. Ganesh and Abraham, A. Ajith},
  title     = {Introduction to Optimization},
  booktitle = {Optimization},
  year      = {2017},
  month     = sep,
  volume    = {114},
  pages     = {1--7},
  isbn      = {978-3-319-44253-2},
  doi       = {10.1007/978-3-319-44254-9_1}
}


@book{eiben2015,
  author    = {A.E. Eiben and J.E. Smith},
  title     = {Introduction to Evolutionary Computing},
  edition   = {2nd},
  series    = {Natural Computing Series},
  publisher = {Springer},
  year      = {2015},
  address   = {Berlin, Heidelberg},
  pages     = {30},
  doi       = {10.1007/978-3-662-44874-8},
  isbn      = {978-3-662-44873-1},
  note      = {S2CID 20912932}
}


@book{stone1972,
  author    = {Stone, James L.},
  title     = {Introduction to Computer Organization and Data Structures},
  year      = {1972},
  publisher = {McGraw-Hill},
  address   = {New York}
}

@book{johnjeffery_automata,
  title     = {Introduction to Automata Theory, Languages and Computation},
  author    = {John E. Hopcroft and Jeffrey D. Ullman},
  publisher = {Addison-Wesley Publishing Company},
  year      = {1979},
  series    = {Addison-Wesley Series in Computer Science},
  edition   = {1st}
}


@book{leeuwen_algorithms_1998,
  author    = {Leeuwen, Jan van},
  booktitle = {Handbook of theoretical computer science / ed. by {Jan}. van {Leeuwen}},
  title     = {Algorithms and complexity},
  year      = {1998},
  isbn      = {978-0-262-22038-5},
  url       = {http://www.gbv.de/dms/bowker/toc/9780444880710.pdf},
  abstract  = {This first part presents chapters on models of computation, complexity theory, data structures, and efficient computation in many recognized sub-disciplines of Theoretical Computer Science.},
  language  = {eng},
  urldate   = {2024-04-20},
  publisher = {Elsevier},
  address   = {Amsterdam},
  edition   = {1. MIT Press paperback ed., 2. printing},
  series    = {Handbook of theoretical computer science / ed. by {Jan}. van {Leeuwen}},
  note      = {OCLC: 247934368}
}

@book{shalev2014understanding,
  author    = {Shalev-Shwartz, Shai and Ben-David, Shai},
  title     = {Understanding Machine Learning: From Theory to Algorithms},
  year      = {2014},
  publisher = {CUP},
  isbn      = {978-3-319-44253-2}
}


@misc{venkat2018curse,
  author = {Venkat, Naveen},
  year   = {2018},
  month  = {09},
  title  = {The Curse of Dimensionality: Inside Out},
  doi    = {10.13140/RG.2.2.29631.36006}
}

@book{bellman1957dynamic,
  author    = {Bellman, Richard},
  title     = {Dynamic Programming},
  publisher = {Princeton Univ Pr},
  year      = {1957},
  isbn      = {978-0691079516}
}

@misc{peng_interpreting_2024,
  author   = {Peng, Dehua and Gui, Zhipeng and Wu, Huayi},
  title    = {Interpreting the {Curse} of {Dimensionality} from {Distance} {Concentration} and {Manifold} {Effect}},
  year     = {2024},
  month    = jan,
  url      = {http://arxiv.org/abs/2401.00422},
  doi      = {10.48550/arXiv.2401.00422},
  abstract = {The characteristics of data like distribution and heterogeneity, become more complex and counterintuitive as the dimensionality increases. This phenomenon is known as curse of dimensionality, where common patterns and relationships (e.g., internal and boundary pattern) that hold in low-dimensional space may be invalid in higher-dimensional space. It leads to a decreasing performance for the regression, classification or clustering models or algorithms. Curse of dimensionality can be attributed to many causes. In this paper, we first summarize five challenges associated with manipulating high-dimensional data, and explains the potential causes for the failure of regression, classification or clustering tasks. Subsequently, we delve into two major causes of the curse of dimensionality, distance concentration and manifold effect, by performing theoretical and empirical analyses. The results demonstrate that nearest neighbor search (NNS) using three typical distance measurements, Minkowski distance, Chebyshev distance, and cosine distance, becomes meaningless as the dimensionality increases. Meanwhile, the data incorporates more redundant features, and the variance contribution of principal component analysis (PCA) is skewed towards a few dimensions. By interpreting the causes of the curse of dimensionality, we can better understand the limitations of current models and algorithms, and drive to improve the performance of data analysis and machine learning tasks in high-dimensional space.},
  language = {en-US},
  urldate  = {2024-04-20},
  note     = {arXiv:2401.00422 [cs]},
  keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning},
  annote   = {Comment: 17 pages, 11 figures}
}


@book{bellman1961adaptive,
  author    = {Bellman, Richard E.},
  title     = {Adaptive Control Processes},
  year      = {1961},
  publisher = {Princeton University Press}
}

@inproceedings{beyer99nn,
  author    = {Beyer, Kevin and Goldstein, Jonathan and Ramakrishnan, Raghu and Shaft, Uri},
  title     = {When Is ``Nearest Neighbor'' Meaningful?},
  booktitle = {Proceedings of the 7th International Conference on Database Theory},
  year      = {1999},
  pages     = {217--235},
  publisher = {Springer},
  series    = {Lecture Notes in Computer Science},
  volume    = {1540},
  file      = {beyer99nn.pdf:papers\\beyer99nn.pdf:PDF},
  language  = {english},
  url       = {http://www.springerlink.com/link.asp?id=04p94cqnbge862kh}
}

@article{bianchi2009survey,
  author  = {Bianchi, Leonora and Dorigo, Marco and Gambardella, Luca Maria and Gutjahr, Walter J.},
  title   = {A survey on metaheuristics for stochastic combinatorial optimization},
  journal = {Natural Computing},
  volume  = {8},
  number  = {2},
  pages   = {239--287},
  year    = {2009},
  doi     = {10.1007/s11047-008-9098-4}
}

@inproceedings{xu2014exploration,
  author       = {Xu, Jie and Zhang, Jun},
  title        = {Exploration-exploitation tradeoffs in metaheuristics: Survey and analysis},
  booktitle    = {Proceedings of the 33rd Chinese control conference},
  year         = {2014},
  organization = {IEEE},
  pages        = {8633--8638}
}


@inproceedings{6896450,
  author    = {Xu, Junqin and Zhang, Jihui},
  title     = {Exploration-exploitation tradeoffs in metaheuristics: Survey and analysis},
  booktitle = {Proceedings of the 33rd Chinese Control Conference},
  year      = {2014},
  pages     = {8633--8638},
  keywords  = {Optimization; Sociology; Statistics; Heuristic algorithms; Algorithm design and analysis; Genetic algorithms; Evolutionary computation; Metaheuristics; Exploration; Exploitation; Human intelligence; Hard optimization problems; Complexity},
  doi       = {10.1109/ChiCC.2014.6896450}
}


@article{585893,
  author   = {Wolpert, D. H. and Macready, W. G.},
  title    = {No free lunch theorems for optimization},
  journal  = {IEEE Transactions on Evolutionary Computation},
  year     = {1997},
  volume   = {1},
  number   = {1},
  pages    = {67--82},
  keywords = {Iron; Evolutionary computation; Information theory; Minimax techniques; Simulated annealing; Algorithm design and analysis; Performance analysis; Probability distribution; Bayesian methods},
  doi      = {10.1109/4235.585893}
}


@misc{goldblum2023free,
  author        = {Goldblum, Micah and Finzi, Marc and Rowan, Keefer and Wilson, Andrew Gordon},
  title         = {The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning},
  year          = {2023},
  eprint        = {2304.05366},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@book{murphy2012machine,
  author    = {Murphy, Kevin P.},
  title     = {Machine Learning: A Probabilistic Perspective},
  publisher = {The MIT Press},
  year      = {2012},
  series    = {Adaptive Computation and Machine Learning},
  edition   = {1}
}


@misc{sah2020machine,
  author = {Sah, Shagan},
  title  = {Machine Learning: A Review of Learning Types},
  year   = {2020},
  month  = {07},
  doi    = {10.20944/preprints202007.0230.v1}
}

@misc{scikit-learn-svm,
  author       = {{scikit-learn Developers}},
  title        = {{Support Vector Machines — scikit-learn documentation}},
  howpublished = {\url{https://scikit-learn.org/stable/modules/svm.html}},
  year         = {2024},
  note         = {Archived from the original on 2017-11-08, Retrieved 2024-04-27}
}


@book{hastie2009elements,
  author    = {T. Hastie and R. Tibshirani and J. Friedman},
  title     = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  edition   = {2nd},
  year      = {2009},
  publisher = {Springer},
  address   = {New York}
}


@inproceedings{10.1007/978-3-540-39964-3_62,
  author    = {G. Guo and H. Wang and D. Bell and Y. Bi and K. Greer},
  title     = {KNN Model-Based Approach in Classification},
  booktitle = {On The Move to Meaningful Internet Systems 2003: CoopIS, DOA, and ODBASE},
  year      = {2003},
  editor    = {R. Meersman and Z. Tari and D. C. Schmidt},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {986--996},
  abstract  = {The k-Nearest-Neighbours (kNN) is a simple but effective method for classification. The major drawbacks with respect to kNN are (1) its low efficiency -- being a lazy learning method prohibits it in many applications such as dynamic web mining for a large repository, and (2) its dependency on the selection of a ``good value'' for k. In this paper, we propose a novel kNN type method for classification that is aimed at overcoming these shortcomings. Our method constructs a kNN model for the data, which replaces the data to serve as the basis of classification. The value of k is automatically determined, is varied for different data, and is optimal in terms of classification accuracy. The construction of the model reduces the dependency on k and makes classification faster. Experiments were carried out on some public datasets collected from the UCI machine learning repository in order to test our method. The experimental results show that the kNN based model compares well with C5.0 and kNN in terms of classification accuracy, but is more efficient than the standard kNN.},
  isbn      = {978-3-540-39964-3}
}


@book{10.5555/522098,
  author    = {M. Mitchell},
  title     = {An Introduction to Genetic Algorithms},
  year      = {1998},
  isbn      = {0262631857},
  publisher = {MIT Press},
  address   = {Cambridge, MA, USA},
  abstract  = {Genetic algorithms have been used in science and engineering as adaptive algorithms for solving practical problems and as computational models of natural evolutionary systems. This brief, accessible introduction describes some of the most interesting research in the field and also enables readers to implement and experiment with genetic algorithms on their own. It focuses in depth on a small set of important and interesting topics -- particularly in machine learning, scientific modeling, and artificial life -- and reviews a broad span of research, including the work of Mitchell and her colleagues. The descriptions of applications and modeling projects stretch beyond the strict boundaries of computer science to include dynamical systems theory, game theory, molecular biology, ecology, evolutionary biology, and population genetics, underscoring the exciting "general purpose" nature of genetic algorithms as search methods that can be employed across disciplines. An Introduction to Genetic Algorithms is accessible to students and researchers in any scientific discipline. It includes many thought and computer exercises that build on and reinforce the reader's understanding of the text.}
}

@article{mathew2012genetic,
  author  = {T. V. Mathew},
  title   = {Genetic algorithm},
  journal = {Report submitted at IIT Bombay},
  volume  = {53},
  year    = {2012}
}


@article{mirjalili2019genetic,
  author    = {S. Mirjalili and S. Mirjalili},
  title     = {Genetic algorithm},
  journal   = {Evolutionary algorithms and neural networks: Theory and applications},
  pages     = {43--55},
  year      = {2019},
  publisher = {Springer}
}


@incollection{DAGDIA2020283,
  author    = {Z. C. Dagdia and M. Mirchev},
  title     = {Chapter 15 - When Evolutionary Computing Meets Astro- and Geoinformatics},
  booktitle = {Knowledge Discovery in Big Data from Astronomy and Earth Observation},
  editor    = {P. Škoda and F. Adam},
  publisher = {Elsevier},
  year      = {2020},
  pages     = {283-306},
  isbn      = {978-0-12-819154-5},
  doi       = {10.1016/B978-0-12-819154-5.00026-6},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780128191545000266},
  keywords  = {Evolutionary computation, Bio-inspired computing, Metaheuristics, Astroinformatics, Geoinformatics},
  abstract  = {Knowledge discovery from data typically includes solving some type of an optimization problem that can be efficiently addressed using algorithms belonging to the class of evolutionary and bio-inspired computation. In this chapter, we give an overview of the various kinds of evolutionary algorithms, such as genetic algorithms, evolutionary strategy, evolutionary and genetic programming, differential evolution, and coevolutionary algorithms, as well as several other bio-inspired approaches, like swarm intelligence and artificial immune systems. After elaborating on the methodology, we provide numerous examples of applications in astronomy and geoscience and show how these algorithms can be applied within a distributed environment, by making use of parallel computing, which is essential when dealing with Big Data.}
}


@misc{purduelecture,
  author       = {{Purdue University College of Engineering}},
  title        = {{Lecture 4: Real-Coded Genetic Algorithms}},
  howpublished = {Lecture notes},
  url          = {https://engineering.purdue.edu/~sudhoff/ee630/Lecture04.pdf},
  note         = {Accessed on April 27, 2024}
}


@article{miller_genetic_nodate,
  author   = {B. L. Miller},
  title    = {Genetic {Algorithms}, {Tournament} {Selection}, and the {Effects} of {Noise}},
  language = {en}
}


@article{kashef_advanced_2015,
  author   = {S. Kashef and H. Nezamabadi-pour},
  title    = {An advanced {ACO} algorithm for feature subset selection},
  journal  = {Neurocomputing},
  volume   = {147},
  pages    = {271--279},
  year     = {2015},
  month    = jan,
  issn     = {0925-2312},
  doi      = {10.1016/j.neucom.2014.06.067},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231214008601},
  abstract = {Feature selection is an important task for data analysis and information retrieval processing, pattern classification systems, and data mining applications. It reduces the number of features by removing noisy, irrelevant and redundant data. In this paper, a novel feature selection algorithm based on Ant Colony Optimization (ACO), called Advanced Binary ACO (ABACO), is presented. Features are treated as graph nodes to construct a graph model and are fully connected to each other. In this graph, each node has two sub-nodes, one for selecting and the other for deselecting the feature. Ant colony algorithm is used to select nodes while ants should visit all features. The use of several statistical measures is examined as the heuristic function for visibility of the edges in the graph. At the end of a tour, each ant has a binary vector with the same length as the number of features, where 1 implies selecting and 0 implies deselecting the corresponding feature. The performance of proposed algorithm is compared to the performance of Binary Genetic Algorithm (BGA), Binary Particle Swarm Optimization (BPSO), CatfishBPSO, Improved Binary Gravitational Search Algorithm (IBGSA), and some prominent ACO-based algorithms on the task of feature selection on 12 well-known UCI datasets. Simulation results verify that the algorithm provides a suitable feature subset with good classification accuracy using a smaller feature set than competing feature selection methods.},
  keywords = {Classification, Feature selection, Ant colony optimization (ACO), Binary ACO, Wrapper}
}



@inproceedings{dorigo_ant_1999,
  author    = {M. Dorigo and G. Di Caro},
  title     = {Ant colony optimization: a new meta-heuristic},
  booktitle = {Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406)},
  year      = {1999},
  volume    = {2},
  pages     = {1470--1477 Vol. 2},
  month     = jul,
  doi       = {10.1109/CEC.1999.782657},
  url       = {https://ieeexplore.ieee.org/document/782657},
  abstract  = {Recently, a number of algorithms inspired by the foraging behavior of ant colonies have been applied to the solution of difficult discrete optimization problems. We put these algorithms in a common framework by defining the Ant Colony Optimization (ACO) meta-heuristic. A couple of paradigmatic examples of applications of these novel meta-heuristic are given, as well as a brief overview of existing applications.},
  keywords  = {Ant colony optimization, Circuits, Cities and towns, Cost function, Routing, Time measurement, Traveling salesman problems, Vehicles}
}



@inproceedings{kennedy_particle_1995,
  author    = {J. Kennedy and R. Eberhart},
  title     = {Particle swarm optimization},
  booktitle = {Proceedings of ICNN'95 - International Conference on Neural Networks},
  year      = {1995},
  month     = nov,
  volume    = {4},
  pages     = {1942--1948 vol.4},
  doi       = {10.1109/ICNN.1995.488968},
  url       = {https://ieeexplore.ieee.org/document/488968},
  abstract  = {A concept for the optimization of nonlinear functions using particle swarm methodology is introduced. The evolution of several paradigms is outlined, and an implementation of one of the paradigms is discussed. Benchmark testing of the paradigm is described, and applications, including nonlinear function optimization and neural network training, are proposed. The relationships between particle swarm optimization and both artificial life and genetic algorithms are described.},
  language  = {en-US}
}



@article{storn_differential_1997,
  author   = {R. Storn and K. Price},
  title    = {Differential Evolution - A Simple and Efficient Heuristic for global Optimization over Continuous Spaces},
  journal  = {Journal of Global Optimization},
  volume   = {11},
  number   = {4},
  pages    = {341--359},
  year     = {1997},
  month    = dec,
  issn     = {1573-2916},
  doi      = {10.1023/A:1008202821328},
  url      = {https://doi.org/10.1023/A:1008202821328},
  abstract = {A new heuristic approach for minimizing possibly nonlinear and non-differentiable continuous space functions is presented. By means of an extensive testbed it is demonstrated that the new method converges faster and with more certainty than many other acclaimed global optimization methods. The new method requires few control variables, is robust, easy to use, and lends itself very well to parallel computation.},
  keywords = {evolution strategy, genetic algorithm, global optimization, nonlinear optimization, Stochastic optimization}
}


@book{10.5555/1557464,
  author    = {A. P. Engelbrecht},
  title     = {Computational Intelligence: An Introduction},
  year      = {2007},
  publisher = {Wiley Publishing},
  isbn      = {0470035617},
  edition   = {2nd},
  abstract  = {Computational Intelligence: An Introduction, Second Edition offers an in-depth exploration into the adaptive mechanisms that enable intelligent behaviour in complex and changing environments. The main focus of this text is centred on the computational modelling of biological and natural intelligent systems, encompassing swarm intelligence, fuzzy systems, artificial neutral networks, artificial immune systems and evolutionary computation. Engelbrecht provides readers with a wide knowledge of Computational Intelligence (CI) paradigms and algorithms; inviting readers to implement and problem solve real-world, complex problems within the CI development framework. This implementation framework will enable readers to tackle new problems without any difficulty through a single Java class as part of the CI library. Key features of this second edition include: A tutorial, hands-on based presentation of the material. State-of-the-art coverage of the most recent developments in computational intelligence with more elaborate discussions on intelligence and artificial intelligence (AI). New discussion of Darwinian evolution versus Lamarckian evolution, also including swarm robotics, hybrid systems and artificial immune systems. A section on how to perform empirical studies; topics including statistical analysis of stochastic algorithms, and an open source library of CI algorithms. Tables, illustrations, graphs, examples, assignments, Java code implementing the algorithms, and a complete CI implementation and experimental framework. Computational Intelligence: An Introduction, Second Edition is essential reading for third and fourth year undergraduate and postgraduate students studying CI. The first edition has been prescribed by a number of overseas universities and is thus a valuable teaching tool. In addition, it will also be a useful resource for researchers in Computational Intelligence and Artificial Intelligence, as well as engineers, statisticians, operational researchers, and bioinformaticians with an interest in applying AI or CI to solve problems in their domains. Check out http://www.ci.cs.up.ac.za for examples, assignments and Java code implementing the algorithms.}
}


@article{genetic-drift,
  author = {K. Price},
  year   = {2008},
  month  = jan,
  title  = {Eliminating Drift Bias from the Differential Evolution Algorithm},
  volume = {143},
  isbn   = {978-3-540-68827-3},
  doi    = {10.1007/978-3-540-68830-3_2}
}


@article{karaboga_idea_nodate,
  title    = {{AN IDEA BASED ON HONEY BEE SWARM FOR NUMERICAL OPTIMIZATION}},
  author   = {D. Karaboga},
  language = {en}
}


@article{saremi_grasshopper_2017,
  title    = {Grasshopper {Optimisation} {Algorithm}: {Theory} and application},
  author   = {S. Saremi and S. Mirjalili and A. Lewis},
  journal  = {Advances in Engineering Software},
  volume   = {105},
  pages    = {30--47},
  year     = {2017},
  month    = mar,
  doi      = {10.1016/j.advengsoft.2017.01.004},
  issn     = {0965-9978},
  abstract = {This paper proposes an optimisation algorithm called Grasshopper Optimisation Algorithm (GOA) and applies it to challenging problems in structural optimisation. The proposed algorithm mathematically models and mimics the behaviour of grasshopper swarms in nature for solving optimisation problems. The GOA algorithm is first benchmarked on a set of test problems including CEC2005 to test and verify its performance qualitatively and quantitatively. It is then employed to find the optimal shape for a 52-bar truss, 3-bar truss, and cantilever beam to demonstrate its applicability. The results show that the proposed algorithm is able to provide superior results compared to well-known and recent algorithms in the literature. The results of the real applications also prove the merits of GOA in solving real problems with unknown search spaces.},
  keywords = {Algorithm, Benchmark, Constrained optimization, Heuristic algorithm, Optimization, Metaheuristics, Optimization techniques},
  language = {en-US},
  urldate  = {2023-11-04},
  url      = {https://www.sciencedirect.com/science/article/pii/S0965997816305646}
}

@article{mirjalili_dragonfly_2016,
  title    = {Dragonfly algorithm: a new meta-heuristic optimization technique for solving single-objective, discrete, and multi-objective problems},
  author   = {S. Mirjalili},
  journal  = {Neural Computing and Applications},
  volume   = {27},
  number   = {4},
  pages    = {1053--1073},
  year     = {2016},
  month    = may,
  doi      = {10.1007/s00521-015-1920-1},
  issn     = {1433-3058},
  abstract = {A novel swarm intelligence optimization technique is proposed called dragonfly algorithm (DA). The main inspiration of the DA algorithm originates from the static and dynamic swarming behaviours of dragonflies in nature. Two essential phases of optimization, exploration and exploitation, are designed by modelling the social interaction of dragonflies in navigating, searching for foods, and avoiding enemies when swarming dynamically or statistically. The paper also considers the proposal of binary and multi-objective versions of DA called binary DA (BDA) and multi-objective DA (MODA), respectively. The proposed algorithms are benchmarked by several mathematical test functions and one real case study qualitatively and quantitatively. The results of DA and BDA prove that the proposed algorithms are able to improve the initial random population for a given problem, converge towards the global optimum, and provide very competitive results compared to other well-known algorithms in the literature. The results of MODA also show that this algorithm tends to find very accurate approximations of Pareto optimal solutions with high uniform distribution for multi-objective problems. The set of designs obtained for the submarine propeller design problem demonstrate the merits of MODA in solving challenging real problems with unknown true Pareto optimal front as well. Note that the source codes of the DA, BDA, and MODA algorithms are publicly available at http://www.alimirjalili.com/DA.html.},
  keywords = {Benchmark, Constrained optimization, Genetic algorithm, Optimization, Particle swarm optimization, Multi-objective optimization, Binary optimization, Evolutionary algorithms, Swarm intelligence},
  language = {en},
  urldate  = {2023-11-06},
  url      = {https://doi.org/10.1007/s00521-015-1920-1}
}


@article{mirjalili_whale_2016,
  title    = {The {Whale} {Optimization} {Algorithm}},
  author   = {S. Mirjalili and A. Lewis},
  journal  = {Advances in Engineering Software},
  volume   = {95},
  pages    = {51--67},
  year     = {2016},
  month    = may,
  doi      = {10.1016/j.advengsoft.2016.01.008},
  issn     = {0965-9978},
  abstract = {This paper proposes a novel nature-inspired meta-heuristic optimization algorithm, called Whale Optimization Algorithm (WOA), which mimics the social behavior of humpback whales. The algorithm is inspired by the bubble-net hunting strategy. WOA is tested with 29 mathematical optimization problems and 6 structural design problems. Optimization results prove that the WOA algorithm is very competitive compared to the state-of-art meta-heuristic algorithms as well as conventional methods. The source codes of the WOA algorithm are publicly available at http://www.alimirjalili.com/WOA.html.},
  keywords = {Algorithm, Benchmark, Constrained optimization, Genetic algorithm, Heuristic algorithm, Optimization, Particle swarm optimization, Structural optimization},
  language = {en-US},
  urldate  = {2023-10-14},
  url      = {https://www.sciencedirect.com/science/article/pii/S0965997816300163}
}


@misc{yang_new_2010,
  title     = {A {New} {Metaheuristic} {Bat}-{Inspired} {Algorithm}},
  author    = {Xin-She Yang},
  year      = {2010},
  month     = apr,
  url       = {http://arxiv.org/abs/1004.4170},
  doi       = {10.48550/arXiv.1004.4170},
  abstract  = {Metaheuristic algorithms such as particle swarm optimization, firefly algorithm and harmony search are now becoming powerful methods for solving many tough optimization problems. In this paper, we propose a new metaheuristic method, the Bat Algorithm, based on the echolocation behaviour of bats. We also intend to combine the advantages of existing algorithms into the new bat algorithm. After a detailed formulation and explanation of its implementation, we will then compare the proposed algorithm with other existing algorithms, including genetic algorithms and particle swarm optimization. Simulations show that the proposed algorithm seems much superior to other algorithms, and further studies are also discussed.},
  language  = {en-US},
  urldate   = {2024-01-22},
  publisher = {arXiv},
  note      = {Issue: arXiv:1004.4170
               arXiv:1004.4170 [physics]},
  keywords  = {Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control, Physics - Biological Physics, Physics - Computational Physics},
  annote    = {Comment: 10 pages, 2 figures}
}

@article{mirjalili_grey_2014,
  author   = {Mirjalili, S. and Mirjalili, S. M. and Lewis, A.},
  title    = {Grey {Wolf} {Optimizer}},
  journal  = {Advances in Engineering Software},
  volume   = {69},
  number   = {},
  pages    = {46--61},
  year     = {2014},
  month    = mar,
  doi      = {10.1016/j.advengsoft.2013.12.007},
  url      = {https://www.sciencedirect.com/science/article/pii/S0965997813001853},
  abstract = {This work proposes a new meta-heuristic called Grey Wolf Optimizer (GWO) inspired by grey wolves (Canis lupus). The GWO algorithm mimics the leadership hierarchy and hunting mechanism of grey wolves in nature. Four types of grey wolves such as alpha, beta, delta, and omega are employed for simulating the leadership hierarchy. In addition, the three main steps of hunting, searching for prey, encircling prey, and attacking prey, are implemented. The algorithm is then benchmarked on 29 well-known test functions, and the results are verified by a comparative study with Particle Swarm Optimization (PSO), Gravitational Search Algorithm (GSA), Differential Evolution (DE), Evolutionary Programming (EP), and Evolution Strategy (ES). The results show that the GWO algorithm is able to provide very competitive results compared to these well-known meta-heuristics. The paper also considers solving three classical engineering design problems (tension/compression spring, welded beam, and pressure vessel designs) and presents a real application of the proposed method in the field of optical engineering. The results of the classical engineering design problems and real application prove that the proposed algorithm is applicable to challenging problems with unknown search spaces.},
  keywords = {Constrained optimization, Heuristic algorithm, Optimization, Metaheuristics, Optimization techniques, GWO},
  issn     = {0965-9978},
  language = {en-US},
  urldate  = {2023-11-18}
}



@incollection{yang_chapter_2014,
  author    = {Yang, Xin-She},
  editor    = {Yang, Xin-She},
  title     = {Chapter 8 - {Firefly} {Algorithms}},
  booktitle = {Nature-{Inspired} {Optimization} {Algorithms}},
  pages     = {111--127},
  year      = {2014},
  month     = jan,
  publisher = {Elsevier},
  address   = {Oxford},
  isbn      = {978-0-12-416743-8},
  doi       = {10.1016/B978-0-12-416743-8.00008-7},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780124167438000087},
  abstract  = {Firefly algorithms (FA) appeared about five years ago, in 2008, and the literature has expanded dramatically with diverse applications. In this chapter, we introduce the standard firefly algorithm, then briefly review the variants, together with a selection of recent publications. We also analyze the characteristics of FA and try to answer the question of why FA is so efficient.},
  language  = {en-US},
  urldate   = {2024-01-23},
  keywords  = {Optimization, Attraction, Firefly algorithm, Metaheuristic, Multimodality, Nature-inspired}
}
s

@misc{noauthor_levy_nodate,
  title   = {Lévy {Flight} - an overview {\textbar} {ScienceDirect} {Topics}},
  url     = {https://www.sciencedirect.com/topics/physics-and-astronomy/levy-flight},
  urldate = {2024-05-06}
}


@misc{yang_cuckoo_2010,
  title     = {Cuckoo {Search} via {Levy} {Flights}},
  url       = {http://arxiv.org/abs/1003.1594},
  doi       = {10.48550/arXiv.1003.1594},
  abstract  = {In this paper, we intend to formulate a new metaheuristic algorithm, called Cuckoo Search (CS), for solving optimization problems. This algorithm is based on the obligate brood parasitic behaviour of some cuckoo species in combination with the Levy flight behaviour of some birds and fruit flies. We validate the proposed algorithm against test functions and then compare its performance with those of genetic algorithms and particle swarm optimization. Finally, we discuss the implication of the results and suggestion for further research.},
  language  = {en-US},
  urldate   = {2024-03-13},
  publisher = {arXiv},
  author    = {Yang, Xin-She and Deb, Suash},
  month     = mar,
  year      = {2010},
  note      = {arXiv:1003.1594 [math]
               version: 1},
  keywords  = {Mathematics - Optimization and Control, optimization and control}
}


@inproceedings{as,
  author    = {Colorni, Alberto and Dorigo, Marco and Maniezzo, Vittorio},
  year      = {1991},
  month     = jan,
  title     = {Distributed Optimization by Ant Colonies},
  booktitle = {Proceedings of the First European Conference on Artificial Life}
}


@book{simon2013evolutionary,
  title     = {Evolutionary Optimization Algorithms},
  author    = {Simon, D.},
  isbn      = {9781118659502},
  url       = {https://books.google.es/books?id=gwUwIEPqk30C},
  year      = {2013},
  publisher = {Wiley}
}


@inproceedings{dorigo_ant_1999,
  author     = {Dorigo, M. and Di Caro, G.},
  title      = {Ant colony optimization: a new meta-heuristic},
  volume     = {2},
  shorttitle = {Ant colony optimization},
  url        = {https://ieeexplore.ieee.org/document/782657},
  doi        = {10.1109/CEC.1999.782657},
  abstract   = {Recently, a number of algorithms inspired by the foraging behavior of ant colonies have been applied to the solution of difficult discrete optimization problems. We put these algorithms in a common framework by defining the Ant Colony Optimization (ACO) meta-heuristic. A couple of paradigmatic examples of applications of these novel meta-heuristic are given, as well as a brief overview of existing applications.},
  language   = {en-US},
  urldate    = {2024-01-30},
  booktitle  = {Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406)},
  month      = jul,
  year       = {1999},
  keywords   = {Ant colony optimization, Circuits, Cities and towns, Cost function, Routing, Time measurement, Traveling salesman problems, Vehicles},
  pages      = {1470--1477 Vol. 2}
}

@book{Holland:1975,
  author    = {John H. Holland},
  title     = {Adaptation in Natural and Artificial Systems},
  year      = {1975},
  edition   = {2nd},
  note      = {second edition, 1992},
  publisher = {University of Michigan Press},
  address   = {Ann Arbor, MI}
}


@misc{jaderberg2017population,
  title         = {Population Based Training of Neural Networks},
  author        = {Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray},
  year          = {2017},
  eprint        = {1711.09846},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}


@article{dokeroglu_comprehensive_2022,
  title    = {A comprehensive survey on recent metaheuristics for feature selection},
  volume   = {494},
  issn     = {0925-2312},
  url      = {https://www.sciencedirect.com/science/article/pii/S092523122200474X},
  doi      = {10.1016/j.neucom.2022.04.083},
  abstract = {Feature selection has become an indispensable machine learning process for data preprocessing due to the ever-increasing sizes in actual data. There have been many solution methods proposed for feature selection since the 1970s. For the last two decades, we have witnessed the superiority of metaheuristic feature selection algorithms, and tens of new ones are being proposed every year. This survey focuses on the most outstanding recent metaheuristic feature selection algorithms of the last two decades in terms of their performance in exploration/exploitation operators, selection methods, transfer functions, fitness value evaluations, and parameter setting techniques. Current challenges of the metaheuristic feature selection algorithms and possible future research topics are examined and brought to the attention of the researchers as well.},
  language = {en-US},
  urldate  = {2023-09-22},
  journal  = {Neurocomputing},
  author   = {Dokeroglu, Tansel and Deniz, Ayça and Kiziloz, Hakan Ezgi},
  month    = jul,
  year     = {2022},
  keywords = {Classification, Feature selection, Machine learning, Metaheuristic algorithms, Survey},
  pages    = {269--296}
}

@article{boussaid_survey_2013,
  series   = {Prediction, Control and Diagnosis using Advanced Neural Computations},
  title    = {A survey on optimization metaheuristics},
  volume   = {237},
  issn     = {0020-0255},
  url      = {https://www.sciencedirect.com/science/article/pii/S0020025513001588},
  doi      = {10.1016/j.ins.2013.02.041},
  abstract = {Metaheuristics are widely recognized as efficient approaches for many hard optimization problems. This paper provides a survey of some of the main metaheuristics. It outlines the components and concepts that are used in various metaheuristics in order to analyze their similarities and differences. The classification adopted in this paper differentiates between single solution based metaheuristics and population based metaheuristics. The literature survey is accompanied by the presentation of references for further details, including applications. Recent trends are also briefly discussed.},
  urldate  = {2024-04-20},
  journal  = {Information Sciences},
  author   = {Boussaïd, Ilhem and Lepagnot, Julien and Siarry, Patrick},
  month    = jul,
  year     = {2013},
  keywords = {Diversification, Intensification, Population based metaheuristic, Single solution based metaheuristic},
  pages    = {82--117}
}


@article{agrawal_metaheuristic_2021,
  title      = {Metaheuristic algorithms on feature selection: {A} survey of one decade of research (2009-2019)},
  volume     = {9},
  issn       = {2169-3536},
  shorttitle = {Metaheuristic algorithms on feature selection},
  doi        = {10.1109/ACCESS.2021.3056407},
  abstract   = {Feature selection is a critical and prominent task in machine learning. To reduce the dimension of the feature set while maintaining the accuracy of the performance is the main aim of the feature selection problem. Various methods have been developed to classify the datasets. However, metaheuristic algorithms have achieved great attention in solving numerous optimization problem. Therefore, this paper presents an extensive literature review on solving feature selection problem using metaheuristic algorithms which are developed in the ten years (2009-2019). Further, metaheuristic algorithms have been classified into four categories based on their behaviour. Moreover, a categorical list of more than a hundred metaheuristic algorithms is presented. To solve the feature selection problem, only binary variants of metaheuristic algorithms have been reviewed and corresponding to their categories, a detailed description of them explained. The metaheuristic algorithms in solving feature selection problem are given with their binary classification, name of the classifier used, datasets and the evaluation metrics. After reviewing the papers, challenges and issues are also identified in obtaining the best feature subset using different metaheuristic algorithms. Finally, some research gaps are also highlighted for the researchers who want to pursue their research in developing or modifying metaheuristic algorithms for classification. For an application, a case study is presented in which datasets are adopted from the UCI repository and numerous metaheuristic algorithms are employed to obtain the optimal feature subset.},
  language   = {en-US},
  journal    = {IEEE Access},
  author     = {Agrawal, P. and Abutarboush, H.F. and Ganesh, T. and Mohamed, A.W.},
  year       = {2021},
  keywords   = {Classification, Feature selection, Metaheuristic algorithms, Binary variants, Literature review},
  pages      = {26766--26791},
  annote     = {Cited By :174}
}


@article{GUY2008585,
  title    = {Avoidance of conspecific odour by carabid beetles: a mechanism for the emergence of scale-free searching patterns},
  journal  = {Animal Behaviour},
  volume   = {76},
  number   = {3},
  pages    = {585-591},
  year     = {2008},
  issn     = {0003-3472},
  doi      = {10.1016/j.anbehav.2008.04.004},
  url      = {https://www.sciencedirect.com/science/article/pii/S0003347208001668},
  author   = {Guy, Adam G. and Bohan, David A. and Powers, Stephen J. and Reynolds, Andrew M.},
  keywords = {carabid, conspecific avoidance, Lévy flight, optimal searching strategy},
  abstract = {We monitored the movements of a starved model predator, the carabid beetle Pterostichus melanarius, in arenas containing test papers upon which beetles had previously walked and unexposed control papers. Significantly, beetles accumulated on the unexposed controls, indicating conspecific avoidance (i.e. behaviour designed to avoid locations previously traversed by individuals of the same species). This finding is novel and important because optimal Lévy-flight (scale-free) search patterns for the location of sparsely and randomly distributed prey resources can emerge from conspecific avoidance. This finding may account for the shortcomings of random-walk (scale-finite) models when used to predict large-scale movement and search patterns by extrapolating from observations made at small scales.}
}


@article{emary_binary_2016,
  title    = {Binary grey wolf optimization approaches for feature selection},
  volume   = {172},
  issn     = {0925-2312},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231215010504},
  doi      = {10.1016/j.neucom.2015.06.083},
  abstract = {In this work, a novel binary version of the grey wolf optimization (GWO) is proposed and used to select optimal feature subset for classification purposes. Grey wolf optimizer (GWO) is one of the latest bio-inspired optimization techniques, which simulate the hunting process of grey wolves in nature. The binary version introduced here is performed using two different approaches. In the first approach, individual steps toward the first three best solutions are binarized and then stochastic crossover is performed among the three basic moves to find the updated binary grey wolf position. In the second approach, sigmoidal function is used to squash the continuous updated position, then stochastically threshold these values to find the updated binary grey wolf position. The two approach for binary grey wolf optimization (bGWO) are hired in the feature selection domain for finding feature subset maximizing the classification accuracy while minimizing the number of selected features. The proposed binary versions were compared to two of the common optimizers used in this domain namely particle swarm optimizer and genetic algorithms. A set of assessment indicators are used to evaluate and compared the different methods over 18 different datasets from the UCI repository. Results prove the capability of the proposed binary version of grey wolf optimization (bGWO) to search the feature space for optimal feature combinations regardless of the initialization and the used stochastic operators.},
  language = {en-US},
  urldate  = {2023-11-18},
  journal  = {Neurocomputing},
  author   = {Emary, E. and Zawbaa, Hossam M. and Hassanien, Aboul Ella},
  month    = jan,
  year     = {2016},
  keywords = {Feature selection, Binary grey wolf optimization, Bio-inspired optimization, Evolutionary computation, Grey wolf optimization},
  pages    = {371--381}
}

@inproceedings{hussien_s-shaped_2019,
  address   = {Singapore},
  series    = {Advances in Intelligent Systems and Computing},
  title     = {S-shaped Binary Whale Optimization Algorithm for Feature Selection},
  isbn      = {978-981-10-8863-6},
  doi       = {10.1007/978-981-10-8863-6_9},
  abstract  = {Whale optimization algorithm is one of the recent nature-inspired optimization technique based on the behavior of bubble-net hunting strategy. In this paper, a novel binary version of whale optimization algorithm (bWOA) is proposed to select the optimal feature subset for dimensionality reduction and classifications problem. The new approach is based on a sigmoid transfer function (S-shape). By dealing with the feature selection problem, a free position of the whale must be transformed to their corresponding binary solutions. This transformation is performed by applying an S-shaped transfer function in every dimension that defines the probability of transforming the position vectors’ elements from 0 to 1 and vice versa and hence force the search agents to move in a binary space. K-NN classifier is applied to ensure that the selected features are the relevant ones. A set of criteria are used to evaluate and compare the proposed bWOA-S with the native one over eleven different datasets. The results proved that the new algorithm has a significant performance in finding the optimal feature.},
  language  = {en},
  booktitle = {Recent Trends in Signal and Image Processing},
  publisher = {Springer},
  author    = {Hussien, Abdelazim G. and Hassanien, Aboul Ella and Houssein, Essam H. and Bhattacharyya, Siddhartha and Amin, Mohamed},
  editor    = {Bhattacharyya, Siddhartha and Mukherjee, Anirban and Bhaumik, Hrishikesh and Das, Swagatam and Yoshida, Kaori},
  year      = {2019},
  keywords  = {Classification, Feature selection, Whale optimization algorithm, Binary whale optimization algorithm, Dimensionality reduction, S-shaped},
  pages     = {79--87}
}

@article{mafarja_whale_2018,
  title    = {Whale optimization approaches for wrapper feature selection},
  volume   = {62},
  issn     = {1568-4946},
  url      = {https://www.sciencedirect.com/science/article/pii/S1568494617306695},
  doi      = {10.1016/j.asoc.2017.11.006},
  abstract = {Classification accuracy highly dependents on the nature of the features in a dataset which may contain irrelevant or redundant data. The main aim of feature selection is to eliminate these types of features to enhance the classification accuracy. The wrapper feature selection model works on the feature set to reduce the number of features and improve the classification accuracy simultaneously. In this work, a new wrapper feature selection approach is proposed based on Whale Optimization Algorithm (WOA). WOA is a newly proposed algorithm that has not been systematically applied to feature selection problems yet. Two binary variants of the WOA algorithm are proposed to search the optimal feature subsets for classification purposes. In the first one, we aim to study the influence of using the Tournament and Roulette Wheel selection mechanisms instead of using a random operator in the searching process. In the second approach, crossover and mutation operators are used to enhance the exploitation of the WOA algorithm. The proposed methods are tested on standard benchmark datasets and then compared to three algorithms such as Particle Swarm Optimization (PSO), Genetic Algorithm (GA), the Ant Lion Optimizer (ALO), and five standard filter feature selection methods. The paper also considers an extensive study of the parameter setting for the proposed technique. The results show the efficiency of the proposed approaches in searching for the optimal feature subsets.},
  urldate  = {2023-11-20},
  journal  = {Applied Soft Computing},
  author   = {Mafarja, Majdi and Mirjalili, Seyedali},
  month    = jan,
  year     = {2018},
  keywords = {Classification, Feature selection, Optimization, Crossover, Evolutionary operators, Mutation, Selection, Whale optimization algorithm, WOA},
  pages    = {441--453}
}


@article{mafarja_binary_2018,
  title    = {Binary dragonfly optimization for feature selection using time-varying transfer functions},
  volume   = {161},
  issn     = {0950-7051},
  doi      = {10.1016/j.knosys.2018.08.003},
  abstract = {The Dragonfly Algorithm (DA) is a recently proposed heuristic search algorithm that was shown to have excellent performance for numerous optimization problems. In this paper, a wrapper-feature selection algorithm is proposed based on the Binary Dragonfly Algorithm (BDA). The key component of the BDA is the transfer function that maps a continuous search space to a discrete search space. In this study, eight transfer functions, categorized into two families (S-shaped and V-shaped functions) are integrated into the BDA and evaluated using eighteen benchmark datasets obtained from the UCI data repository. The main contribution of this paper is the proposal of time-varying S-shaped and V-shaped transfer functions to leverage the impact of the step vector on balancing exploration and exploitation. During the early stages of the optimization process, the probability of changing the position of an element is high, which facilitates the exploration of new solutions starting from the initial population. On the other hand, the probability of changing the position of an element becomes lower towards the end of the optimization process. This behavior is obtained by considering the current iteration number as a parameter of transfer functions. The performance of the proposed approaches is compared with that of other state-of-art approaches including the DA, binary grey wolf optimizer (bGWO), binary gravitational search algorithm (BGSA), binary bat algorithm (BBA), particle swarm optimization (PSO), and genetic algorithm in terms of classification accuracy, sensitivity, specificity, area under the curve, and number of selected attributes. Results show that the time-varying S-shaped BDA approach outperforms compared approaches. © 2018},
  language = {English},
  journal  = {Knowledge-Based Systems},
  author   = {Mafarja, M. and Aljarah, I. and Heidari, A.A. and Faris, H. and Fournier-Viger, P. and Li, X. and Mirjalili, S.},
  year     = {2018},
  keywords = {Classification, Feature selection, Optimization, Binary dragonfly algorithm, Transfer functions},
  pages    = {185--204}
}


@article{mafarja_binary_2019,
  title    = {Binary grasshopper optimisation algorithm approaches for feature selection problems},
  volume   = {117},
  issn     = {0957-4174},
  url      = {https://www.sciencedirect.com/science/article/pii/S0957417418305864},
  doi      = {10.1016/j.eswa.2018.09.015},
  abstract = {Feature Selection (FS) is a challenging machine learning-related task that aims at reducing the number of features by removing irrelevant, redundant and noisy data while maintaining an acceptable level of classification accuracy. FS can be considered as an optimisation problem. Due to the difficulty of this problem and having a large number of local solutions, stochastic optimisation algorithms are promising techniques to solve this problem. As a seminal attempt, binary variants of the recent Grasshopper Optimisation Algorithm (GOA) are proposed in this work and employed to select the optimal feature subset for classification purposes within a wrapper-based framework. Two mechanisms are employed to design a binary GOA, the first one is based on Sigmoid and V-shaped transfer functions, and will be indicated by BGOA-S and BGOA-V, respectively. While the second mechanism uses a novel technique that combines the best solution obtained so far. In addition, a mutation operator is employed to enhance the exploration phase in BGOA algorithm (BGOA-M). The proposed methods are evaluated using 25 standard UCI datasets and compared with 8 well-regarded metaheuristic wrapper-based approaches, and six well known filter-based (e.g., correlation FS) approaches. The comparative results show the superior performance of the BGOA and BGOA-M methods compared to other similar techniques in the literature.},
  urldate  = {2023-10-23},
  journal  = {Expert Systems with Applications},
  author   = {Mafarja, Majdi and Aljarah, Ibrahim and Faris, Hossam and Hammouri, Abdelaziz I. and Al-Zoubi, Ala’ M. and Mirjalili, Seyedali},
  month    = mar,
  year     = {2019},
  keywords = {Classification, Feature selection, Binary grasshopper optimisation algorithm, GOA, Optimisation},
  pages    = {267--286}
}


@article{mirjalili_binary_2014,
  title    = {Binary bat algorithm},
  volume   = {25},
  issn     = {0941-0643},
  doi      = {10.1007/s00521-013-1525-5},
  abstract = {Bat algorithm (BA) is one of the recently proposed heuristic algorithms imitating the echolocation behavior of bats to perform global optimization. The superior performance of this algorithm has been proven among the other most well-known algorithms such as genetic algorithm (GA) and particle swarm optimization (PSO). However, the original version of this algorithm is suitable for continuous problems, so it cannot be applied to binary problems directly. In this paper, a binary version of this algorithm is proposed. A comparative study with binary PSO and GA over twenty-two benchmark functions is conducted to draw a conclusion. Furthermore, Wilcoxon's rank-sum nonparametric statistical test was carried out at 5 \% significance level to judge whether the results of the proposed algorithm differ from those of the other algorithms in a statistically significant way. The results prove that the proposed binary bat algorithm (BBA) is able to significantly outperform others on majority of the benchmark functions. In addition, there is a real application of the proposed method in optical engineering called optical buffer design at the end of the paper. The results of the real application also evidence the superior performance of BBA in practice. © 2013 Springer-Verlag London.},
  language = {en-US},
  number   = {3-4},
  journal  = {Neural Computing and Applications},
  author   = {Mirjalili, S. and Mirjalili, S.M. and Yang, X.-S.},
  year     = {2014},
  note     = {Number: 3-4},
  keywords = {Optimization, Binary optimization, Bat algorithm, Bio-inspired algorithm, Discrete evolutionary algorithms, Discrete optimization, Optical buffer design},
  pages    = {663--681}
}


@article{zhang_return-cost-based_2017,
  title    = {A return-cost-based binary firefly algorithm for feature selection},
  volume   = {418-419},
  issn     = {00200255},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0020025516314098},
  doi      = {10.1016/j.ins.2017.08.047},
  abstract = {Various real-world applications can be formulated as feature selection problems, which have been known to be NP-hard. In this paper, we propose an effective feature selection method based on firefly algorithm (FFA), called return-cost-based binary FFA (RcBBFA). The proposed method has the capability of preventing premature convergence and is particularly efficient attributed to the following three aspects. An indicator based on the return-cost is first defined to measure a firefly’s attractiveness from other fireflies. Then, a Pareto dominance-based strategy is presented to seek the attractive one for each firefly. Finally, a binary movement operator based on the return-cost attractiveness and the adaptive jump is developed to update the position of a firefly. The experimental results on a series of public datasets show that the proposed method is competitive in comparison with other feature selection algorithms, including the traditional algorithms, the GA-based algorithm, the PSO-based algorithm, and the FFA-based algorithms.},
  language = {en},
  urldate  = {2024-03-21},
  journal  = {Information Sciences},
  author   = {Zhang, Yong and Song, Xian-fang and Gong, Dun-wei},
  month    = dec,
  year     = {2017},
  pages    = {561--574}
}


@inproceedings{rodrigues_bcs_2013,
  title      = {{BCS}: {A} {Binary} {Cuckoo} {Search} algorithm for feature selection},
  shorttitle = {{BCS}},
  url        = {https://ieeexplore.ieee.org/document/6571881},
  doi        = {10.1109/ISCAS.2013.6571881},
  abstract   = {Feature selection has been actively pursued in the last years, since finding the most discriminative set of features can enhance the recognition rates and also make feature extraction faster. In this paper, the propose a new feature selection called Binary Cuckoo Search, which is based on the behavior of cuckoo birds. The experiments were carried out in the context of theft detection in power distribution systems in two datasets obtained from a Brazilian electrical power company, and have demonstrated the robustness of the proposed technique against several other nature-inspired optimization techniques.},
  urldate    = {2024-03-13},
  booktitle  = {2013 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})},
  author     = {Rodrigues, D. and Pereira, L. A. M. and Almeida, T. N. S. and Papa, J. P. and Souza, A. N. and Ramos, C. C. O. and Yang, Xin-She},
  month      = may,
  year       = {2013},
  note       = {ISSN: 2158-1525},
  keywords   = {Optimization, Accuracy, Birds, Context, Search problems, Training, Vectors},
  pages      = {465--468}
}


@article{mirjalili_s-shaped_2013,
  title    = {S-shaped versus {V}-shaped transfer functions for binary {Particle} {Swarm} {Optimization}},
  volume   = {9},
  issn     = {2210-6502},
  url      = {https://www.sciencedirect.com/science/article/pii/S2210650212000648},
  doi      = {10.1016/j.swevo.2012.09.002},
  abstract = {Particle Swarm Optimization (PSO) is one of the most widely used heuristic algorithms. The simplicity and inexpensive computational cost make this algorithm very popular and powerful in solving a wide range of problems. The binary version of this algorithm has been introduced for solving binary problems. The main part of the binary version is a transfer function which is responsible to map a continuous search space to a discrete search space. Currently, there appears to be insufficient focus on the transfer function in the literature despite its apparent importance. In this study, six new transfer functions divided into two families, S-shaped and V-shaped, are introduced and evaluated. Twenty-five benchmark optimization functions provided by CEC 2005 special session are employed to evaluate these transfer functions and select the best one in terms of avoiding local minima and convergence speed. In order to validate the performance of the best transfer function, a comparative study with six recent modifications of BPSO is provided as well. The results prove that the new introduced V-shaped family of transfer functions significantly improves the performance of the original binary PSO.},
  urldate  = {2023-10-29},
  journal  = {Swarm and Evolutionary Computation},
  author   = {Mirjalili, Seyedali and Lewis, Andrew},
  month    = apr,
  year     = {2013},
  keywords = {Heuristic algorithm, BPSO, Evolutionary algorithm, Particle swarm, PSO, Transfer function},
  pages    = {1--14}
}


@article{kiran_binary_2021,
  title    = {A binary artificial bee colony algorithm and its performance assessment},
  volume   = {175},
  issn     = {09574174},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S095741742100258X},
  doi      = {10.1016/j.eswa.2021.114817},
  abstract = {Artificial bee colony algorithm, ABC for short, is a swarm-based optimization algorithm proposed for solving continuous optimization problems. Due to its simple but effective structure, some binary versions of the algorithm have been developed. In this study, we focus on the modification of its xor-based binary version, called as binABC. The solution update rule of basic ABC is replaced with a xor logic gate in binABC algorithm, and binABC works on discretely-structured solution space. The rest of components in binABC are the same as with the basic ABC algorithm. In order to improve local search capability and convergence characteristics of binABC, a stigmergic behavior-based update rule for onlooker bees of binABC and an extended version of xor-based update rule are proposed in the present study. The developed version of binABC is applied to solve a modern benchmark problem set (CEC2015). To validate the performance of the proposed algorithm, a series of comparisons are conducted on this problem set. The proposed algorithm is first compared with the basic ABC and binABC on CEC2015 set. After its performance validation, six binary versions of ABC algorithm are considered for comparison of the algorithms, and a comprehensive comparison among the state-of-the-art variants of swarm intelligence or evolutionary computation algorithms is conducted on this set of functions. Finally, an uncapacitated facility location problem set, a pure binary optimization problem, is considered for the comparison of the proposed algorithm and binary variants of ABC algorithm. The experimental results and comparisons show that the proposed algorithm is successful and effective in solving binary optimization problems as its basic version in solving continuous optimization problems.},
  language = {en},
  urldate  = {2023-11-28},
  journal  = {Expert Systems with Applications},
  author   = {Kiran, Mustafa Servet},
  month    = aug,
  year     = {2021},
  pages    = {114817}
}


@inproceedings{pampara_binary_2006,
  title     = {Binary {Differential} {Evolution}},
  url       = {https://ieeexplore.ieee.org/document/1688535},
  doi       = {10.1109/CEC.2006.1688535},
  abstract  = {The ability of differential evolution (DE) to perform well in continuous-valued search spaces is well documented. The arithmetic reproduction operator used by differential evolution is simple, however, the manner in which the operator is defined, makes it practically impossible to effectively apply the standard DE to other problem spaces. An interesting and unique mapping method is examined which will enable the DE algorithm to operate within binary space. Using angle modulation, a bit string can be generated using a trigonometric generating function. The DE is used to evolve the coefficients to the trigonometric function, thereby allowing a mapping from continuous-space to binary-space. Instead of evolving the higher-dimensional binary solution directly, angle modulation is used together with DE to reduce the complexity of the problem into a 4-dimensional continuous-valued problem. Experimental results indicate the effectiveness of the technique and the viability for the DE to operate in binary space.},
  urldate   = {2024-03-25},
  booktitle = {2006 {IEEE} {International} {Conference} on {Evolutionary} {Computation}},
  author    = {Pampara, G. and Engelbrecht, A.P. and Franken, N.},
  month     = jul,
  year      = {2006},
  note      = {ISSN: 1941-0026},
  keywords  = {Particle swarm optimization, Evolutionary computation, Africa, Arithmetic, Computer science, Genetic algorithms, Optimization methods, Probability density function, Stochastic processes, Testing},
  pages     = {1873--1879}
}

@inproceedings{socha_aco_2004,
  address   = {Berlin, Heidelberg},
  series    = {Lecture {Notes} in {Computer} {Science}},
  title     = {{ACO} for {Continuous} and {Mixed}-{Variable} {Optimization}},
  isbn      = {978-3-540-28646-2},
  doi       = {10.1007/978-3-540-28646-2_3},
  abstract  = {This paper presents how the Ant Colony Optimization (ACO) metaheuristic can be extended to continuous search domains and applied to both continuous and mixed discrete-continuous optimization problems. The paper describes the general underlying idea, enumerates some possible design choices, presents a first implementation, and provides some preliminary results obtained on well-known benchmark problems. The proposed method is compared to other ant, as well as non-ant methods for continuous optimization.},
  language  = {en},
  booktitle = {Ant {Colony} {Optimization} and {Swarm} {Intelligence}},
  publisher = {Springer},
  author    = {Socha, Krzysztof},
  editor    = {Dorigo, Marco and Birattari, Mauro and Blum, Christian and Gambardella, Luca Maria and Mondada, Francesco and Stützle, Thomas},
  year      = {2004},
  keywords  = {Continuous Domain, Continuous Optimization, Future Generation Computer System, Normal Probability Density Function, Probability Density Function},
  pages     = {25--36}
}

@article{dokeroglu_comprehensive_2022,
  title    = {A comprehensive survey on recent metaheuristics for feature selection},
  volume   = {494},
  issn     = {0925-2312},
  url      = {https://www.sciencedirect.com/science/article/pii/S092523122200474X},
  doi      = {10.1016/j.neucom.2022.04.083},
  abstract = {Feature selection has become an indispensable machine learning process for data preprocessing due to the ever-increasing sizes in actual data. There have been many solution methods proposed for feature selection since the 1970s. For the last two decades, we have witnessed the superiority of metaheuristic feature selection algorithms, and tens of new ones are being proposed every year. This survey focuses on the most outstanding recent metaheuristic feature selection algorithms of the last two decades in terms of their performance in exploration/exploitation operators, selection methods, transfer functions, fitness value evaluations, and parameter setting techniques. Current challenges of the metaheuristic feature selection algorithms and possible future research topics are examined and brought to the attention of the researchers as well.},
  language = {en-US},
  urldate  = {2023-09-22},
  journal  = {Neurocomputing},
  author   = {Dokeroglu, Tansel and Deniz, Ayça and Kiziloz, Hakan Ezgi},
  month    = jul,
  year     = {2022},
  keywords = {Classification, Feature selection, Machine learning, Metaheuristic algorithms, Survey},
  pages    = {269--296}
}


@article{mirjalili_s-shaped_2013,
  title    = {S-shaped versus {V}-shaped transfer functions for binary {Particle} {Swarm} {Optimization}},
  volume   = {9},
  issn     = {2210-6502},
  url      = {https://www.sciencedirect.com/science/article/pii/S2210650212000648},
  doi      = {10.1016/j.swevo.2012.09.002},
  abstract = {Particle Swarm Optimization (PSO) is one of the most widely used heuristic algorithms. The simplicity and inexpensive computational cost make this algorithm very popular and powerful in solving a wide range of problems. The binary version of this algorithm has been introduced for solving binary problems. The main part of the binary version is a transfer function which is responsible for mapping a continuous search space to a discrete search space. Currently, there appears to be insufficient focus on the transfer function in the literature despite its apparent importance. In this study, six new transfer functions divided into two families, s-shaped and v-shaped, are introduced and evaluated. Twenty-five benchmark optimization functions provided by CEC 2005 special session are employed to evaluate these transfer functions and select the best one in terms of avoiding local minima and convergence speed. In order to validate the performance of the best transfer function, a comparative study with six recent modifications of BPSO is provided as well. The results prove that the newly introduced v-shaped family of transfer functions significantly improves the performance of the original binary PSO.},
  urldate  = {2023-10-29},
  journal  = {Swarm and Evolutionary Computation},
  author   = {Mirjalili, Seyedali and Lewis, Andrew},
  month    = apr,
  year     = {2013},
  keywords = {Heuristic algorithm, BPSO, Evolutionary algorithm, Particle swarm, PSO, Transfer function},
  pages    = {1--14}
}


@article{he_novel_2022,
  title    = {Novel binary differential evolution algorithm based on {Taper}-shaped transfer functions for binary optimization problems},
  volume   = {69},
  issn     = {2210-6502},
  url      = {https://www.sciencedirect.com/science/article/pii/S221065022100184X},
  doi      = {10.1016/j.swevo.2021.101022},
  abstract = {In order to efficiently solve binary optimization problems using differential evolution (DE), a class of new transfer functions, Taper-shaped transfer functions, is proposed for the first time, employing power functions. Subsequently, the novel binary differential evolution algorithm based on Taper-shaped transfer functions (T-NBDE) is introduced. T-NBDE transforms a real vector representing the individual encoding into a binary vector using the Taper-shaped transfer function, which is suitable for solving binary optimization problems. To verify the practicability of Taper-shaped transfer functions and the excellent performance of T-NBDE, T-NBDE is compared with binary DE based on S-shaped, U-shaped, and V-shaped transfer functions, respectively. Furthermore, it is compared with state-of-the-art algorithms for solving the knapsack problem with a single continuous variable (KPC) and the uncapacitated facility location problem (UFLP). The comparison results show that Taper-shaped transfer functions are competitive with existing transfer functions, and T-NBDE is more effective than existing algorithms for solving the KPC problem and UFLP problem.},
  urldate  = {2024-03-27},
  journal  = {Swarm and Evolutionary Computation},
  author   = {He, Yichao and Zhang, Fazhan and Mirjalili, Seyedali and Zhang, Tong},
  month    = mar,
  year     = {2022},
  keywords = {Evolutionary algorithm, Transfer function, Differential evolution, Knapsack problem with a single continuous variable, Uncapacitated facility location problem},
  pages    = {101022}
}


@misc{citicugr,
  title        = {{CITIC-UGR Sala de Servidores y Clústeres de Computadores}},
  howpublished = {\url{http://citic.ugr.es/pages/informacion_general/equipamiento/cluster}},
  note         = {[Online; accessed 12-May-2024]}
}


@misc{scikit-learn,
  title        = {{MinMaxScaler}},
  howpublished = {Scikit-learn Documentation},
  month        = {},
  year         = {},
  note         = {URL: \url{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html}}
}


@article{molina_comprehensive_2020,
  title      = {Comprehensive {Taxonomies} of {Nature}- and {Bio}-inspired {Optimization}: {Inspiration} {Versus} {Algorithmic} {Behavior}, {Critical} {Analysis} {Recommendations}},
  volume     = {12},
  issn       = {1866-9956},
  shorttitle = {Comprehensive {Taxonomies} of {Nature}- and {Bio}-inspired {Optimization}},
  doi        = {10.1007/s12559-020-09730-8},
  abstract   = {In recent algorithmic family simulates different biological processes observed in Nature in order to efficiently address complex optimization problems. In the last years the number of bio-inspired optimization approaches in literature has grown considerably, reaching unprecedented levels that dark the future prospects of this field of research. This paper addresses this problem by proposing two comprehensive, principle-based taxonomies that allow researchers to organize existing and future algorithmic developments into well-defined categories, considering two different criteria: the source of inspiration and the behavior of each algorithm. Using these taxonomies we review more than three hundred publications dealing with nature-inspired and bio-inspired algorithms, and proposals falling within each of these categories are examined, leading to a critical summary of design trends and similarities between them, and the identification of the most similar classical algorithm for each reviewed paper. From our analysis we conclude that a poor relationship is often found between the natural inspiration of an algorithm and its behavior. Furthermore, similarities in terms of behavior between different algorithms are greater than what is claimed in their public disclosure: specifically, we show that more than one-third of the reviewed bio-inspired solvers are versions of classical algorithms. Grounded on the conclusions of our critical analysis, we give several recommendations and points of improvement for better methodological practices in this active and growing research field. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.},
  language   = {English},
  number     = {5},
  journal    = {Cognitive Computation},
  author     = {Molina, D. and Poyatos, J. and Ser, J.D. and García, S. and Hussain, A. and Herrera, F.},
  year       = {2020},
  keywords   = {Bio-inspired optimization, Classification, Nature-inspired algorithms, Taxonomy},
  pages      = {897--939}
}


@article{Li2019AnAW,
  title   = {An Adaptive Whale Optimization Algorithm Using Gaussian Distribution Strategies and Its Application in Heterogeneous UCAVs Task Allocation},
  author  = {Yintong Li and Tong Han and Hui Zhao and Hanjie Gao},
  journal = {IEEE Access},
  year    = {2019},
  volume  = {7},
  pages   = {110138--110158},
  url     = {https://api.semanticscholar.org/CorpusID:201621393}
}


@article{zhang2016optimal,
  title   = {Optimal feature selection using distance-based discrete firefly algorithm with mutual information criterion},
  author  = {Zhang, L. and Shan, L. and Wang, J.},
  journal = {Neural Computing and Applications},
  year    = {2016},
  doi     = {10.1007/s00521-016-2204-0}
}

@misc{taco_website,
  author       = {Daniel Molina},
  title        = {{TACO Website}},
  howpublished = {\url{https://tacolab.org/}},
  year         = {2024},
  note         = {Accessed: 2024-06-01}
}

@article{garcia-martinez_arbitrary_2012,
  title    = {Arbitrary function optimisation with metaheuristics},
  volume   = {16},
  issn     = {1433-7479},
  url      = {https://doi.org/10.1007/s00500-012-0881-x},
  doi      = {10.1007/s00500-012-0881-x},
  abstract = {No free lunch theorems for optimisation suggest that empirical studies on benchmarking problems are pointless, or even cast negative doubts, when algorithms are being applied to other problems not clearly related to the previous ones. Roughly speaking, reported empirical results are not just the result of algorithms’ performances, but the benchmark used therein as well; and consequently, recommending one algorithm over another for solving a new problem might be always disputable. In this work, we propose an empirical framework, arbitrary function optimisation framework, that allows researchers to formulate conclusions independent of the benchmark problems that were actually addressed, as long as the context of the problem class is mentioned. Experiments on sufficiently general scenarios are reported with the aim of assessing this independence. Additionally, this article presents, to the best of our knowledge, the first thorough empirical study on the no free lunch theorems, which is possible thanks to the application of the proposed methodology, and whose main result is that no free lunch theorems unlikely hold on the set of binary real-world problems. In particular, it is shown that exploiting reasonable heuristics becomes more beneficial than random search when dealing with binary real-world applications.},
  language = {en},
  number   = {12},
  urldate  = {2024-06-04},
  journal  = {Soft Computing},
  author   = {García-Martínez, Carlos and Rodriguez, Francisco J. and Lozano, Manuel},
  month    = dec,
  year     = {2012},
  keywords = {Empirical studies, General-purpose algorithms, No free lunch theorems, Real-world problems, Unbiased results},
  pages    = {2115--2133},
  note     = {Therefore, there is a need for studies that provide a more rigorous and comprehensive comparative evaluation of the different proposals in this field, proposing a dual study that uses both binary and real adaptations with modern algorithms.}
}

@article{Al-Tashi201939496,
  author  = {Qasem Al-Tashi and Said Jadid Abdul Kadir and Helmi Md Rais and Seyedali Mirjalili and Hitham Alhussian},
  title   = {Binary Optimization Using Hybrid Grey Wolf Optimization for Feature Selection},
  year    = {2019},
  journal = {IEEE Access},
  volume  = {7},
  pages   = {39496--39508},
  doi     = {10.1109/ACCESS.2019.2906757},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065103601&doi=10.1109%2fACCESS.2019.2906757&partnerID=40&md5=ec600f296b2a0e1986aecc74e526443c},
  note    = {Cited by: 354; All Open Access, Gold Open Access, Green Open Access}
}

@article{Hu2020,
  author  = {Pei Hu and Jeng-Shyang Pan and Shu-Chuan Chu},
  title   = {Improved Binary Grey Wolf Optimizer and Its application for feature selection},
  year    = {2020},
  journal = {Knowledge-Based Systems},
  volume  = {195},
  doi     = {10.1016/j.knosys.2020.105746},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081912362&doi=10.1016%2fj.knosys.2020.105746&partnerID=40&md5=e4bc8c5352463511026244d5841acd92},
  note    = {Cited by: 282}
}

@article{Hammouri2020,
  author  = {A. I. Hammouri and M. Mafarja and M. A. Al-Betar and M. A. Awadallah and I. Abu-Doush},
  title   = {An improved Dragonfly Algorithm for feature selection},
  year    = {2020},
  journal = {Knowledge-Based Systems},
  volume  = {203},
  doi     = {10.1016/j.knosys.2020.106131},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086804574&doi=10.1016%2fj.knosys.2020.106131&partnerID=40&md5=b7fb8f78062741d4eb957090cadbdae7},
  note    = {Cited by: 139}
}

@article{Sayed2019188,
  author  = {G. I. Sayed and A. Tharwat and A. E. Hassanien},
  title   = {Chaotic dragonfly algorithm: an improved metaheuristic algorithm for feature selection},
  year    = {2019},
  journal = {Applied Intelligence},
  volume  = {49},
  number  = {1},
  pages   = {188--205},
  doi     = {10.1007/s10489-018-1261-8},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052571042&doi=10.1007%2fs10489-018-1261-8&partnerID=40&md5=eabb8615b280a1c2e012b4786cf398b2},
  note    = {Cited by: 201; All Open Access, Bronze Open Access}
}

@conference{Mafarja201712,
  author  = {M. M. Mafarja and D. Eleyan and I. Jaber and A. Hammouri and S. Mirjalili},
  title   = {Binary Dragonfly Algorithm for Feature Selection},
  year    = {2017},
  journal = {Proceedings - 2017 International Conference on New Trends in Computing Sciences, ICTCS 2017},
  volume  = {2018-January},
  pages   = {12--17},
  doi     = {10.1109/ICTCS.2017.43},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050194810&doi=10.1109%2fICTCS.2017.43&partnerID=40&md5=a1c7a06b5517e09598543c9ea3aec738},
  note    = {Cited by: 184; All Open Access, Green Open Access}
}

@article{Aljarah2018478,
  author  = {I. Aljarah and A. M. Al-Zoubi and H. Faris and M. A. Hassonah and S. Mirjalili and H. Saadeh},
  title   = {Simultaneous Feature Selection and Support Vector Machine Optimization Using the Grasshopper Optimization Algorithm},
  year    = {2018},
  journal = {Cognitive Computation},
  volume  = {10},
  number  = {3},
  pages   = {478--495},
  doi     = {10.1007/s12559-017-9542-9},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040796296&doi=10.1007%2fs12559-017-9542-9&partnerID=40&md5=2ab6babc5f6bf1d512e4fce656ac88e9},
  note    = {Cited by: 198; All Open Access, Green Open Access}
}

@article{Mafarja201825,
  author  = {M. Mafarja and I. Aljarah and A. A. Heidari and A. I. Hammouri and H. Faris and A. M. Al-Zoubi and S. Mirjalili},
  title   = {Evolutionary Population Dynamics and Grasshopper Optimization approaches for feature selection problems},
  year    = {2018},
  journal = {Knowledge-Based Systems},
  volume  = {145},
  pages   = {25--45},
  doi     = {10.1016/j.knosys.2017.12.037},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040020546&doi=10.1016%2fj.knosys.2017.12.037&partnerID=40&md5=48c70ba8b45fb49d6218a39cfdc5c11d},
  note    = {Cited by: 351; All Open Access, Green Open Access}
}

@conference{Nakamura2012291,
  author  = {R. Y. M. Nakamura and L. A. M. Pereira and K. A. Costa and D. Rodrigues and J. P. Papa and X.-S. Yang},
  title   = {BBA: A binary bat algorithm for feature selection},
  year    = {2012},
  journal = {Brazilian Symposium of Computer Graphic and Image Processing},
  pages   = {291--297},
  doi     = {10.1109/SIBGRAPI.2012.47},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872367831&doi=10.1109%2fSIBGRAPI.2012.47&partnerID=40&md5=ff823b28bacfe7b29f95d173351af4bf},
  note    = {Cited by: 340}
}

@article{Rodrigues20142250,
  author  = {D. Rodrigues and L. A. M. Pereira and R. Y. M. Nakamura and K. A. P. Costa and X.-S. Yang and A. N. Souza and J. P. Papa},
  title   = {A wrapper approach for feature selection based on Bat Algorithm and Optimum-Path Forest},
  year    = {2014},
  journal = {Expert Systems with Applications},
  volume  = {41},
  number  = {5},
  pages   = {2250--2258},
  doi     = {10.1016/j.eswa.2013.09.023},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890121325&doi=10.1016%2fj.eswa.2013.09.023&partnerID=40&md5=a8b03951959d33ab4549ab19d93b1131},
  note    = {Cited by: 223}
}

@article{Zhang2017561,
  author  = {Y. Zhang and X. Song and D. Gong},
  title   = {A return-cost-based binary firefly algorithm for feature selection},
  year    = {2017},
  journal = {Information Sciences},
  volume  = {418-419},
  pages   = {561--574},
  doi     = {10.1016/j.ins.2017.08.047},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027418461&doi=10.1016%2fj.ins.2017.08.047&partnerID=40&md5=d58e60ad246a9e78eeeed90fee3db508},
  note    = {Cited by: 198}
}

@article{Selvakumar2019148,
  author  = {B. Selvakumar and K. Muneeswaran},
  title   = {Firefly algorithm based feature selection for network intrusion detection},
  year    = {2019},
  journal = {Computers and Security},
  volume  = {81},
  pages   = {148--155},
  doi     = {10.1016/j.cose.2018.11.005},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058021385&doi=10.1016%2fj.cose.2018.11.005&partnerID=40&md5=7b5ec941535b4fbdbba6e9fa389d0559},
  note    = {Cited by: 225}
}

@article{Hu201517,
  author  = {Z. Hu and Y. Bao and T. Xiong and R. Chiong},
  title   = {Hybrid filter-wrapper feature selection for short-term load forecasting},
  year    = {2015},
  journal = {Engineering Applications of Artificial Intelligence},
  volume  = {40},
  pages   = {17--27},
  doi     = {10.1016/j.engappai.2014.12.014},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923374513&doi=10.1016%2fj.engappai.2014.12.014&partnerID=40&md5=0da35f2297a63bf29bb4533935cf5e9c},
  note    = {Cited by: 169}
}

@article{Lin20081817,
  author  = {S. W. Lin and K. C. Ying and S. C. Chen and Z. J. Lee},
  title   = {Particle swarm optimization for parameter determination and feature selection of support vector machines},
  year    = {2008},
  journal = {Expert Systems with Applications},
  volume  = {35},
  number  = {4},
  pages   = {1817--1824},
  doi     = {10.1016/j.eswa.2007.08.088},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-48749109333&doi=10.1016%2fj.eswa.2007.08.088&partnerID=40&md5=797a9356d2b699d1b67214c0dfc71c05},
  note    = {Cited by: 801}
}

@article{Xue20131656,
  author  = {B. Xue and M. Zhang and W. N. Browne},
  title   = {Particle swarm optimization for feature selection in classification: A multi-objective approach},
  year    = {2013},
  journal = {IEEE Transactions on Cybernetics},
  volume  = {43},
  number  = {6},
  pages   = {1656--1671},
  doi     = {10.1109/TSMCB.2012.2227469},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887799080&doi=10.1109%2fTSMCB.2012.2227469&partnerID=40&md5=77e7aaf2d80ac386fa7702f464f25d81},
  note    = {Cited by: 965}
}

@article{Wang2007459,
  author  = {X. Wang and J. Yang and X. Teng and W. Xia and R. Jensen},
  title   = {Feature selection based on rough sets and particle swarm optimization},
  year    = {2007},
  journal = {Pattern Recognition Letters},
  volume  = {28},
  number  = {4},
  pages   = {459--471},
  doi     = {10.1016/j.patrec.2006.09.003},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845523839&doi=10.1016%2fj.patrec.2006.09.003&partnerID=40&md5=7d2f5d2292c94da6d266d62a5af22217},
  note    = {Cited by: 745; All Open Access, Green Open Access}
}

@article{Rao2019634,
  author  = {H. Rao and X. Shi and A. K. Rodrigue and J. Feng and Y. Xia and M. Elhoseny and X. Yuan and L. Gu},
  title   = {Feature selection based on artificial bee colony and gradient boosting decision tree},
  year    = {2019},
  journal = {Applied Soft Computing Journal},
  volume  = {74},
  pages   = {634--642},
  doi     = {10.1016/j.asoc.2018.10.036},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056633453&doi=10.1016%2fj.asoc.2018.10.036&partnerID=40&md5=ea596d5758105a3777fa9232e27645f8},
  note    = {Cited by: 424}
}

@article{Zorarpaci201691,
  author  = {E. Zorarpaci and S. A. Özel},
  title   = {A hybrid approach of differential evolution and artificial bee colony for feature selection},
  year    = {2016},
  journal = {Expert Systems with Applications},
  volume  = {62},
  pages   = {91--103},
  doi     = {10.1016/j.eswa.2016.06.004},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975037777&doi=10.1016%2fj.eswa.2016.06.004&partnerID=40&md5=9e481521fd271007fadbac43e199aa6f},
  note    = {Cited by: 303}
}

@article{Hancer2018462,
  author  = {E. Hancer and B. Xue and M. Zhang and D. Karaboga and B. Akay},
  title   = {Pareto front feature selection based on artificial bee colony optimization},
  year    = {2018},
  journal = {Information Sciences},
  volume  = {422},
  pages   = {462--479},
  doi     = {10.1016/j.ins.2017.09.028},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029537929&doi=10.1016%2fj.ins.2017.09.028&partnerID=40&md5=43f4305ef38c9560eaa0436f6d936ee1},
  note    = {Cited by: 254; All Open Access, Green Open Access}
}

@article{Zhang202067,
  author  = {Y. Zhang and D. Gong and X. Gao and T. Tian and X. Sun},
  title   = {Binary differential evolution with self-learning for multi-objective feature selection},
  year    = {2020},
  journal = {Information Sciences},
  volume  = {507},
  pages   = {67--85},
  doi     = {10.1016/j.ins.2019.08.040},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070734759&doi=10.1016%2fj.ins.2019.08.040&partnerID=40&md5=84fb3bbba58178da727bd9143d385c70},
  note    = {Cited by: 314}
}

@article{Hancer2018103,
  author  = {E. Hancer and B. Xue and M. Zhang},
  title   = {Differential evolution for filter feature selection based on information theory and feature ranking},
  year    = {2018},
  journal = {Knowledge-Based Systems},
  volume  = {140},
  pages   = {103--119},
  doi     = {10.1016/j.knosys.2017.10.028},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034999468&doi=10.1016%2fj.knosys.2017.10.028&partnerID=40&md5=234726be47f971c3d7b5785b3919b973},
  note    = {Cited by: 295; All Open Access, Green Open Access}
}

@article{Karaboga2009108,
  author  = {D. Karaboga and B. Akay},
  title   = {A comparative study of Artificial Bee Colony algorithm},
  year    = {2009},
  journal = {Applied Mathematics and Computation},
  volume  = {214},
  number  = {1},
  pages   = {108--132},
  doi     = {10.1016/j.amc.2009.03.090},
  url     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-67349273050&doi=10.1016%2fj.amc.2009.03.090&partnerID=40&md5=505464030a4a96a1998b20803cd113ce},
  note    = {Cited by: 2940}
}
